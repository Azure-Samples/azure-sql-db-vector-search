{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "a56b4788-b5d4-4314-82b6-9aa5c0bd4c81"
            },
            "source": [
                "\n",
                "# Create, Store, and Query OpenAI Embeddings in Azure SQL DB\n",
                "\n",
                "Learn how to integrate Azure OpenAI API with Azure SQL DB to create, store, and query embeddings for advanced similarity searches and LLM generation augmentation.\n",
                "\n",
                "## Tutorial Overview\n",
                "\n",
                "This Python [notebook](EmbeddingsWithSQL.ipynb) will teach you to:\n",
                "\n",
                "- **Create Embeddings**: Generate embeddings from content using the Azure OpenAI API.\n",
                "- **Vector Database Utilization**: Use Azure SQL DB to store embeddings and perform similarity searches.\n",
                "- **LLM Generation Augmentation**: Enhance language model generation with embeddings from a vector database. In this case we use the embeddings to inform a GPT-4 chat model, enabling it to provide rich, context-aware answers about products based on past customer reviews.\n",
                "\n",
                "\n",
                "\n",
                "## Dataset\n",
                "We use the Fine Foods Review Dataset from Kaggle, which contains Amazon reviews of fine foods. \n",
                "\n",
                "- For simplicity, this tutorial uses a smaller sample [Fine Foods Review Dataset](https://github.com/Azure-Samples/azure-sql-db-vector-search/blob/a181e15337402e568f4fc66fe5941e5973171972/VectorSearch_Notebooks/Datasets/Reviews.csv) to demonstrate embedding generation. \n",
                "- Alternatively, if **you to wish bypass embedding generation** and jump straight to similarity search in SQLDB. you can download the pre-generated [finefoodembeddings.csv](https://github.com/Azure-Samples/azure-sql-db-vector-search/blob/a181e15337402e568f4fc66fe5941e5973171972/VectorSearch_Notebooks/Datasets/finefoodembeddings.csv) \n",
                "\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- **Azure Subscription**: [Create one for free](https:\\azure.microsoft.com\\free\\cognitive-services?azure-portal=true)\n",
                "- **Azure SQL Database**: [Set up your database for free](https:\\learn.microsoft.com\\azure\\azure-sql\\database\\free-offer?view=azuresql)\n",
                "- **Azure Data Studio**: Download [here](https://azure.microsoft.com/products/data-studio) to manage your Azure SQL database and [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel)\n",
                "\n",
                "\n",
                "## Additional Requirements for Embedding Generation\n",
                "\n",
                "- **Azure OpenAI Access**: Apply for access in the desired Azure subscription at [https://aka.ms/oai/access](https:\\aka.ms\\oai\\access)\n",
                "- **Azure OpenAI Resource**: Deploy an embeddings model (e.g., `text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Refer to the [resource deployment guide](https:\\learn.microsoft.com\\azure\\ai-services\\openai\\how-to\\create-resource)\n",
                "- **Python**: Version 3.7.1 or later from Python.org. (Sample has been tested with Python 3.11)\n",
                "- **Python Libraries**: Install the required libraries openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken, and pyodbc.\n",
                "- **Jupyter Notebooks**: Use within [Azure Data Studio](https:\\learn.microsoft.com\\en-us\\azure-data-studio\\notebooks\\notebooks-guidance) or Visual Studio Code .\n",
                "\n",
                "Code snippets are adapted from the [Azure OpenAI Service embeddings Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python)\n",
                "\n",
                "## Getting Started\n",
                "\n",
                "1. **Database Setup**: Execute SQL commands from the `createtable.sql` script to create the necessary table in your database.\n",
                "2. **Model Deployment**: Deploy an embeddings model (`text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion .\n",
                "Note the 2 model deployment names for later use.\n",
                "![Deployed OpenAI Models](../../Assets/modeldeployment.png)\n",
                "\n",
                "\n",
                "3. **Connection String**: Find your Azure SQL DB connection string in the Azure portal under your database settings.\n",
                "4. **Configuration**: Populate the `.env` file with your SQL server connection details , Azure OpenAI key, and endpoint values. \n",
                "\n",
                "You can retrieve the Azure OpenAI *endpoint* and *key*:\n",
                "\n",
                "![Azure OpenAI Endpoint and Key](../../Assets/endpoint.png)\n",
                "\n",
                "## Running the Notebook\n",
                "\n",
                "To [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel), connect to your Azure SQL database using Azure Data Studio, which can be downloaded [here](https://azure.microsoft.com/products/data-studio)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b0b8ed3a-2313-469c-a038-4ea72214bf97",
                "language": "python"
            },
            "source": [
                "# Part 1 : Create Embeddings from Content Using the Azure OpenAI API\n",
                "\n",
                "This section guides you through the process of generating embeddings from your content using the Azure OpenAI API\n",
                "\n",
                "**NOTE**: If you prefer to bypass the embedding generation and proceed directly to similarity search in SQLDB, please move on to **Part 2: Load and Store Embeddings in Azure SQL DB**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "azdata_cell_guid": "59dd0902-a737-4867-b4a3-11c5396a1fad",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: pyodbc in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 1)) (5.1.0)\n",
                        "Requirement already satisfied: python-dotenv in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
                        "Requirement already satisfied: openai in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 3)) (1.30.1)\n",
                        "Requirement already satisfied: num2words in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 4)) (0.5.13)\n",
                        "Requirement already satisfied: matplotlib in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 5)) (3.9.0)\n",
                        "Requirement already satisfied: plotly in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 6)) (5.22.0)\n",
                        "Requirement already satisfied: scipy in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 7)) (1.13.0)\n",
                        "Requirement already satisfied: scikit-learn in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 8)) (1.4.2)\n",
                        "Requirement already satisfied: pandas in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 9)) (2.2.2)\n",
                        "Requirement already satisfied: tiktoken in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 10)) (0.7.0)\n",
                        "Requirement already satisfied: tokenizer in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 11)) (3.4.3)\n",
                        "Requirement already satisfied: azure-identity in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 12)) (1.16.0)\n",
                        "Requirement already satisfied: PrettyTable in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 13)) (3.10.0)\n",
                        "Collecting nltk (from -r requirements.txt (line 14))\n",
                        "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
                        "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (4.3.0)\n",
                        "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (1.9.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (0.27.0)\n",
                        "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (2.7.1)\n",
                        "Requirement already satisfied: sniffio in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (1.3.1)\n",
                        "Requirement already satisfied: tqdm>4 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (4.66.4)\n",
                        "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 3)) (4.11.0)\n",
                        "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from num2words->-r requirements.txt (line 4)) (0.6.2)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.1)\n",
                        "Requirement already satisfied: cycler>=0.10 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.51.0)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
                        "Requirement already satisfied: numpy>=1.23 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.26.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (24.0)\n",
                        "Requirement already satisfied: pillow>=8 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.3.0)\n",
                        "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 5)) (2.9.0.post0)\n",
                        "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from plotly->-r requirements.txt (line 6)) (8.3.0)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.5.0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 9)) (2024.1)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 9)) (2024.1)\n",
                        "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken->-r requirements.txt (line 10)) (2024.5.15)\n",
                        "Requirement already satisfied: requests>=2.26.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken->-r requirements.txt (line 10)) (2.31.0)\n",
                        "Requirement already satisfied: azure-core>=1.23.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 12)) (1.30.1)\n",
                        "Requirement already satisfied: cryptography>=2.5 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 12)) (42.0.7)\n",
                        "Requirement already satisfied: msal>=1.24.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 12)) (1.28.0)\n",
                        "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 12)) (1.1.0)\n",
                        "Requirement already satisfied: wcwidth in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from PrettyTable->-r requirements.txt (line 13)) (0.2.13)\n",
                        "Collecting click (from nltk->-r requirements.txt (line 14))\n",
                        "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
                        "Requirement already satisfied: idna>=2.8 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 3)) (3.7)\n",
                        "Requirement already satisfied: six>=1.11.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-core>=1.23.0->azure-identity->-r requirements.txt (line 12)) (1.16.0)\n",
                        "Requirement already satisfied: cffi>=1.12 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 12)) (1.16.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (2024.2.2)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.5)\n",
                        "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.14.0)\n",
                        "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity->-r requirements.txt (line 12)) (2.8.0)\n",
                        "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from msal-extensions>=0.3.0->azure-identity->-r requirements.txt (line 12)) (2.8.2)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 3)) (0.6.0)\n",
                        "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 3)) (2.18.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 10)) (3.3.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 10)) (2.2.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai->-r requirements.txt (line 3)) (0.4.6)\n",
                        "Requirement already satisfied: pycparser in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->-r requirements.txt (line 12)) (2.22)\n",
                        "Requirement already satisfied: pywin32>=226 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from portalocker<3,>=1.6->msal-extensions>=0.3.0->azure-identity->-r requirements.txt (line 12)) (306)\n",
                        "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
                        "   ---------------------------------------- 1.5/1.5 MB 6.0 MB/s eta 0:00:00Note: you may need to restart the kernel to use updated packages.\n",
                        "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
                        "   ---------------------------------------- 97.9/97.9 kB 5.5 MB/s eta 0:00:00\n",
                        "Installing collected packages: click, nltk\n",
                        "Successfully installed click-8.1.7 nltk-3.8.1\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "#Setup the python libraries required for this notebook\n",
                "#Please ensure that you navigate to the directory containing the `requirements.txt` file in your terminal\n",
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "azdata_cell_guid": "ab896eec-22dd-4a2d-ad5e-eab1d662fb52",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#Load the env details\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ef9509c5-f27a-44ec-ade2-a2f8902e18c1",
                "language": "python"
            },
            "source": [
                "Let us define the function to generate text embeddings from the Azure Open AI `text-embedding-small` model.\n",
                "\n",
                "An **embedding** is a special format of data representation that is optimized for use by machine learning models and algorithms. It is an information-dense representation of the semantic meaning of a piece of text.\n",
                "\n",
                "Each embedding is a vector of floating point numbers. The key characteristic of these vectors is that the distance between two embeddings in the vector space is indicative of the semantic similarity between the two corresponding inputs in their original format. For instance:\n",
                "\n",
                "- If two pieces of text are semantically similar, their vector representations will also be close to each other.\n",
                "- Conversely, dissimilar texts will have embeddings that are farther apart in the vector space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "azdata_cell_guid": "5357f9e6-1a24-474a-b3fc-147441e9b6c8",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "import sys\n",
                "from num2words import num2words\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import tiktoken\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Specify you model name\n",
                "openai_embedding_model = \"embedding\"\n",
                "\n",
                "# Assuming openai_url and openai_key are environment variables\n",
                "openai_url = os.environ.get('AZURE_OPENAI_ENDPOINT') + f\"/openai/deployments/{openai_embedding_model}/embeddings?api-version=2023-03-15-preview\"\n",
                "openai_key = os.environ.get('AZURE_OPENAI_API_KEY')\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\n",
                "\n",
                "    Args:\n",
                "        text (str): Text to embed.\n",
                "\n",
                "    Returns:\n",
                "        dict: A dictionary containing the embedding.\n",
                "    \"\"\"\n",
                "    response = requests.post(openai_url,\n",
                "        headers={\"api-key\": openai_key, \"Content-Type\": \"application/json\"},\n",
                "        json={\"input\": [text]}  # Embed a single sentence\n",
                "    )\n",
                "    embedding = response.json()['data'][0]['embedding']\n",
                "    return embedding"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "0343bba0-01ea-4537-a793-adaadb904781"
            },
            "source": [
                "In this tutorial we will be using the Fine Foods Review Dataset. This dataset consists of reviews of fine foods from Amazon.\n",
                "\n",
                "Now we need to read our [Reviews.csv](https:\\github.com\\Azure-Samples\\azure-sql-db-vector-search\\blob\\a181e15337402e568f4fc66fe5941e5973171972\\VectorSearch_Notebooks\\Datasets\\Reviews.csv) file and create a pandas DataFrame. After the initial DataFrame is created, we can view the contents of the table by running the below. For the purpose of the quick embedding generation tutorial we will only use nrows = 500."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "azdata_cell_guid": "f6815045-f75d-4bba-960f-f76818baa592",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Id</th>\n",
                            "      <th>Time</th>\n",
                            "      <th>ProductId</th>\n",
                            "      <th>UserId</th>\n",
                            "      <th>Score</th>\n",
                            "      <th>Summary</th>\n",
                            "      <th>Text</th>\n",
                            "      <th>combined</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1303862400</td>\n",
                            "      <td>B001E4KFG0</td>\n",
                            "      <td>A3SGXH7AUHU8GW</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Good Quality Dog Food</td>\n",
                            "      <td>I have bought several of the Vitality canned d...</td>\n",
                            "      <td>Good Quality Dog Food: I have bought several o...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2</td>\n",
                            "      <td>1346976000</td>\n",
                            "      <td>B00813GRG4</td>\n",
                            "      <td>A1D87F6ZCVE5NK</td>\n",
                            "      <td>1</td>\n",
                            "      <td>Not as Advertised</td>\n",
                            "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
                            "      <td>Not as Advertised: Product arrived labeled as ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>3</td>\n",
                            "      <td>1219017600</td>\n",
                            "      <td>B000LQOCH0</td>\n",
                            "      <td>ABXLMWJIXXAIN</td>\n",
                            "      <td>4</td>\n",
                            "      <td>\"Delight\" says it all</td>\n",
                            "      <td>This is a confection that has been around a fe...</td>\n",
                            "      <td>\"Delight\" says it all: This is a confection th...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>4</td>\n",
                            "      <td>1307923200</td>\n",
                            "      <td>B000UA0QIQ</td>\n",
                            "      <td>A395BORC6FGVXV</td>\n",
                            "      <td>2</td>\n",
                            "      <td>Cough Medicine</td>\n",
                            "      <td>If you are looking for the secret ingredient i...</td>\n",
                            "      <td>Cough Medicine: If you are looking for the sec...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>5</td>\n",
                            "      <td>1350777600</td>\n",
                            "      <td>B006K2ZZ7K</td>\n",
                            "      <td>A1UQRSCLF8GW1T</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Great taffy</td>\n",
                            "      <td>Great taffy at a great price.  There was a wid...</td>\n",
                            "      <td>Great taffy: Great taffy at a great price.  Th...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>6</td>\n",
                            "      <td>1342051200</td>\n",
                            "      <td>B006K2ZZ7K</td>\n",
                            "      <td>ADT0SRK1MGOEU</td>\n",
                            "      <td>4</td>\n",
                            "      <td>Nice Taffy</td>\n",
                            "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
                            "      <td>Nice Taffy: I got a wild hair for taffy and or...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>7</td>\n",
                            "      <td>1340150400</td>\n",
                            "      <td>B006K2ZZ7K</td>\n",
                            "      <td>A1SP2KVKFXXRU1</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Great!  Just as good as the expensive brands!</td>\n",
                            "      <td>This saltwater taffy had great flavors and was...</td>\n",
                            "      <td>Great!  Just as good as the expensive brands!:...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>8</td>\n",
                            "      <td>1336003200</td>\n",
                            "      <td>B006K2ZZ7K</td>\n",
                            "      <td>A3JRGQVEQN31IQ</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Wonderful, tasty taffy</td>\n",
                            "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
                            "      <td>Wonderful, tasty taffy: This taffy is so good....</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>9</td>\n",
                            "      <td>1322006400</td>\n",
                            "      <td>B000E7L2R4</td>\n",
                            "      <td>A1MZYO9TZK0BBI</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Yay Barley</td>\n",
                            "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
                            "      <td>Yay Barley: Right now I'm mostly just sproutin...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>10</td>\n",
                            "      <td>1351209600</td>\n",
                            "      <td>B00171APVA</td>\n",
                            "      <td>A21BT40VZCCYT4</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Healthy Dog Food</td>\n",
                            "      <td>This is a very healthy dog food. Good for thei...</td>\n",
                            "      <td>Healthy Dog Food: This is a very healthy dog f...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   Id        Time   ProductId          UserId  Score  \\\n",
                            "0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5   \n",
                            "1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1   \n",
                            "2   3  1219017600  B000LQOCH0   ABXLMWJIXXAIN      4   \n",
                            "3   4  1307923200  B000UA0QIQ  A395BORC6FGVXV      2   \n",
                            "4   5  1350777600  B006K2ZZ7K  A1UQRSCLF8GW1T      5   \n",
                            "5   6  1342051200  B006K2ZZ7K   ADT0SRK1MGOEU      4   \n",
                            "6   7  1340150400  B006K2ZZ7K  A1SP2KVKFXXRU1      5   \n",
                            "7   8  1336003200  B006K2ZZ7K  A3JRGQVEQN31IQ      5   \n",
                            "8   9  1322006400  B000E7L2R4  A1MZYO9TZK0BBI      5   \n",
                            "9  10  1351209600  B00171APVA  A21BT40VZCCYT4      5   \n",
                            "\n",
                            "                                         Summary  \\\n",
                            "0                          Good Quality Dog Food   \n",
                            "1                              Not as Advertised   \n",
                            "2                          \"Delight\" says it all   \n",
                            "3                                 Cough Medicine   \n",
                            "4                                    Great taffy   \n",
                            "5                                     Nice Taffy   \n",
                            "6  Great!  Just as good as the expensive brands!   \n",
                            "7                         Wonderful, tasty taffy   \n",
                            "8                                     Yay Barley   \n",
                            "9                               Healthy Dog Food   \n",
                            "\n",
                            "                                                Text  \\\n",
                            "0  I have bought several of the Vitality canned d...   \n",
                            "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
                            "2  This is a confection that has been around a fe...   \n",
                            "3  If you are looking for the secret ingredient i...   \n",
                            "4  Great taffy at a great price.  There was a wid...   \n",
                            "5  I got a wild hair for taffy and ordered this f...   \n",
                            "6  This saltwater taffy had great flavors and was...   \n",
                            "7  This taffy is so good.  It is very soft and ch...   \n",
                            "8  Right now I'm mostly just sprouting this so my...   \n",
                            "9  This is a very healthy dog food. Good for thei...   \n",
                            "\n",
                            "                                            combined  \n",
                            "0  Good Quality Dog Food: I have bought several o...  \n",
                            "1  Not as Advertised: Product arrived labeled as ...  \n",
                            "2  \"Delight\" says it all: This is a confection th...  \n",
                            "3  Cough Medicine: If you are looking for the sec...  \n",
                            "4  Great taffy: Great taffy at a great price.  Th...  \n",
                            "5  Nice Taffy: I got a wild hair for taffy and or...  \n",
                            "6  Great!  Just as good as the expensive brands!:...  \n",
                            "7  Wonderful, tasty taffy: This taffy is so good....  \n",
                            "8  Yay Barley: Right now I'm mostly just sproutin...  \n",
                            "9  Healthy Dog Food: This is a very healthy dog f...  "
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# load & inspect dataset\n",
                "df = pd.read_csv(r\"../Datasets/Reviews.csv\", nrows=500)\n",
                "df = df[[\"Id\" , \"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
                "df = df.dropna()\n",
                "df[\"combined\"] = (\n",
                "    df.Summary.str.strip() + \": \" + df.Text.str.strip()\n",
                ")\n",
                "df.head(10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "027376ed-edc8-49ab-a361-a9aea8781b77",
                "language": "python"
            },
            "source": [
                "Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization. We will also remove comments that are too long for the token limit (8192 tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "3146e381-1ff7-4932-a457-60564daf1b7f",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\damauri\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   Id        Time   ProductId          UserId  Score                Summary  \\\n",
                        "0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5  Good Quality Dog Food   \n",
                        "1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1      Not as Advertised   \n",
                        "\n",
                        "                                                Text  \\\n",
                        "0  I have bought several of the Vitality canned d...   \n",
                        "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
                        "\n",
                        "                                            combined  n_tokens  \n",
                        "0  good quality dog food bought several vitality ...        30  \n",
                        "1  advertised product arrived labeled jumbo salte...        26  \n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import tiktoken\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "import string\n",
                "\n",
                "# Remove null values\n",
                "df.dropna(subset=['combined'], inplace=True)\n",
                "\n",
                "# Convert to lowercase\n",
                "df['combined'] = df['combined'].str.lower()\n",
                "\n",
                "# Remove accented letters\n",
                "df['combined'] = df['combined'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
                "\n",
                "# Remove punctuation marks\n",
                "translator = str.maketrans('', '', string.punctuation)\n",
                "df['combined'] = df['combined'].apply(lambda x: x.translate(translator))\n",
                "\n",
                "# Remove redundant white space\n",
                "df['combined'] = df['combined'].str.strip()\n",
                "\n",
                "# Remove stopwords using NLTK\n",
                "nltk.download('stopwords')\n",
                "stop_words = set(stopwords.words('english'))\n",
                "df['combined'] = df['combined'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
                "\n",
                "# Remove Unicode characters (like emojis)\n",
                "df['combined'] = df['combined'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
                "\n",
                "# Tokenize using tiktoken\n",
                "# Assuming you have already imported tiktoken and loaded the tokenizer\n",
                "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "\n",
                "# Convert the list of tokens to a single integer (assuming each list contains token IDs)\n",
                "df['n_tokens'] = df['combined'].apply(lambda x: len(tokenizer.encode(x)))\n",
                "\n",
                "# Filter rows where 'n_tokens' is less than 8000\n",
                "df = df[df['n_tokens'] < 8000]\n",
                "\n",
                "# Print the modified DataFrame\n",
                "print(df.head(2))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "780449e5-f623-4f73-a992-105bbffe00dc"
            },
            "source": [
                "The `n_tokens` column is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192. When we pass the comment to the embeddings model, it will break the comment into tokens similar (though not necessarily identical) to the examples above and then convert the tokens to a series of floating point numbers that will be accessible via vector search. These embeddings can be stored locally or in a [Azure SQL Database to support Vector Search.](https:\\learn.microsoft.com\\azure\\azure-sql\\database\\ai-artificial-intelligence-intelligent-applications?view=azuresql) \n",
                "\n",
                "As a result, each **combined**  (Product summary + Review) comment will have its own corresponding embedding vector in the new vector column on the right side of the DataFrame"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "azdata_cell_guid": "7cbd3730-2106-4124-b9a4-117acd8a3a99",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[b'nice',\n",
                            " b' t',\n",
                            " b'aff',\n",
                            " b'y',\n",
                            " b' got',\n",
                            " b' wild',\n",
                            " b' hair',\n",
                            " b' t',\n",
                            " b'aff',\n",
                            " b'y',\n",
                            " b' ordered',\n",
                            " b' five',\n",
                            " b' pound',\n",
                            " b' bag',\n",
                            " b' t',\n",
                            " b'aff',\n",
                            " b'y',\n",
                            " b' enjoyable',\n",
                            " b' many',\n",
                            " b' flavors',\n",
                            " b' water',\n",
                            " b'melon',\n",
                            " b' root',\n",
                            " b' beer',\n",
                            " b' mel',\n",
                            " b'on',\n",
                            " b' pepp',\n",
                            " b'ermint',\n",
                            " b' grape',\n",
                            " b' etc',\n",
                            " b' complaint',\n",
                            " b' bit',\n",
                            " b' much',\n",
                            " b' re',\n",
                            " b'dbl',\n",
                            " b'ack',\n",
                            " b' lic',\n",
                            " b'or',\n",
                            " b'ice',\n",
                            " b'fl',\n",
                            " b'avored',\n",
                            " b' pieces',\n",
                            " b' particular',\n",
                            " b' favorites',\n",
                            " b' kids',\n",
                            " b' husband',\n",
                            " b' lasted',\n",
                            " b' two',\n",
                            " b' weeks',\n",
                            " b' would',\n",
                            " b' recommend',\n",
                            " b' brand',\n",
                            " b' t',\n",
                            " b'aff',\n",
                            " b'y',\n",
                            " b' delightful',\n",
                            " b' treat']"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample_encode = tokenizer.encode(df.combined[5]) \n",
                "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
                "decode"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "02dd23b3-a622-429d-b680-274efad2c3b4"
            },
            "source": [
                "We will now **generate the embeddings** for the 'combined' column using the **get\\_embeddings** function we had defined earlier.  \n",
                "We now have an additional column in the dataframe called vector which has the embeddings.\n",
                "\n",
                "This will take sometime depending on the Service Tier of Azure Open AI resource you have."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "azdata_cell_guid": "a6e49745-42d0-4484-a90d-c1a9a88f9b3d",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "ename": "ConnectionError",
                    "evalue": "HTTPSConnectionPool(host='%3cyourresource%3e.openai.azure.com', port=443): Max retries exceeded with url: //openai/deployments/embedding/embeddings?api-version=2023-03-15-preview (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1D79D7810>: Failed to resolve '%3cyourresource%3e.openai.azure.com' ([Errno 11001] getaddrinfo failed)\"))",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
                        "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
                        "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001F1D79D7810>: Failed to resolve '%3cyourresource%3e.openai.azure.com' ([Errno 11001] getaddrinfo failed)",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
                        "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='%3cyourresource%3e.openai.azure.com', port=443): Max retries exceeded with url: //openai/deployments/embedding/embeddings?api-version=2023-03-15-preview (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1D79D7810>: Failed to resolve '%3cyourresource%3e.openai.azure.com' ([Errno 11001] getaddrinfo failed)\"))",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m current_batch \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Apply your function to the 'combined' column\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Append the batch results to the list\u001b[39;00m\n\u001b[0;32m     23\u001b[0m all_embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
                        "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
                        "Cell \u001b[1;32mIn[9], line 27\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Get sentence embedding using the Azure OpenAI text-embedding-small model.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m        dict: A dictionary containing the embedding.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi-key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Embed a single sentence\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
                        "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='%3cyourresource%3e.openai.azure.com', port=443): Max retries exceeded with url: //openai/deployments/embedding/embeddings?api-version=2023-03-15-preview (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F1D79D7810>: Failed to resolve '%3cyourresource%3e.openai.azure.com' ([Errno 11001] getaddrinfo failed)\"))"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Assuming you have a DataFrame 'df' with a 'combined' column\n",
                "# and you want to apply 'get_embeddings' to each batch of 100 rows\n",
                "\n",
                "batch_size = 100\n",
                "num_batches = len(df) // batch_size\n",
                "\n",
                "# Initialize an empty list to store the results\n",
                "all_embeddings = []\n",
                "\n",
                "for i in range(num_batches):\n",
                "    start_idx = i * batch_size\n",
                "    end_idx = (i + 1) * batch_size\n",
                "\n",
                "    # Get the current batch\n",
                "    current_batch = df.iloc[start_idx:end_idx]\n",
                "\n",
                "    # Apply your function to the 'combined' column\n",
                "    batch_embeddings = current_batch['combined'].apply(get_embedding)\n",
                "\n",
                "    # Append the batch results to the list\n",
                "    all_embeddings.extend(batch_embeddings)\n",
                "    print(f\"Batch {i+1} completed. Processed {end_idx} rows.\")\n",
                "\n",
                "# Create a new column 'vector' with the combined embeddings\n",
                "df['vector'] = all_embeddings\n",
                "\n",
                "# Print the updated DataFrame\n",
                "df.head(2)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f9478fe8-be9f-4ecf-b197-2b6a0bf30ff0",
                "language": "python"
            },
            "source": [
                "# Part 2 : Store & Query embeddings in Azure SQL DB."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "bd6cb5e4-8207-4003-bce4-a1d7daf28e5c"
            },
            "source": [
                "## Connect to Azure SQL Database and load the data from the dataframe into a SQL Table\n",
                "\n",
                "We will insert our vectors into the SQL Table now. The table embeddings has a column called vector which is varbinary(8000) type. Ensure you have created the table and index using the script [createtable.sql](https://github.com/Azure-Samples/azure-sql-db-vector-search/blob/bbe23e5908799fb940f8eb702a7666584c2dc4f3/VectorSearch_Notebooks/Python_Notebook_Example/createtable.sql)\n",
                "\n",
                "We will pass the vectors to the new built in function **JSON\\_ARRAY\\_TO\\_VECTOR** that will converts a JSON array to a compact **binary** representation of a vector.   Vectors are stored in an efficient binary format that also enables usage of dedicated CPU vector processing extensions like SIMD and AVX.           \n",
                "\n",
                "On that table we can create a **column store index t**o efficiently store and search for vectors. Then it is just a matter of calculating the distance between vectors to find the closest. Thanks to the internal optimization of the columnstore (that uses SIMD AVX-512 instructions to speed up vector operations) the distance calculation to find the exact nearest neighbour search is extremely fast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {
                "azdata_cell_guid": "17832bcf-7f42-45db-a4e8-dbbfac7504c4",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#lets define a function to connect to SQLDB\n",
                "\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "def get_mssql_connection():\n",
                "    # Retrieve the connection string from the environment variables\n",
                "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\n",
                "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\n",
                "\n",
                "    # Determine the authentication method and connect to the database\n",
                "    if entra_connection_string:\n",
                "        # Entra ID Service Principal Authentication\n",
                "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \n",
                "        token = credential.get_token('https://database.windows.net/.default')\n",
                "        token_bytes = token.token.encode('UTF-16LE')\n",
                "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
                "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\n",
                "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
                "    elif sql_connection_string:\n",
                "        # SQL Authentication\n",
                "        conn = pyodbc.connect(sql_connection_string)\n",
                "    else:\n",
                "        raise ValueError(\"No valid connection string found in the environment variables.\")\n",
                "\n",
                "    return conn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "00014687-c0fc-402f-be59-3720bc38011f"
            },
            "source": [
                "\n",
                "**Before we begin, please determine which path you will be following for this part of the tutorial:**\n",
                "\n",
                "- **Option A:** If you have completed Part 1 and generated embeddings using the Azure OpenAI API, you will insert the embeddings directly from the dataframe/generated output.\n",
                "\n",
                "- **Option B:** If you have not completed Part 1 or wish to use precalculated embeddings, you should download the [finefoodembeddings.csv](https:\\github.com\\azure-samples\\azure-sql-db-vector-search\\blob\\a181e15337402e568f4fc66fe5941e5973171972\\vectorsearch_notebooks\\datasets\\finefoodembeddings.csv) file and we will use this to insert data into the SQL database.\n",
                "\n",
                "**Please follow the instructions corresponding to your chosen option.**\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "d3c9e853-7d4e-484c-9226-f53d5a852fb4"
            },
            "source": [
                "## Option A: Inserting Embeddings Generated in Part 1 into the SQL Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {
                "azdata_cell_guid": "1fe3ff69-a5a4-48f4-bcae-8df068734bea",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data inserted successfully into the 'embeddings' table.\n"
                    ]
                }
            ],
            "source": [
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                " \n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                " \n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                " \n",
                "# Loop through the DataFrame rows and insert them into the table\n",
                "for index, row in df.iterrows():\n",
                "    Id = row['Id']\n",
                "    ProductId = row['ProductId']\n",
                "    UserId = row['UserId']\n",
                "    score = row['Score']\n",
                "    summary = row['Summary']\n",
                "    text = row['Text']\n",
                "    combined = row['combined']\n",
                "    vector = row['vector']\n",
                "    # Use placeholders for the parameters in the SQL query\n",
                "    query = \"\"\"\n",
                "    INSERT INTO embeddings (Id, ProductId, UserId, score, summary, text, combined, vector)\n",
                "    VALUES (?, ?, ?, ?, ?, ?, ?, JSON_ARRAY_TO_VECTOR(CAST(? AS VARCHAR(MAX))))\n",
                "    \"\"\"\n",
                "    # Execute the query with the parameters\n",
                "    cursor.execute(query, Id, ProductId, UserId, score, summary, text, combined, json.dumps(vector))\n",
                " \n",
                "# Commit the changes\n",
                "conn.commit()\n",
                " \n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'embeddings' table.\")\n",
                " \n",
                "# Close the connection\n",
                "conn.close()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "32a9799b-c990-46cf-b9c0-d9d3b88c3246"
            },
            "source": [
                "## Option B: Inserting precalculated embeddings from the CSV file\n",
                "\n",
                "You can import this [finefoodembeddings.csv](..\\Datasets\\finefoodembeddings.csv) with precalculated embeddings directly to your SQL table if you have not performed the above steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "45013e22-b118-4da9-b27a-bfabcf1ee869",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Unnamed: 0</th>\n",
                            "      <th>Id</th>\n",
                            "      <th>Time</th>\n",
                            "      <th>ProductId</th>\n",
                            "      <th>UserId</th>\n",
                            "      <th>Score</th>\n",
                            "      <th>Summary</th>\n",
                            "      <th>Text</th>\n",
                            "      <th>combined</th>\n",
                            "      <th>vector</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1303862400</td>\n",
                            "      <td>B001E4KFG0</td>\n",
                            "      <td>A3SGXH7AUHU8GW</td>\n",
                            "      <td>5</td>\n",
                            "      <td>Good Quality Dog Food</td>\n",
                            "      <td>I have bought several of the Vitality canned d...</td>\n",
                            "      <td>Title: Good Quality Dog Food; Content: I have ...</td>\n",
                            "      <td>[0.02088439, -0.00022463033, -0.0019172364, -0...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1346976000</td>\n",
                            "      <td>B00813GRG4</td>\n",
                            "      <td>A1D87F6ZCVE5NK</td>\n",
                            "      <td>1</td>\n",
                            "      <td>Not as Advertised</td>\n",
                            "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
                            "      <td>Title: Not as Advertised; Content: Product arr...</td>\n",
                            "      <td>[-0.0044591213, 0.00078397157, -0.022424141, 0...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   Unnamed: 0  Id        Time   ProductId          UserId  Score  \\\n",
                            "0           0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5   \n",
                            "1           1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1   \n",
                            "\n",
                            "                 Summary                                               Text  \\\n",
                            "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
                            "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
                            "\n",
                            "                                            combined  \\\n",
                            "0  Title: Good Quality Dog Food; Content: I have ...   \n",
                            "1  Title: Not as Advertised; Content: Product arr...   \n",
                            "\n",
                            "                                              vector  \n",
                            "0  [0.02088439, -0.00022463033, -0.0019172364, -0...  \n",
                            "1  [-0.0044591213, 0.00078397157, -0.022424141, 0...  "
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# load & inspect dataset\n",
                "df = pd.read_csv(r\"../Datasets/FineFoodEmbeddings.csv\")\n",
                "df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {
                "azdata_cell_guid": "b54b85a0-4966-43f5-b26c-984145cf9f32",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data inserted successfully into the 'embeddings' table.\n"
                    ]
                }
            ],
            "source": [
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                "\n",
                "# Assuming 'df' is your DataFrame and it's already defined\n",
                "# Prepare your data to be inserted as a list of tuples\n",
                "data_to_insert = [\n",
                "    (\n",
                "        row['Id'],\n",
                "        row['ProductId'],\n",
                "        row['UserId'],\n",
                "        row['Score'],\n",
                "        row['Summary'],\n",
                "        row['Text'],\n",
                "        row['combined'],\n",
                "        row['vector']\n",
                "    ) for index, row in df.iterrows()\n",
                "]\n",
                "\n",
                "# Define your SQL insert query. \n",
                "#We are using the JSON_ARRAY_TO_VECTOR function here which  converts the jsonarray to a compact binary representation of the vector\n",
                "insert_query = \"\"\"\n",
                "INSERT INTO embeddings (Id, ProductId, UserId, score, summary, text, combined, vector)\n",
                "VALUES (?, ?, ?, ?, ?, ?, ?, JSON_ARRAY_TO_VECTOR(CAST(? AS VARCHAR(MAX))))\n",
                "\"\"\"\n",
                "\n",
                "# Execute batch insert\n",
                "cursor.executemany(insert_query, data_to_insert)\n",
                "\n",
                "# Commit the transaction\n",
                "conn.commit()\n",
                "\n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'embeddings' table.\")\n",
                "\n",
                "# Close the cursor and connection\n",
                "cursor.close()\n",
                "conn.close()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "0f8850d8-8e41-4a43-a046-e796bcd11fdf"
            },
            "source": [
                "## Using the native vector functions in SQL DB\n",
                "\n",
                "Lets take a look at how the vector is stored in the SQL DB table & also make use of the newly introduced helper functions\n",
                "\n",
                "**ISVECTOR** Checks if the provided object is a valid vector: Returns 1 if valid, otherwise returns 0. Returns NULL if the expression is NULL\n",
                "\n",
                "**VECTOR\\_DIMENSIONS** Takes a vector as an input and returns the number of dimensions as an output. In this case we see the number of dimensions of the vector are 1536 (as we are using Azure OpenAI text embeddings)\n",
                "\n",
                "**VECTOR\\_TO\\_JSON\\_ARRAY** Converts a vector in a compact binary format to a human-readable string format. The string format is the same as the one used by JSON to represent arrays"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "azdata_cell_guid": "37251f52-1d9b-4ed7-8404-67eb9dd88d69",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------+------------+----------------------+----------------------+----------------------+------------+\n",
                        "| isvector | dimensions |       summary        |        vector        |      jsonvector      | ProductId  |\n",
                        "+----------+------------+----------------------+----------------------+----------------------+------------+\n",
                        "|    1     |    1536    | Good Quality Dog Foo | b'\\xa9\\x01\\x00\\x06\\x | [0.0208843909204006, | B001E4KFG0 |\n",
                        "|    1     |    1536    | \"Delight\" says it al | b'\\xa9\\x01\\x00\\x06\\x | [0.0197708476334810, | B000LQOCH0 |\n",
                        "|    1     |    1536    |     Good Instant     | b'\\xa9\\x01\\x00\\x06\\x | [0.0118939895182848, | B001EO5QW8 |\n",
                        "|    1     |    1536    | My Cats Are Not Fans | b'\\xa9\\x01\\x00\\x06\\x | [0.0110512115061283, | B0009XLVG0 |\n",
                        "|    1     |    1536    |  Delicious product!  | b'\\xa9\\x01\\x00\\x06\\x | [0.0398664325475693, | B001GVISJM |\n",
                        "|    1     |    1536    |      satisfying      | b'\\xa9\\x01\\x00\\x06\\x | [-0.0110548865050077 | B001EO5QW8 |\n",
                        "|    1     |    1536    |  Good Hot Breakfast  | b'\\xa9\\x01\\x00\\x06\\x | [0.0008212000248022, | B001EO5QW8 |\n",
                        "|    1     |    1536    | HOT!  And good!  Cam | b'\\xa9\\x01\\x00\\x06\\x | [-0.0062388749793172 | B000G6RPMY |\n",
                        "|    1     |    1536    |     great deal.      | b'\\xa9\\x01\\x00\\x06\\x | [0.0372267924249172, | B004N5KULM |\n",
                        "|    1     |    1536    | How much would you p | b'\\xa9\\x01\\x00\\x06\\x | [0.0322386324405670, | B000E7VI7S |\n",
                        "+----------+------------+----------------------+----------------------+----------------------+------------+\n"
                    ]
                }
            ],
            "source": [
                "from prettytable import PrettyTable\n",
                "\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "from azure.identity import DefaultAzureCredential\n",
                "from azure import identity\n",
                "import struct\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the connection string from the environment variables\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Use placeholders for the parameters in the SQL query\n",
                "query = \"SELECT TOP(10) ISVECTOR(vector) as isvector, VECTOR_DIMENSIONS(vector) as dimensions , summary , vector , VECTOR_TO_JSON_ARRAY(vector) as jsonvector,  ProductId FROM dbo.embeddings\"\n",
                "\n",
                "# Execute the query with the parameters\n",
                "cursor.execute(query)\n",
                "queryresults = cursor.fetchall()\n",
                "\n",
                "# Get column names from cursor.description\n",
                "column_names = [column[0] for column in cursor.description]\n",
                "\n",
                "# Create a PrettyTable object\n",
                "table = PrettyTable()\n",
                "\n",
                "# Add column names to the table\n",
                "table.field_names = column_names\n",
                "\n",
                "# Set max width for each column to truncate data\n",
                "table.max_width = 20\n",
                "\n",
                "# Add rows to the table\n",
                "for row in queryresults:\n",
                "    # Truncate each value to 20 characters\n",
                "    truncated_row = [str(value)[:20] for value in row]\n",
                "    table.add_row(truncated_row)\n",
                "\n",
                "# Print the table\n",
                "print(table)\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "# Close the connection\n",
                "conn.close()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "356e7a68-c999-42de-9fe5-12f5f18d0f20",
                "language": "python"
            },
            "source": [
                "## Performing Vector Similarity Search in Azure SQL DB\n",
                "\n",
                "Let's now query our embedding table to get the top similar reviews given the User search query.\n",
                "\n",
                "What we are doing: Given any user search query, we can get the vector representation of that text.\n",
                "\n",
                "Then we can use that vector to calculate the cosine distance against all the customer review comments stored in the database and take only the closest ones which will return the product most likely connected to the product we are interested in. The reviews with the highest similarity are considered the most relevant to the query, helping users discover products or experiences related to their search.\n",
                "\n",
                "The most common distance is the cosine similarity, which can be calculated quite easily in SQL with the help of the new distance functions.\n",
                "\n",
                "```\n",
                "VECTOR_DISTANCE('distance metric', V1, V2)\n",
                "```\n",
                "\n",
                "We can use **cosine**, **euclidean**, and **dot** as the distance metric today.\n",
                "\n",
                "We will define the function `vector_search_sql`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "dc8b6a89-2894-4607-9549-a8502af81a2c",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "def vector_search_sql(query, num_results=5):\n",
                "    # Load environment variables from .env file\n",
                "    load_dotenv()\n",
                "\n",
                "    #Use the get_mssql_connection function to get the connection string details\n",
                "    conn = get_mssql_connection()\n",
                "\n",
                "    # Create a cursor object\n",
                "    cursor = conn.cursor()\n",
                "\n",
                "    # Generate the query embedding for the user's search query\n",
                "    user_query_embedding = get_embedding(query)\n",
                "\n",
                "    # Convert user_query_embedding to a JSON string\n",
                "    user_query_embedding_json = json.dumps(user_query_embedding)\n",
                "\n",
                "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\n",
                "    sql_similarity_search = \"\"\"\n",
                "    SELECT TOP(?) ProductId, Summary, text,\n",
                "           1 - vector_distance('cosine', JSON_ARRAY_TO_VECTOR(cast(? as varchar(max))), [vector]) AS similarity_score\n",
                "    FROM dbo.embeddings\n",
                "    ORDER BY similarity_score desc\n",
                "    \"\"\"\n",
                "\n",
                "    cursor.execute(sql_similarity_search, (num_results, user_query_embedding_json))\n",
                "    results = cursor.fetchall()\n",
                "\n",
                "    # Close the database connection\n",
                "    conn.close()\n",
                "\n",
                "    return results\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {
                "azdata_cell_guid": "5d77335d-3540-44bf-9e24-aa7037feddb0",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Product ID: B001DIM8K8\n",
                        "summary : Nothing like the crappy stuff I grew up on.\n",
                        "Text : My daughter and I eat low-carb. Which oatmeal is not really, by the way. But I was looking for something that we could eat in the morning that was not meat/dairy/egg, and was gluten-free, and filling, without being pure sugar like most breakfast foods and especially grains. I decided to try these long-cooking irish oats. We use 2 cups water and 1/2 cup dry oats, for 1/4cup dry each. It takes this stuff 30 minutes+ to cook so it is not fast food, for sure. But strangely enough, it's filling. Adding fats (couple tablespoons of cream) to the serving helps that of course. In any case, it is much tastier, and much more long-lasting for keeping us satisfied, than I expected, and surprisingly stable on blood sugar. Nor has it set off cravings like most grains, fruits, etc. do if we eat those; the sugars digest so slowly in this apparently. It is more solid and nutty than the flakey quick-oats I grew up with, which I now consider a gummy gluey sugarbomb insult to true oats. These are harder to find and cost more but they're worth it.\n",
                        "Similarity Score: 0.5673871961458196\n",
                        "\n",
                        "Product ID: B001EO5QW8\n",
                        "summary : it's oatmeal\n",
                        "Text : What else do you need to know? Oatmeal, instant (make it with a half cup of low-fat milk and add raisins;nuke for 90 seconds). More expensive than Kroger store brand oatmeal and maybe a little tastier or better texture or something. It's still just oatmeal. Mmm, convenient!\n",
                        "Similarity Score: 0.5668513542501259\n",
                        "\n",
                        "Product ID: B001DIM8K8\n",
                        "summary : Texture and the taste are totally different from the pressed oats!\n",
                        "Text : I have been having Quaker Oatmeal for yesrs until trying this today! The texture and the taste are completely different from the pressed oat. It worths cooking for half hour. My son love it at the first bite! My 8-month-old baby is also love it! I uaually cook the portion which will be enough for 2~3 days. Drop one spoon or two into the worm milk every morning to make my breakfast much more healthier! This one is so good that it will be hard for me to go back to pressed oatmeal.....\n",
                        "Similarity Score: 0.5633615577032233\n",
                        "\n",
                        "Product ID: B001DIM8K8\n",
                        "summary : Slow cooking\n",
                        "Text : I'll be honest, if you want a quick breakfast don't purchase this oatmeal. But, if you have time to spare or don't mind waiting for some \"good eats\", this is the oatmeal for you.<br /><br />Step 1. pour some milk into a sauce pan; Step 2. add a dash of salt [or salt to taste]; Step 3. add oatmeal, making sure it stays covered by milk; Step 4. turn on stove to low heat [not too low but definitely not too high, as you don't want the milk to evaporate/dry out leaving you with tough oats in pan] and cover pan; Step 5. do some little cleaning to built up appetite making sure to ck on progress periodically; Step 6. add more milk if oats are still too tough and let it simmer for a little while longer; Step 7. get your ladle/just pour directing into a bowl add a few raisins/cranberries/your favorite dried fruit and ENJOY!\n",
                        "Similarity Score: 0.5528843265139003\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Assuming you have implemented the `vector_search_sql` function as shown earlier\n",
                "\n",
                "# Example usage\n",
                "query = \"oatmeal options for my toddler\"\n",
                "num_results = 4\n",
                "search_results = vector_search_sql(query, num_results)\n",
                "for result in search_results:\n",
                "    product_id = result[0]  # Assuming ProductId is the first column\n",
                "    summary = result [1]\n",
                "    text = result [2]\n",
                "    similarity_score = result[3]  # Assuming similarity_score is the third column\n",
                "    \n",
                "    print(f\"Product ID: {product_id}\")\n",
                "    print(f\"summary : {summary}\")\n",
                "    print (f\"Text : {text}\")\n",
                "    print(f\"Similarity Score: {similarity_score}\\n\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "a4919ceb-ae7c-415f-9872-4fc6b4e11c99"
            },
            "source": [
                "# Part 3 - Use embeddings retrieved from a vector database to augment LLM generation\n",
                "\n",
                "Lets create a helper function to feed prompts into the [Completions model](https:\\learn.microsoft.com\\en-us\\azure\\ai-services\\openai\\concepts\\models#gpt-35-models) & create interactive loop where you can pose questions to the model and receive information grounded in your data.\n",
                "\n",
                "The function `generate_completion` is defined to help ground the GPT3.5 model with prompts and system instructions.   \n",
                "Note that we are passing the results of the `vector_search_sql` we defined earlier to the model and we define the system prompt .  \n",
                "We are using gpt-35-turbo-16k model here. \n",
                "\n",
                "You can get more information on using Azure Open AI GPT chat models [here](https:\\learn.microsoft.com\\en-us\\azure\\ai-services\\openai\\chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {
                "azdata_cell_guid": "d2cf8d04-8323-4024-996c-f9e0acd1ef93",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the API key and endpoint from the environment variables\n",
                "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
                "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
                "\n",
                "# Create a chat completion request\n",
                "client = AzureOpenAI(\n",
                "    api_key=api_key,\n",
                "    api_version=\"2023-05-15\",\n",
                "    azure_endpoint=azure_endpoint\n",
                ")\n",
                "\n",
                "\n",
                "def generate_completion(search_results, user_input):\n",
                "    system_prompt = '''\n",
                "You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\n",
                "- Use the information from `search_results` to generate your responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\n",
                "  Product ID: {product_id}\n",
                "  Summary: {summary}\n",
                "  Review: {text}\n",
                "  Similarity Score: {similarity_score}\n",
                "- Avoid any other external data sources.\n",
                "- Add a fun fact related to the overall product searched at the end of the recommendations.\n",
                "'''\n",
                "\n",
                "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
                "    search_results = vector_search_sql(user_input, num_results)\n",
                "    \n",
                "    # Create an empty list to store the results\n",
                "    result_list = []\n",
                "\n",
                "    # Iterate through the search results and append relevant information to the list\n",
                "    for result in search_results:\n",
                "        product_id = result[0]  # Assuming ProductId is the first column\n",
                "        summary = result[1]\n",
                "        text = result[2]\n",
                "        similarity_score = result[3]  # Assuming similarity_score is the third column\n",
                "        \n",
                "        # Append the relevant information as a dictionary to the result_list\n",
                "        result_list.append({\n",
                "            \"product_id\": product_id,\n",
                "            \"summary\": summary,\n",
                "            \"text\": text,\n",
                "            \"similarity_score\": similarity_score\n",
                "        })\n",
                "\n",
                "    #print (result_list)\n",
                "    messages.append({\"role\": \"system\", \"content\": f\"{result_list}\"})\n",
                "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
                "    response = client.chat.completions.create(model='chatcompletion', messages=messages, temperature=0) #replace with your model deployment name\n",
                "\n",
                "    return response.dict()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {
                "azdata_cell_guid": "57c6ba6b-8d63-4c21-a9d2-af7fec6d80f9",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "*** What products are you looking for? Ask me & I can help you :) Type 'end' to end the session.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "User asked: I want some healthy options for cat food. Mind you my cat is super fussy and doesnt like stuff easily\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "AI's response:\n",
                        "I understand that your cat is a picky eater, but I have some healthy options for you to consider based on the reviews:\n",
                        "\n",
                        "1. Product ID: B002PNGY28\n",
                        "   Summary: Great soft little treats for your finicky feline\n",
                        "   Review: I have 2 cats. One will eat anything. The other one is soooo picky. She's had to have some of her teeth removed due to a mouth virus, so I needed to find some good soft treats. She likes these more than anything I've tried. They are really thin little tiny strips. I like Wellness, all natural products. Both cats like their brand cat food as well.\n",
                        "   Similarity Score: 0.651522667741025\n",
                        "\n",
                        "2. Product ID: B003SE19UK\n",
                        "   Summary: Palatable and healthy\n",
                        "   Review: Before I was educated about feline nutrition, I allowed my cats to become addicted to dry cat food. I always offered both canned and dry, but wish I would have fed them premium quality canned food and limited dry food. I have two 15-year-old cats and two 5-year-old cats. The only good quality dry foods they will eat are Wellness and Innova. Innova's manufacturer was recently purchased by Procter&Gamble. I began looking for a replacement. After once again offering several samples (from my local holistic pet store) Holistic Select was the only one (other than the usual Wellness and Innova) they would eat. For finicky cats, I recommend trying Holistic Select. It is a good quality food that is very palatable for finicky eaters.\n",
                        "   Similarity Score: 0.6401906033145333\n",
                        "\n",
                        "3. Product ID: B000N5Z5RU\n",
                        "   Summary: My Picky Cat-Child Loves It\n",
                        "   Review: My youngest cat is very particular about what he eats. He absolutely will not eat if something doesn't meet his expectations. Between that and the recent pet food recalls, I've had a really hard time finding cat food that I can feed him. I'm happy to say that the Merrick Turducken clicks with him, plus it has no wheat gluten or other fillers. The texture of it is appealing, too... it's not just \"mush\" or Spam-looking like many foods. I'll continue to buy Turducken and plan to also try out some of the other Merrick flavors.\n",
                        "   Similarity Score: 0.6269133034740086\n",
                        "\n",
                        "4. Product ID: B00180O980\n",
                        "   Summary: Economical and my cat loves it\n",
                        "   Review: Cats are finicky animals, and what one loves, another won't touch - that is not a reflection on the food itself. I have two Burmese, and their eating habits are very different. My local Pet Nutrition Center kindly gave me a sample of Prowl the other day, and while my one cat absolutely loves it, the other won't touch it. That's why samples are nice. Both of my cats have been on Evo kibble, but this one cat who now loves the Prowl has a habit of throwing up, and I'm on the prowl myself for an alternative food that will help her keep food down. So far, no barf. She also laps it up as soon as I've added warm water - no waiting. I highly recommend you obtain a sample and let the cat decide, as the food is economical as well as healthy.\n",
                        "   Similarity Score: 0.615546735849085\n",
                        "\n",
                        "Fun Fact: Did you know that cats have a specialized protein taste receptor that makes them unable to taste sweetness? That's why they are not interested in sugary foods!\n"
                    ]
                }
            ],
            "source": [
                "# Create a loop of user input and model output to perform Q&A on the FineFoods Sample data\n",
                "\n",
                "print(\"*** What products are you looking for? Ask me & I can help you :) Type 'end' to end the session.\\n\")\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"User prompt: \")\n",
                "    if user_input.lower() == \"end\":\n",
                "        break\n",
                "\n",
                "    # Print the user's question\n",
                "    print(f\"\\nUser asked: {user_input}\")\n",
                "\n",
                "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\n",
                "    search_results = vector_search_sql(user_input)\n",
                "    completions_results = generate_completion(search_results, user_input)\n",
                "\n",
                "    # Print the model's response\n",
                "    print(\"\\nAI's response:\")\n",
                "    print(completions_results['choices'][0]['message']['content'])\n",
                "\n",
                "# The loop will continue until the user types 'end'\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "d1ac7c15-bbda-4365-918b-a99c1a0822d3"
            },
            "source": [
                "You can use also use Semantic Search with Keyword search in SQL DB along with using filters and aggregations . \n",
                "Check out the Hybrid samples in the same repo"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
