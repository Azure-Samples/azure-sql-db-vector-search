{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "# Create, Store, and Query OpenAI Embeddings in Azure SQL DB\n",
                "\n",
                "Learn how to integrate Azure OpenAI API with Azure SQL DB to create, store, and query embeddings for advanced similarity searches and LLM generation augmentation.\n",
                "\n",
                "## Tutorial Overview\n",
                "\n",
                "This Python [notebook](EmbeddingsWithSQL.ipynb) will teach you to:\n",
                "\n",
                "- **Create Embeddings**: Generate embeddings from content using the Azure OpenAI API.\n",
                "- **Vector Database Utilization**: Use Azure SQL DB to store embeddings and perform similarity searches.\n",
                "- **LLM Generation Augmentation**: Enhance language model generation with embeddings from a vector database. In this case we use the embeddings to inform a GPT-4 chat model, enabling it to provide rich, context-aware answers about products based on past customer reviews.\n",
                "\n",
                "\n",
                "\n",
                "## Dataset\n",
                "We use the Fine Foods Review Dataset from Kaggle, which contains Amazon reviews of fine foods. \n",
                "\n",
                "- For simplicity, this tutorial uses a smaller sample [Fine Foods Review Dataset](https://github.com/Azure-Samples/azure-sql-db-vector-search/blob/a181e15337402e568f4fc66fe5941e5973171972/VectorSearch_Notebooks/Datasets/Reviews.csv) to demonstrate embedding generation. \n",
                "- Alternatively, if **you to wish bypass embedding generation** and jump straight to similarity search in SQLDB. you can download the pre-generated [finefoodembeddings.csv](https://github.com/Azure-Samples/azure-sql-db-vector-search/blob/a181e15337402e568f4fc66fe5941e5973171972/VectorSearch_Notebooks/Datasets/finefoodembeddings.csv) \n",
                "\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- **Azure Subscription**: [Create one for free](https:\\azure.microsoft.com\\free\\cognitive-services?azure-portal=true)\n",
                "- **Azure SQL Database**: [Set up your database for free](https:\\learn.microsoft.com\\azure\\azure-sql\\database\\free-offer?view=azuresql)\n",
                "- **Azure Data Studio**: Download [here](https://azure.microsoft.com/products/data-studio) to manage your Azure SQL database and [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel)\n",
                "\n",
                "\n",
                "## Additional Requirements for Embedding Generation\n",
                "\n",
                "- **Azure OpenAI Access**: Apply for access in the desired Azure subscription at [https://aka.ms/oai/access](https:\\aka.ms\\oai\\access)\n",
                "- **Azure OpenAI Resource**: Deploy an embeddings model (e.g., `text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Refer to the [resource deployment guide](https:\\learn.microsoft.com\\azure\\ai-services\\openai\\how-to\\create-resource)\n",
                "- **Python**: Version 3.7.1 or later from Python.org. (Sample has been tested with Python 3.11)\n",
                "- **Python Libraries**: Install the required libraries openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken, and pyodbc.\n",
                "- **Jupyter Notebooks**: Use within [Azure Data Studio](https:\\learn.microsoft.com\\en-us\\azure-data-studio\\notebooks\\notebooks-guidance) or Visual Studio Code .\n",
                "\n",
                "Code snippets are adapted from the [Azure OpenAI Service embeddings Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python)\n",
                "\n",
                "## Getting Started\n",
                "\n",
                "1. **Database Setup**: Execute SQL commands from the `createtable.sql` script to create the necessary table in your database.\n",
                "2. **Model Deployment**: Deploy an embeddings model (`text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Note the 2 models deployment names for later use.\n",
                "\n",
                "![Deployed OpenAI Models](../Assets/modeldeployment.png)\n",
                "\n",
                "3. **Connection String**: Find your Azure SQL DB connection string in the Azure portal under your database settings.\n",
                "4. **Configuration**: Populate the `.env` file with your SQL server connection details , Azure OpenAI key, and endpoint values. \n",
                "\n",
                "You can retrieve the Azure OpenAI *endpoint* and *key*:\n",
                "\n",
                "![Azure OpenAI Endpoint and Key](../Assets/endpoint.png)\n",
                "\n",
                "## Running the Notebook\n",
                "\n",
                "To [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel), connect to your Azure SQL database using Azure Data Studio, which can be downloaded [here](https://azure.microsoft.com/products/data-studio)\n"
            ],
            "metadata": {
                "azdata_cell_guid": "a56b4788-b5d4-4314-82b6-9aa5c0bd4c81"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Part 1 : Create Embeddings from Content Using the Azure OpenAI API\n",
                "\n",
                "This section guides you through the process of generating embeddings from your content using the Azure OpenAI API\n",
                "\n",
                "**NOTE**: If you prefer to bypass the embedding generation and proceed directly to similarity search in SQLDB, please move on to **Part 2: Load and Store Embeddings in Azure SQL DB**."
            ],
            "metadata": {
                "azdata_cell_guid": "b0b8ed3a-2313-469c-a038-4ea72214bf97",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#Setup the python libraries required for this notebook\n",
                "#Please ensure that you navigate to the directory containing the `requirements.txt` file in your terminal\n",
                "%pip install -r requirements.txt"
            ],
            "metadata": {
                "azdata_cell_guid": "59dd0902-a737-4867-b4a3-11c5396a1fad",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#Load the env details\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()"
            ],
            "metadata": {
                "azdata_cell_guid": "ab896eec-22dd-4a2d-ad5e-eab1d662fb52",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let us define the function to generate text embeddings from the Azure Open AI `text-embedding-small` model.\n",
                "\n",
                "An **embedding** is a special format of data representation that is optimized for use by machine learning models and algorithms. It is an information-dense representation of the semantic meaning of a piece of text.\n",
                "\n",
                "Each embedding is a vector of floating point numbers. The key characteristic of these vectors is that the distance between two embeddings in the vector space is indicative of the semantic similarity between the two corresponding inputs in their original format. For instance:\n",
                "\n",
                "- If two pieces of text are semantically similar, their vector representations will also be close to each other.\n",
                "- Conversely, dissimilar texts will have embeddings that are farther apart in the vector space."
            ],
            "metadata": {
                "azdata_cell_guid": "ef9509c5-f27a-44ec-ade2-a2f8902e18c1",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import requests\n",
                "import sys\n",
                "from num2words import num2words\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import tiktoken\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Specify you model name\n",
                "openai_embedding_model = \"<yourdeployedmodelname>\"\n",
                "\n",
                "# Assuming openai_url and openai_key are environment variables\n",
                "openai_url = os.environ.get('AZURE_OPENAI_ENDPOINT') + f\"/openai/deployments/{openai_embedding_model}/embeddings?api-version=2023-03-15-preview\"\n",
                "openai_key = os.environ.get('AZURE_OPENAI_API_KEY')\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\n",
                "\n",
                "    Args:\n",
                "        text (str): Text to embed.\n",
                "\n",
                "    Returns:\n",
                "        dict: A dictionary containing the embedding.\n",
                "    \"\"\"\n",
                "    response = requests.post(openai_url,\n",
                "        headers={\"api-key\": openai_key, \"Content-Type\": \"application/json\"},\n",
                "        json={\"input\": [text]}  # Embed a single sentence\n",
                "    )\n",
                "    embedding = response.json()['data'][0]['embedding']\n",
                "    return embedding"
            ],
            "metadata": {
                "azdata_cell_guid": "5357f9e6-1a24-474a-b3fc-147441e9b6c8",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "In this tutorial we will be using the Fine Foods Review Dataset. This dataset consists of reviews of fine foods from Amazon.\n",
                "\n",
                "Now we need to read our [Reviews.csv](https:\\github.com\\Azure-Samples\\azure-sql-db-vector-search\\blob\\a181e15337402e568f4fc66fe5941e5973171972\\VectorSearch_Notebooks\\Datasets\\Reviews.csv) file and create a pandas DataFrame. After the initial DataFrame is created, we can view the contents of the table by running the below. For the purpose of the quick embedding generation tutorial we will only use nrows = 500."
            ],
            "metadata": {
                "azdata_cell_guid": "0343bba0-01ea-4537-a793-adaadb904781"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# load & inspect dataset\n",
                "df = pd.read_csv(r\"../Datasets/Reviews.csv\", nrows=500)\n",
                "df = df[[\"Id\" , \"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
                "df = df.dropna()\n",
                "df[\"combined\"] = (\n",
                "    df.Summary.str.strip() + \": \" + df.Text.str.strip()\n",
                ")\n",
                "df.head(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "f6815045-f75d-4bba-960f-f76818baa592",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "   Id        Time   ProductId          UserId  Score  \\\n0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5   \n1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1   \n2   3  1219017600  B000LQOCH0   ABXLMWJIXXAIN      4   \n3   4  1307923200  B000UA0QIQ  A395BORC6FGVXV      2   \n4   5  1350777600  B006K2ZZ7K  A1UQRSCLF8GW1T      5   \n5   6  1342051200  B006K2ZZ7K   ADT0SRK1MGOEU      4   \n6   7  1340150400  B006K2ZZ7K  A1SP2KVKFXXRU1      5   \n7   8  1336003200  B006K2ZZ7K  A3JRGQVEQN31IQ      5   \n8   9  1322006400  B000E7L2R4  A1MZYO9TZK0BBI      5   \n9  10  1351209600  B00171APVA  A21BT40VZCCYT4      5   \n\n                                         Summary  \\\n0                          Good Quality Dog Food   \n1                              Not as Advertised   \n2                          \"Delight\" says it all   \n3                                 Cough Medicine   \n4                                    Great taffy   \n5                                     Nice Taffy   \n6  Great!  Just as good as the expensive brands!   \n7                         Wonderful, tasty taffy   \n8                                     Yay Barley   \n9                               Healthy Dog Food   \n\n                                                Text  \\\n0  I have bought several of the Vitality canned d...   \n1  Product arrived labeled as Jumbo Salted Peanut...   \n2  This is a confection that has been around a fe...   \n3  If you are looking for the secret ingredient i...   \n4  Great taffy at a great price.  There was a wid...   \n5  I got a wild hair for taffy and ordered this f...   \n6  This saltwater taffy had great flavors and was...   \n7  This taffy is so good.  It is very soft and ch...   \n8  Right now I'm mostly just sprouting this so my...   \n9  This is a very healthy dog food. Good for thei...   \n\n                                            combined  \n0  Good Quality Dog Food: I have bought several o...  \n1  Not as Advertised: Product arrived labeled as ...  \n2  \"Delight\" says it all: This is a confection th...  \n3  Cough Medicine: If you are looking for the sec...  \n4  Great taffy: Great taffy at a great price.  Th...  \n5  Nice Taffy: I got a wild hair for taffy and or...  \n6  Great!  Just as good as the expensive brands!:...  \n7  Wonderful, tasty taffy: This taffy is so good....  \n8  Yay Barley: Right now I'm mostly just sproutin...  \n9  Healthy Dog Food: This is a very healthy dog f...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Time</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>Score</th>\n      <th>Summary</th>\n      <th>Text</th>\n      <th>combined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1303862400</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>5</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n      <td>Good Quality Dog Food: I have bought several o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1346976000</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>1</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>Not as Advertised: Product arrived labeled as ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1219017600</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>4</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n      <td>\"Delight\" says it all: This is a confection th...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1307923200</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>2</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n      <td>Cough Medicine: If you are looking for the sec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>5</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n      <td>Great taffy: Great taffy at a great price.  Th...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1342051200</td>\n      <td>B006K2ZZ7K</td>\n      <td>ADT0SRK1MGOEU</td>\n      <td>4</td>\n      <td>Nice Taffy</td>\n      <td>I got a wild hair for taffy and ordered this f...</td>\n      <td>Nice Taffy: I got a wild hair for taffy and or...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1340150400</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1SP2KVKFXXRU1</td>\n      <td>5</td>\n      <td>Great!  Just as good as the expensive brands!</td>\n      <td>This saltwater taffy had great flavors and was...</td>\n      <td>Great!  Just as good as the expensive brands!:...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1336003200</td>\n      <td>B006K2ZZ7K</td>\n      <td>A3JRGQVEQN31IQ</td>\n      <td>5</td>\n      <td>Wonderful, tasty taffy</td>\n      <td>This taffy is so good.  It is very soft and ch...</td>\n      <td>Wonderful, tasty taffy: This taffy is so good....</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1322006400</td>\n      <td>B000E7L2R4</td>\n      <td>A1MZYO9TZK0BBI</td>\n      <td>5</td>\n      <td>Yay Barley</td>\n      <td>Right now I'm mostly just sprouting this so my...</td>\n      <td>Yay Barley: Right now I'm mostly just sproutin...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1351209600</td>\n      <td>B00171APVA</td>\n      <td>A21BT40VZCCYT4</td>\n      <td>5</td>\n      <td>Healthy Dog Food</td>\n      <td>This is a very healthy dog food. Good for thei...</td>\n      <td>Healthy Dog Food: This is a very healthy dog f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "Next we'll perform some light data cleaning by removing redundant whitespace and cleaning up the punctuation to prepare the data for tokenization. We will also remove comments that are too long for the token limit (8192 tokens)"
            ],
            "metadata": {
                "azdata_cell_guid": "027376ed-edc8-49ab-a361-a9aea8781b77",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import tiktoken\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "import string\n",
                "\n",
                "# Remove null values\n",
                "df.dropna(subset=['combined'], inplace=True)\n",
                "\n",
                "# Convert to lowercase\n",
                "df['combined'] = df['combined'].str.lower()\n",
                "\n",
                "# Remove accented letters\n",
                "df['combined'] = df['combined'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
                "\n",
                "# Remove punctuation marks\n",
                "translator = str.maketrans('', '', string.punctuation)\n",
                "df['combined'] = df['combined'].apply(lambda x: x.translate(translator))\n",
                "\n",
                "# Remove redundant white space\n",
                "df['combined'] = df['combined'].str.strip()\n",
                "\n",
                "# Remove stopwords using NLTK\n",
                "nltk.download('stopwords')\n",
                "stop_words = set(stopwords.words('english'))\n",
                "df['combined'] = df['combined'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
                "\n",
                "# Remove Unicode characters (like emojis)\n",
                "df['combined'] = df['combined'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
                "\n",
                "# Tokenize using tiktoken\n",
                "# Assuming you have already imported tiktoken and loaded the tokenizer\n",
                "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "\n",
                "# Convert the list of tokens to a single integer (assuming each list contains token IDs)\n",
                "df['n_tokens'] = df['combined'].apply(lambda x: len(tokenizer.encode(x)))\n",
                "\n",
                "# Filter rows where 'n_tokens' is less than 8000\n",
                "df = df[df['n_tokens'] < 8000]\n",
                "\n",
                "# Print the modified DataFrame\n",
                "print(df.head(2))\n"
            ],
            "metadata": {
                "azdata_cell_guid": "3146e381-1ff7-4932-a457-60564daf1b7f",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\pookam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "   Id        Time   ProductId          UserId  Score                Summary  \\\n0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5  Good Quality Dog Food   \n1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1      Not as Advertised   \n\n                                                Text  \\\n0  I have bought several of the Vitality canned d...   \n1  Product arrived labeled as Jumbo Salted Peanut...   \n\n                                            combined  n_tokens  \n0  good quality dog food bought several vitality ...        30  \n1  advertised product arrived labeled jumbo salte...        26  \n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "The `n_tokens` column is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds 8,191 - the maximum length of input text for the Azure OpenAI embedding models.\n",
                "When faced with content that exceeds the embedding limit, you can also chunk the content into smaller pieces and then embed those one at a time. You can read more about data chunking [here](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents)  \n",
                "\n",
                "When we pass the comment to the embeddings model, it will break the comment into tokens similar (though not necessarily identical) to the examples above and then convert the tokens to a series of floating point numbers that will be accessible via vector search. These embeddings can be stored locally or in a [Azure SQL Database to support Vector Search.](https://learn.microsoft.com/azure/azure-sql/database/ai-artificial-intelligence-intelligent-applications?view=azuresql&preserve-view=true#vector-search) \n",
                "\n",
                "As a result, each **combined** (Product summary + Review) comment will have its own corresponding embedding vector in the new vector column on the right side of the DataFrame"
            ],
            "metadata": {
                "azdata_cell_guid": "780449e5-f623-4f73-a992-105bbffe00dc"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "sample_encode = tokenizer.encode(df.combined[5]) \n",
                "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
                "decode"
            ],
            "metadata": {
                "azdata_cell_guid": "7cbd3730-2106-4124-b9a4-117acd8a3a99",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "[b'nice',\n b' t',\n b'aff',\n b'y',\n b' got',\n b' wild',\n b' hair',\n b' t',\n b'aff',\n b'y',\n b' ordered',\n b' five',\n b' pound',\n b' bag',\n b' t',\n b'aff',\n b'y',\n b' enjoyable',\n b' many',\n b' flavors',\n b' water',\n b'melon',\n b' root',\n b' beer',\n b' mel',\n b'on',\n b' pepp',\n b'ermint',\n b' grape',\n b' etc',\n b' complaint',\n b' bit',\n b' much',\n b' re',\n b'dbl',\n b'ack',\n b' lic',\n b'or',\n b'ice',\n b'fl',\n b'avored',\n b' pieces',\n b' particular',\n b' favorites',\n b' kids',\n b' husband',\n b' lasted',\n b' two',\n b' weeks',\n b' would',\n b' recommend',\n b' brand',\n b' t',\n b'aff',\n b'y',\n b' delightful',\n b' treat']"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "We will now **generate the embeddings** for the 'combined' column using the **get\\_embeddings** function we had defined earlier.  \n",
                "We now have an additional column in the dataframe called vector which has the embeddings.\n",
                "\n",
                "This will take sometime depending on the Service Tier of Azure Open AI resource you have."
            ],
            "metadata": {
                "azdata_cell_guid": "02dd23b3-a622-429d-b680-274efad2c3b4"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "\n",
                "# Assuming you have a DataFrame 'df' with a 'combined' column\n",
                "# and you want to apply 'get_embeddings' to each batch of 100 rows\n",
                "\n",
                "batch_size = 100\n",
                "num_batches = len(df) // batch_size\n",
                "\n",
                "# Initialize an empty list to store the results\n",
                "all_embeddings = []\n",
                "\n",
                "for i in range(num_batches):\n",
                "    start_idx = i * batch_size\n",
                "    end_idx = (i + 1) * batch_size\n",
                "\n",
                "    # Get the current batch\n",
                "    current_batch = df.iloc[start_idx:end_idx]\n",
                "\n",
                "    # Apply your function to the 'combined' column\n",
                "    batch_embeddings = current_batch['combined'].apply(get_embedding)\n",
                "\n",
                "    # Append the batch results to the list\n",
                "    all_embeddings.extend(batch_embeddings)\n",
                "    print(f\"Batch {i+1} completed. Processed {end_idx} rows.\")\n",
                "\n",
                "# Create a new column 'vector' with the combined embeddings\n",
                "df['vector'] = all_embeddings\n",
                "\n",
                "# Print the updated DataFrame\n",
                "df.head(2)\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "a6e49745-42d0-4484-a90d-c1a9a88f9b3d",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Batch 1 completed. Processed 100 rows.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Batch 2 completed. Processed 200 rows.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Batch 3 completed. Processed 300 rows.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Batch 4 completed. Processed 400 rows.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Batch 5 completed. Processed 500 rows.\n"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "   Id        Time   ProductId          UserId  Score                Summary  \\\n0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5  Good Quality Dog Food   \n1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1      Not as Advertised   \n\n                                                Text  \\\n0  I have bought several of the Vitality canned d...   \n1  Product arrived labeled as Jumbo Salted Peanut...   \n\n                                            combined  n_tokens  \\\n0  good quality dog food bought several vitality ...        30   \n1  advertised product arrived labeled jumbo salte...        26   \n\n                                              vector  \n0  [0.017906845, -0.021495823, -0.014191047, -0.0...  \n1  [-0.002080503, -0.015522459, -0.013705523, 0.0...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Time</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>Score</th>\n      <th>Summary</th>\n      <th>Text</th>\n      <th>combined</th>\n      <th>n_tokens</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1303862400</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>5</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n      <td>good quality dog food bought several vitality ...</td>\n      <td>30</td>\n      <td>[0.017906845, -0.021495823, -0.014191047, -0.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1346976000</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>1</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>advertised product arrived labeled jumbo salte...</td>\n      <td>26</td>\n      <td>[-0.002080503, -0.015522459, -0.013705523, 0.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 15
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Part 2 : Store & Query embeddings in Azure SQL DB."
            ],
            "metadata": {
                "azdata_cell_guid": "f9478fe8-be9f-4ecf-b197-2b6a0bf30ff0",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Connect to Azure SQL Database and load the data from the dataframe into a SQL Table\n",
                "\n",
                "We will insert our vectors into the SQL Table now. The table embeddings has a column called vector which is varbinary(8000) type. Ensure you have created the table using the script [createtable.sql](https:\\github.com\\Azure-Samples\\azure-sql-db-vector-search\\blob\\bbe23e5908799fb940f8eb702a7666584c2dc4f3\\VectorSearch_Notebooks\\Python_Notebook_Example\\createtable.sql)\n",
                "\n",
                "We will pass the vectors to the new built in function **JSON\\_ARRAY\\_TO\\_VECTOR** that will converts a JSON array to a compact **binary** representation of a vector.   Vectors are stored in an efficient binary format that also enables usage of dedicated CPU vector processing extensions like SIMD and AVX."
            ],
            "metadata": {
                "azdata_cell_guid": "bd6cb5e4-8207-4003-bce4-a1d7daf28e5c"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#lets define a function to connect to SQLDB\n",
                "\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "def get_mssql_connection():\n",
                "    # Retrieve the connection string from the environment variables\n",
                "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\n",
                "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\n",
                "\n",
                "    # Determine the authentication method and connect to the database\n",
                "    if entra_connection_string:\n",
                "        # Entra ID Service Principal Authentication\n",
                "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \n",
                "        token = credential.get_token('https://database.windows.net/.default')\n",
                "        token_bytes = token.token.encode('UTF-16LE')\n",
                "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
                "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\n",
                "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
                "    elif sql_connection_string:\n",
                "        # SQL Authentication\n",
                "        conn = pyodbc.connect(sql_connection_string)\n",
                "    else:\n",
                "        raise ValueError(\"No valid connection string found in the environment variables.\")\n",
                "\n",
                "    return conn"
            ],
            "metadata": {
                "azdata_cell_guid": "17832bcf-7f42-45db-a4e8-dbbfac7504c4",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "**Before we begin, please determine which path you will be following for this part of the tutorial:**\n",
                "\n",
                "- **Option A:** If you have completed Part 1 and generated embeddings using the Azure OpenAI API, you will insert the embeddings directly from the dataframe/generated output.\n",
                "\n",
                "- **Option B:** If you have not completed Part 1 or wish to use precalculated embeddings, you should download the [finefoodembeddings.csv](https:\\github.com\\azure-samples\\azure-sql-db-vector-search\\blob\\a181e15337402e568f4fc66fe5941e5973171972\\vectorsearch_notebooks\\datasets\\finefoodembeddings.csv) file and we will use this to insert data into the SQL database.\n",
                "\n",
                "**Please follow the instructions corresponding to your chosen option.**\n"
            ],
            "metadata": {
                "azdata_cell_guid": "00014687-c0fc-402f-be59-3720bc38011f"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Option A: Inserting Embeddings Generated in Part 1 into the SQL Table"
            ],
            "metadata": {
                "azdata_cell_guid": "d3c9e853-7d4e-484c-9226-f53d5a852fb4"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                " \n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                " \n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                " \n",
                "# Loop through the DataFrame rows and insert them into the table\n",
                "for index, row in df.iterrows():\n",
                "    Id = row['Id']\n",
                "    ProductId = row['ProductId']\n",
                "    UserId = row['UserId']\n",
                "    score = row['Score']\n",
                "    summary = row['Summary']\n",
                "    text = row['Text']\n",
                "    combined = row['combined']\n",
                "    vector = row['vector']\n",
                "    # Use placeholders for the parameters in the SQL query\n",
                "    query = \"\"\"\n",
                "    INSERT INTO embeddings (Id, ProductId, UserId, score, summary, text, combined, vector)\n",
                "    VALUES (?, ?, ?, ?, ?, ?, ?, JSON_ARRAY_TO_VECTOR(CAST(? AS VARCHAR(MAX))))\n",
                "    \"\"\"\n",
                "    # Execute the query with the parameters\n",
                "    cursor.execute(query, Id, ProductId, UserId, score, summary, text, combined, json.dumps(vector))\n",
                " \n",
                "# Commit the changes\n",
                "conn.commit()\n",
                " \n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'embeddings' table.\")\n",
                " \n",
                "# Close the connection\n",
                "conn.close()"
            ],
            "metadata": {
                "azdata_cell_guid": "1fe3ff69-a5a4-48f4-bcae-8df068734bea",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Data inserted successfully into the 'embeddings' table.\n"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Option B: Inserting precalculated embeddings from the CSV file\n",
                "\n",
                "You can import this [finefoodembeddings.csv](..\\Datasets\\finefoodembeddings.csv) with precalculated embeddings directly to your SQL table if you have not performed the above steps."
            ],
            "metadata": {
                "azdata_cell_guid": "32a9799b-c990-46cf-b9c0-d9d3b88c3246"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "# load & inspect dataset\n",
                "df = pd.read_csv(r\"../Datasets/FineFoodEmbeddings.csv\")\n",
                "df.head(2)"
            ],
            "metadata": {
                "azdata_cell_guid": "45013e22-b118-4da9-b27a-bfabcf1ee869",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "   Unnamed: 0  Id        Time   ProductId          UserId  Score  \\\n0           0   1  1303862400  B001E4KFG0  A3SGXH7AUHU8GW      5   \n1           1   2  1346976000  B00813GRG4  A1D87F6ZCVE5NK      1   \n\n                 Summary                                               Text  \\\n0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n\n                                            combined  \\\n0  Title: Good Quality Dog Food; Content: I have ...   \n1  Title: Not as Advertised; Content: Product arr...   \n\n                                              vector  \n0  [0.02088439, -0.00022463033, -0.0019172364, -0...  \n1  [-0.0044591213, 0.00078397157, -0.022424141, 0...  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Id</th>\n      <th>Time</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>Score</th>\n      <th>Summary</th>\n      <th>Text</th>\n      <th>combined</th>\n      <th>vector</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1303862400</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>5</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n      <td>Title: Good Quality Dog Food; Content: I have ...</td>\n      <td>[0.02088439, -0.00022463033, -0.0019172364, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1346976000</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>1</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>Title: Not as Advertised; Content: Product arr...</td>\n      <td>[-0.0044591213, 0.00078397157, -0.022424141, 0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 19
        },
        {
            "cell_type": "code",
            "source": [
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                "\n",
                "# Assuming 'df' is your DataFrame and it's already defined\n",
                "# Prepare your data to be inserted as a list of tuples\n",
                "data_to_insert = [\n",
                "    (\n",
                "        row['Id'],\n",
                "        row['ProductId'],\n",
                "        row['UserId'],\n",
                "        row['Score'],\n",
                "        row['Summary'],\n",
                "        row['Text'],\n",
                "        row['combined'],\n",
                "        row['vector']\n",
                "    ) for index, row in df.iterrows()\n",
                "]\n",
                "\n",
                "# Define your SQL insert query. \n",
                "#We are using the JSON_ARRAY_TO_VECTOR function here which  converts the jsonarray to a compact binary representation of the vector\n",
                "insert_query = \"\"\"\n",
                "INSERT INTO embeddings (Id, ProductId, UserId, score, summary, text, combined, vector)\n",
                "VALUES (?, ?, ?, ?, ?, ?, ?, JSON_ARRAY_TO_VECTOR(CAST(? AS VARCHAR(MAX))))\n",
                "\"\"\"\n",
                "\n",
                "# Execute batch insert\n",
                "cursor.executemany(insert_query, data_to_insert)\n",
                "\n",
                "# Commit the transaction\n",
                "conn.commit()\n",
                "\n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'embeddings' table.\")\n",
                "\n",
                "# Close the cursor and connection\n",
                "cursor.close()\n",
                "conn.close()\n"
            ],
            "metadata": {
                "azdata_cell_guid": "b54b85a0-4966-43f5-b26c-984145cf9f32",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Data inserted successfully into the 'embeddings' table.\n"
                }
            ],
            "execution_count": 23
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Using the native vector functions in SQL DB\n",
                "\n",
                "Lets take a look at how the vector is stored in the SQL DB table & also make use of the newly introduced helper functions\n",
                "\n",
                "**ISVECTOR** Checks if the provided object is a valid vector: Returns 1 if valid, otherwise returns 0. Returns NULL if the expression is NULL\n",
                "\n",
                "**VECTOR\\_DIMENSIONS** Takes a vector as an input and returns the number of dimensions as an output. In this case we see the number of dimensions of the vector are 1536 (as we are using Azure OpenAI text embeddings)\n",
                "\n",
                "**VECTOR\\_TO\\_JSON\\_ARRAY** Converts a vector in a compact binary format to a human-readable string format. The string format is the same as the one used by JSON to represent arrays"
            ],
            "metadata": {
                "azdata_cell_guid": "0f8850d8-8e41-4a43-a046-e796bcd11fdf"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from prettytable import PrettyTable\n",
                "\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "from azure.identity import DefaultAzureCredential\n",
                "from azure import identity\n",
                "import struct\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the connection string from the environment variables\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Use placeholders for the parameters in the SQL query\n",
                "query = \"SELECT TOP(10) ISVECTOR(vector) as isvector, VECTOR_DIMENSIONS(vector) as dimensions , summary , vector , VECTOR_TO_JSON_ARRAY(vector) as jsonvector,  ProductId FROM dbo.embeddings\"\n",
                "\n",
                "# Execute the query with the parameters\n",
                "cursor.execute(query)\n",
                "queryresults = cursor.fetchall()\n",
                "\n",
                "# Get column names from cursor.description\n",
                "column_names = [column[0] for column in cursor.description]\n",
                "\n",
                "# Create a PrettyTable object\n",
                "table = PrettyTable()\n",
                "\n",
                "# Add column names to the table\n",
                "table.field_names = column_names\n",
                "\n",
                "# Set max width for each column to truncate data\n",
                "table.max_width = 20\n",
                "\n",
                "# Add rows to the table\n",
                "for row in queryresults:\n",
                "    # Truncate each value to 20 characters\n",
                "    truncated_row = [str(value)[:20] for value in row]\n",
                "    table.add_row(truncated_row)\n",
                "\n",
                "# Print the table\n",
                "print(table)\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "# Close the connection\n",
                "conn.close()\n"
            ],
            "metadata": {
                "azdata_cell_guid": "37251f52-1d9b-4ed7-8404-67eb9dd88d69",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+----------+------------+----------------------+----------------------+----------------------+------------+\n| isvector | dimensions |       summary        |        vector        |      jsonvector      | ProductId  |\n+----------+------------+----------------------+----------------------+----------------------+------------+\n|    1     |    1536    |  Not as Advertised   | b'\\xa9\\x01\\x00\\x06\\x | [-0.0044591212645173 | B00813GRG4 |\n|    1     |    1536    |   Healthy Dog Food   | b'\\xa9\\x01\\x00\\x06\\x | [0.0055382279679179, | B00171APVA |\n|    1     |    1536    |       Love it!       | b'\\xa9\\x01\\x00\\x06\\x | [0.0112651837989688, | B001GVISJM |\n|    1     |    1536    | Twizzlers - Strawber | b'\\xa9\\x01\\x00\\x06\\x | [0.0061780856922269, | B001GVISJM |\n|    1     |    1536    |    Great machine!    | b'\\xa9\\x01\\x00\\x06\\x | [-0.0034819100983441 | B003F6UO7K |\n|    1     |    1536    | The best energy shot | b'\\xa9\\x01\\x00\\x06\\x | [-0.0016720767598599 | B001UJEN6C |\n|    1     |    1536    |     it's oatmeal     | b'\\xa9\\x01\\x00\\x06\\x | [-0.0266945492476225 | B001EO5QW8 |\n|    1     |    1536    |    Hearty Oatmeal    | b'\\xa9\\x01\\x00\\x06\\x | [0.0050695422105491, | B001EO5QW8 |\n|    1     |    1536    |    not ass kickin    | b'\\xa9\\x01\\x00\\x06\\x | [0.0006535030552186, | B000G6RPMY |\n|    1     |    1536    |   pretty expensive   | b'\\xa9\\x01\\x00\\x06\\x | [0.0291564352810383, | B004N5KULM |\n+----------+------------+----------------------+----------------------+----------------------+------------+\n"
                }
            ],
            "execution_count": 24
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Performing Vector Similarity Search in Azure SQL DB\n",
                "\n",
                "Let's now query our embedding table to get the top similar reviews given the User search query.\n",
                "\n",
                "What we are doing: Given any user search query, we can get the vector representation of that text.\n",
                "\n",
                "Then we can use that vector to calculate the cosine distance against all the customer review comments stored in the database and take only the closest ones which will return the product most likely connected to the product we are interested in. The reviews with the highest similarity are considered the most relevant to the query, helping users discover products or experiences related to their search.\n",
                "\n",
                "The most common distance is the cosine similarity, which can be calculated quite easily in SQL with the help of the new distance functions.\n",
                "\n",
                "```\n",
                "VECTOR_DISTANCE('distance metric', V1, V2)\n",
                "```\n",
                "\n",
                "We can use **cosine**, **euclidean**, and **dot** as the distance metric today.\n",
                "\n",
                "We will define the function `vector_search_sql`.\n"
            ],
            "metadata": {
                "azdata_cell_guid": "356e7a68-c999-42de-9fe5-12f5f18d0f20",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "def vector_search_sql(query, num_results=5):\n",
                "    # Load environment variables from .env file\n",
                "    load_dotenv()\n",
                "\n",
                "    #Use the get_mssql_connection function to get the connection string details\n",
                "    conn = get_mssql_connection()\n",
                "\n",
                "    # Create a cursor object\n",
                "    cursor = conn.cursor()\n",
                "\n",
                "    # Generate the query embedding for the user's search query\n",
                "    user_query_embedding = get_embedding(query)\n",
                "\n",
                "    # Convert user_query_embedding to a JSON string\n",
                "    user_query_embedding_json = json.dumps(user_query_embedding)\n",
                "\n",
                "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\n",
                "    sql_similarity_search = \"\"\"\n",
                "    SELECT TOP(?) ProductId, Summary, text,\n",
                "           1 - vector_distance('cosine', JSON_ARRAY_TO_VECTOR(cast(? as varchar(max))), [vector]) AS similarity_score\n",
                "    FROM dbo.embeddings\n",
                "    ORDER BY similarity_score desc\n",
                "    \"\"\"\n",
                "\n",
                "    cursor.execute(sql_similarity_search, (num_results, user_query_embedding_json))\n",
                "    results = cursor.fetchall()\n",
                "\n",
                "    # Close the database connection\n",
                "    conn.close()\n",
                "\n",
                "    return results\n"
            ],
            "metadata": {
                "azdata_cell_guid": "dc8b6a89-2894-4607-9549-a8502af81a2c",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 21
        },
        {
            "cell_type": "code",
            "source": [
                "# Assuming you have implemented the `vector_search_sql` function as shown earlier\n",
                "\n",
                "# Example usage\n",
                "query = \"oatmeal options for my toddler\"\n",
                "num_results = 4\n",
                "search_results = vector_search_sql(query, num_results)\n",
                "for result in search_results:\n",
                "    product_id = result[0]  # Assuming ProductId is the first column\n",
                "    summary = result [1]\n",
                "    text = result [2]\n",
                "    similarity_score = result[3]  # Assuming similarity_score is the third column\n",
                "    \n",
                "    print(f\"Product ID: {product_id}\")\n",
                "    print(f\"summary : {summary}\")\n",
                "    print (f\"Text : {text}\")\n",
                "    print(f\"Similarity Score: {similarity_score}\\n\")\n"
            ],
            "metadata": {
                "azdata_cell_guid": "5d77335d-3540-44bf-9e24-aa7037feddb0",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Product ID: B001DIM8K8\nsummary : Nothing like the crappy stuff I grew up on.\nText : My daughter and I eat low-carb. Which oatmeal is not really, by the way. But I was looking for something that we could eat in the morning that was not meat/dairy/egg, and was gluten-free, and filling, without being pure sugar like most breakfast foods and especially grains. I decided to try these long-cooking irish oats. We use 2 cups water and 1/2 cup dry oats, for 1/4cup dry each. It takes this stuff 30 minutes+ to cook so it is not fast food, for sure. But strangely enough, it's filling. Adding fats (couple tablespoons of cream) to the serving helps that of course. In any case, it is much tastier, and much more long-lasting for keeping us satisfied, than I expected, and surprisingly stable on blood sugar. Nor has it set off cravings like most grains, fruits, etc. do if we eat those; the sugars digest so slowly in this apparently. It is more solid and nutty than the flakey quick-oats I grew up with, which I now consider a gummy gluey sugarbomb insult to true oats. These are harder to find and cost more but they're worth it.\nSimilarity Score: 0.5673871961458196\n\nProduct ID: B001EO5QW8\nsummary : it's oatmeal\nText : What else do you need to know? Oatmeal, instant (make it with a half cup of low-fat milk and add raisins;nuke for 90 seconds). More expensive than Kroger store brand oatmeal and maybe a little tastier or better texture or something. It's still just oatmeal. Mmm, convenient!\nSimilarity Score: 0.5668513542501259\n\nProduct ID: B001DIM8K8\nsummary : Texture and the taste are totally different from the pressed oats!\nText : I have been having Quaker Oatmeal for yesrs until trying this today! The texture and the taste are completely different from the pressed oat. It worths cooking for half hour. My son love it at the first bite! My 8-month-old baby is also love it! I uaually cook the portion which will be enough for 2~3 days. Drop one spoon or two into the worm milk every morning to make my breakfast much more healthier! This one is so good that it will be hard for me to go back to pressed oatmeal.....\nSimilarity Score: 0.5633615577032233\n\nProduct ID: B001DIM8K8\nsummary : Slow cooking\nText : I'll be honest, if you want a quick breakfast don't purchase this oatmeal. But, if you have time to spare or don't mind waiting for some \"good eats\", this is the oatmeal for you.<br /><br />Step 1. pour some milk into a sauce pan; Step 2. add a dash of salt [or salt to taste]; Step 3. add oatmeal, making sure it stays covered by milk; Step 4. turn on stove to low heat [not too low but definitely not too high, as you don't want the milk to evaporate/dry out leaving you with tough oats in pan] and cover pan; Step 5. do some little cleaning to built up appetite making sure to ck on progress periodically; Step 6. add more milk if oats are still too tough and let it simmer for a little while longer; Step 7. get your ladle/just pour directing into a bowl add a few raisins/cranberries/your favorite dried fruit and ENJOY!\nSimilarity Score: 0.5528843265139003\n\n"
                }
            ],
            "execution_count": 25
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Part 3 - Use embeddings retrieved from a vector database to augment LLM generation\n",
                "\n",
                "Lets create a helper function to feed prompts into the [Completions model](https:\\learn.microsoft.com\\en-us\\azure\\ai-services\\openai\\concepts\\models#gpt-35-models) & create interactive loop where you can pose questions to the model and receive information grounded in your data.\n",
                "\n",
                "The function `generate_completion` is defined to help ground the GPT3.5 model with prompts and system instructions.   \n",
                "Note that we are passing the results of the `vector_search_sql` we defined earlier to the model and we define the system prompt .  \n",
                "We are using gpt-35-turbo-16k model here. \n",
                "\n",
                "You can get more information on using Azure Open AI GPT chat models [here](https:\\learn.microsoft.com\\en-us\\azure\\ai-services\\openai\\chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)"
            ],
            "metadata": {
                "azdata_cell_guid": "a4919ceb-ae7c-415f-9872-4fc6b4e11c99"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the API key and endpoint from the environment variables\n",
                "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
                "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
                "\n",
                "# Create a chat completion request\n",
                "client = AzureOpenAI(\n",
                "    api_key=api_key,\n",
                "    api_version=\"2023-05-15\",\n",
                "    azure_endpoint=azure_endpoint\n",
                ")\n",
                "\n",
                "\n",
                "def generate_completion(search_results, user_input):\n",
                "    system_prompt = '''\n",
                "You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\n",
                "- Use the information from `search_results` to generate your responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\n",
                "  Product ID: {product_id}\n",
                "  Summary: {summary}\n",
                "  Review: {text}\n",
                "  Similarity Score: {similarity_score}\n",
                "- Avoid any other external data sources.\n",
                "- Add a fun fact related to the overall product searched at the end of the recommendations.\n",
                "'''\n",
                "\n",
                "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
                "    search_results = vector_search_sql(user_input, num_results)\n",
                "    \n",
                "    # Create an empty list to store the results\n",
                "    result_list = []\n",
                "\n",
                "    # Iterate through the search results and append relevant information to the list\n",
                "    for result in search_results:\n",
                "        product_id = result[0]  # Assuming ProductId is the first column\n",
                "        summary = result[1]\n",
                "        text = result[2]\n",
                "        similarity_score = result[3]  # Assuming similarity_score is the third column\n",
                "        \n",
                "        # Append the relevant information as a dictionary to the result_list\n",
                "        result_list.append({\n",
                "            \"product_id\": product_id,\n",
                "            \"summary\": summary,\n",
                "            \"text\": text,\n",
                "            \"similarity_score\": similarity_score\n",
                "        })\n",
                "\n",
                "    #print (result_list)\n",
                "    messages.append({\"role\": \"system\", \"content\": f\"{result_list}\"})\n",
                "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
                "    response = client.chat.completions.create(model='chatcompletion', messages=messages, temperature=0) #replace with your model deployment name\n",
                "\n",
                "    return response.dict()"
            ],
            "metadata": {
                "azdata_cell_guid": "d2cf8d04-8323-4024-996c-f9e0acd1ef93",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 26
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a loop of user input and model output to perform Q&A on the FineFoods Sample data\n",
                "\n",
                "print(\"*** What products are you looking for? Ask me & I can help you :) Type 'end' to end the session.\\n\")\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"User prompt: \")\n",
                "    if user_input.lower() == \"end\":\n",
                "        break\n",
                "\n",
                "    # Print the user's question\n",
                "    print(f\"\\nUser asked: {user_input}\")\n",
                "\n",
                "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\n",
                "    search_results = vector_search_sql(user_input)\n",
                "    completions_results = generate_completion(search_results, user_input)\n",
                "\n",
                "    # Print the model's response\n",
                "    print(\"\\nAI's response:\")\n",
                "    print(completions_results['choices'][0]['message']['content'])\n",
                "\n",
                "# The loop will continue until the user types 'end'\n"
            ],
            "metadata": {
                "azdata_cell_guid": "57c6ba6b-8d63-4c21-a9d2-af7fec6d80f9",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "*** What products are you looking for? Ask me & I can help you :) Type 'end' to end the session.\n\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "\nUser asked: I want some healthy options for cat food. Mind you my cat is super fussy and doesnt like stuff easily\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "\nAI's response:\nI understand that your cat is a picky eater, but I have some healthy options for you to consider based on the reviews:\n\n1. Product ID: B002PNGY28\n   Summary: Great soft little treats for your finicky feline\n   Review: I have 2 cats. One will eat anything. The other one is soooo picky. She's had to have some of her teeth removed due to a mouth virus, so I needed to find some good soft treats. She likes these more than anything I've tried. They are really thin little tiny strips. I like Wellness, all natural products. Both cats like their brand cat food as well.\n   Similarity Score: 0.651522667741025\n\n2. Product ID: B003SE19UK\n   Summary: Palatable and healthy\n   Review: Before I was educated about feline nutrition, I allowed my cats to become addicted to dry cat food. I always offered both canned and dry, but wish I would have fed them premium quality canned food and limited dry food. I have two 15-year-old cats and two 5-year-old cats. The only good quality dry foods they will eat are Wellness and Innova. Innova's manufacturer was recently purchased by Procter&Gamble. I began looking for a replacement. After once again offering several samples (from my local holistic pet store) Holistic Select was the only one (other than the usual Wellness and Innova) they would eat. For finicky cats, I recommend trying Holistic Select. It is a good quality food that is very palatable for finicky eaters.\n   Similarity Score: 0.6401906033145333\n\n3. Product ID: B000N5Z5RU\n   Summary: My Picky Cat-Child Loves It\n   Review: My youngest cat is very particular about what he eats. He absolutely will not eat if something doesn't meet his expectations. Between that and the recent pet food recalls, I've had a really hard time finding cat food that I can feed him. I'm happy to say that the Merrick Turducken clicks with him, plus it has no wheat gluten or other fillers. The texture of it is appealing, too... it's not just \"mush\" or Spam-looking like many foods. I'll continue to buy Turducken and plan to also try out some of the other Merrick flavors.\n   Similarity Score: 0.6269133034740086\n\n4. Product ID: B00180O980\n   Summary: Economical and my cat loves it\n   Review: Cats are finicky animals and what one loves another won't touch - that is not a reflection on the food itself. I have two Burmese and their eating habits are very different. My local Pet Nutrition Center kindly gave me a sample of Prowl the other day and while my one cat absolutely loves it, the other won't touch it. That's why samples are nice. Both of my cats have been on Evo kibble, but this one cat who now loves the Prowl has a habit of throwing up, and I'm on the prowl myself for an alternative food that will help her keep food down. So far, no barf. She also laps it up as soon as I've added warm water - no waiting. I highly recommend you obtain a sample and let the cat decide, as the food is economical as well as healthy.\n   Similarity Score: 0.615546735849085\n\nFun Fact: Did you know that cats have a specialized collarbone that allows them to always land on their feet when they fall? It's called the \"cat righting reflex\"!\n"
                }
            ],
            "execution_count": 27
        },
        {
            "cell_type": "markdown",
            "source": [
                "You can use also use Semantic Search with Keyword search in SQL DB along with using filters and aggregations . Check out the TSQL in the same repo"
            ],
            "metadata": {
                "azdata_cell_guid": "d1ac7c15-bbda-4365-918b-a99c1a0822d3"
            },
            "attachments": {}
        }
    ]
}