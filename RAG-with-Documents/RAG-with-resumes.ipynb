{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "507219d1-d713-4c41-86d5-e938bf69627c",
                "language": "sql"
            },
            "source": [
                "# Leveraging Azure SQL DB’s Native Vector Capabilities for Enhanced Resume Matching with Azure Document Intelligence and RAG\n",
                "\n",
                "In this tutorial, we will explore how to leverage Azure SQL DB’s new vector data type to store embeddings and perform similarity searches using built-in vector functions, enabling advanced resume matching to identify the most suitable candidates. \n",
                "\n",
                "By extracting and chunking content from PDF resumes using Azure Document Intelligence, generating embeddings with Azure OpenAI, and storing these embeddings in Azure SQL DB, we can perform sophisticated vector similarity searches and retrieval-augmented generation (RAG) to identify the most suitable candidates based on their resumes.\n",
                "\n",
                "### **Tutorial Overview**\n",
                "\n",
                "- This Python notebook will teach you to:\n",
                "    1. **Chunk PDF Resumes**: Use **`Azure Document Intelligence`** to extract and chunk content from PDF resumes.\n",
                "    2. **Create Embeddings**: Generate embeddings from the chunked content using the **`Azure OpenAI API`**.\n",
                "    3. **Vector Database Utilization**: Store embeddings in **`Azure SQL DB`** utilizing the **`new Vector Data Type`** and perform similarity searches using built-in vector functions to find the most suitable candidates.\n",
                "    4. **LLM Generation Augmentation**: Enhance language model generation with embeddings from a vector database. In this case, we use the embeddings to inform a GPT-4 chat model, enabling it to provide rich, context-aware answers about candidates based on their resumes\n",
                "\n",
                "## Dataset\n",
                "\n",
                "We use a sample dataset from [Kaggle](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) containing PDF resumes for this tutorial. For the purpose of this tutorial we will use 120 resumes from the **Information-Technology** folder\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- **Azure Subscription**: [Create one for free](https://azure.microsoft.com/free/cognitive-services?azure-portal=true)\n",
                "- **Azure SQL Database**: [Set up your database for free](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql)\n",
                "- **Azure Document Intelligence** [Create a FreeAzure Doc Intelligence resource](https:/learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0)\n",
                "- **Azure Data Studio**: Download [here](https://azure.microsoft.com/products/data-studio) to manage your Azure SQL database and [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel)\n",
                "\n",
                "## Additional Requirements for Embedding Generation\n",
                "\n",
                "- **Azure OpenAI Access**: Apply for access in the desired Azure subscription at [https://aka.ms/oai/access](https://aka.ms/oai/access)\n",
                "- **Azure OpenAI Resource**: Deploy an embeddings model (e.g., `text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4.0` model for chat completion. Refer to the [resource deployment guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
                "- **Python**: Version 3.7.1 or later from Python.org. (Sample has been tested with Python 3.11)\n",
                "- **Python Libraries**: Install the required libraries openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken, and pyodbc.\n",
                "- **Jupyter Notebooks**: Use within [Azure Data Studio](https://learn.microsoft.com/en-us/azure-data-studio/notebooks/notebooks-guidance) or Visual Studio Code .\n",
                "\n",
                "Code snippets are adapted from the [Azure OpenAI Service embeddings Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python)\n",
                "\n",
                "## Getting Started\n",
                "\n",
                "1. **Database Setup**: Execute SQL commands from the `createtable.sql` script to create the necessary table in your database.\n",
                "2. **Model Deployment**: Deploy an embeddings model (`text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Note the 2 models deployment names for later use.\n",
                "\n",
                "![Deployed OpenAI Models](../Assets/modeldeployment.png)\n",
                "\n",
                "3. **Connection String**: Find your Azure SQL DB connection string in the Azure portal under your database settings.\n",
                "4. **Configuration**: Populate the `.env` file with your SQL server connection details , Azure OpenAI key and endpoint, Azure Document Intelligence key and endpoint values.\n",
                "\n",
                "You can retrieve the Azure OpenAI _endpoint_ and _key_:\n",
                "\n",
                "![Azure OpenAI Endpoint and Key](../Assets/endpoint.png)\n",
                "\n",
                "You can [retrieve](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) the Document Intelligence _endpoint_ and _key_:\n",
                "\n",
                "![Azure Document Intelligence Endpoint and Key](../Assets/docintelendpoint.png)\n",
                "\n",
                "## Running the Notebook\n",
                "\n",
                "To [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel), connect to your Azure SQL database using Azure Data Studio, which can be downloaded [here](https://azure.microsoft.com/products/data-studio)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "fe37c601-5918-4055-badc-6c0ba90c68ce",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: tiktoken in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
                        "Requirement already satisfied: tokenizer in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 2)) (3.4.3)\n",
                        "Requirement already satisfied: azure-ai-documentintelligence in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 3)) (1.0.0b4)\n",
                        "Requirement already satisfied: azure-ai-formrecognizer in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 4)) (3.3.3)\n",
                        "Requirement already satisfied: azure-identity in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 5)) (1.16.0)\n",
                        "Requirement already satisfied: azure-core in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 6)) (1.30.1)\n",
                        "Requirement already satisfied: azure-search-documents==11.6.0b3 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 7)) (11.6.0b3)\n",
                        "Requirement already satisfied: python-dotenv in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 8)) (1.0.1)\n",
                        "Requirement already satisfied: openai in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 9)) (1.30.1)\n",
                        "Requirement already satisfied: numpy in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 10)) (1.26.4)\n",
                        "Requirement already satisfied: pyodbc in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 11)) (5.1.0)\n",
                        "Requirement already satisfied: num2words in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 12)) (0.5.13)\n",
                        "Requirement already satisfied: matplotlib in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 13)) (3.9.0)\n",
                        "Requirement already satisfied: plotly in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 14)) (5.22.0)\n",
                        "Requirement already satisfied: scipy in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 15)) (1.13.0)\n",
                        "Requirement already satisfied: scikit-learn in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 16)) (1.4.2)\n",
                        "Requirement already satisfied: pandas in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 17)) (2.2.2)\n",
                        "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: PrettyTable in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 19)) (3.10.0)\n",
                        "Requirement already satisfied: nltk in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 20)) (3.8.1)\n",
                        "Requirement already satisfied: azure-common>=1.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-search-documents==11.6.0b3->-r requirements.txt (line 7)) (1.1.28)\n",
                        "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-search-documents==11.6.0b3->-r requirements.txt (line 7)) (0.6.1)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 24.0 -> 24.2\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken->-r requirements.txt (line 1)) (2024.5.15)\n",
                        "Requirement already satisfied: requests>=2.26.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tiktoken->-r requirements.txt (line 1)) (2.31.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-ai-documentintelligence->-r requirements.txt (line 3)) (4.11.0)\n",
                        "Requirement already satisfied: msrest>=0.6.21 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-ai-formrecognizer->-r requirements.txt (line 4)) (0.7.1)\n",
                        "Requirement already satisfied: cryptography>=2.5 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 5)) (42.0.7)\n",
                        "Requirement already satisfied: msal>=1.24.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 5)) (1.28.0)\n",
                        "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-identity->-r requirements.txt (line 5)) (1.1.0)\n",
                        "Requirement already satisfied: six>=1.11.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from azure-core->-r requirements.txt (line 6)) (1.16.0)\n",
                        "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (4.3.0)\n",
                        "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (1.9.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (0.27.0)\n",
                        "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (2.7.1)\n",
                        "Requirement already satisfied: sniffio in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (1.3.1)\n",
                        "Requirement already satisfied: tqdm>4 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from openai->-r requirements.txt (line 9)) (4.66.4)\n",
                        "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from num2words->-r requirements.txt (line 12)) (0.6.2)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (1.2.1)\n",
                        "Requirement already satisfied: cycler>=0.10 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (4.51.0)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.5)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (24.0)\n",
                        "Requirement already satisfied: pillow>=8 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (10.3.0)\n",
                        "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (3.1.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0.post0)\n",
                        "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from plotly->-r requirements.txt (line 14)) (8.3.0)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->-r requirements.txt (line 16)) (1.4.2)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->-r requirements.txt (line 16)) (3.5.0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 17)) (2024.1)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 17)) (2024.1)\n",
                        "Requirement already satisfied: wcwidth in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from PrettyTable->-r requirements.txt (line 19)) (0.2.13)\n",
                        "Requirement already satisfied: click in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from nltk->-r requirements.txt (line 20)) (8.1.7)\n",
                        "Requirement already satisfied: idna>=2.8 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 9)) (3.7)\n",
                        "Requirement already satisfied: cffi>=1.12 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 5)) (1.16.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (2024.2.2)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (1.0.5)\n",
                        "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 9)) (0.14.0)\n",
                        "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity->-r requirements.txt (line 5)) (2.8.0)\n",
                        "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from msal-extensions>=0.3.0->azure-identity->-r requirements.txt (line 5)) (2.8.2)\n",
                        "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from msrest>=0.6.21->azure-ai-formrecognizer->-r requirements.txt (line 4)) (2.0.0)\n",
                        "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 9)) (0.6.0)\n",
                        "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 9)) (2.18.2)\n",
                        "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (3.3.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (2.2.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai->-r requirements.txt (line 9)) (0.4.6)\n",
                        "Requirement already satisfied: pycparser in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->-r requirements.txt (line 5)) (2.22)\n",
                        "Requirement already satisfied: pywin32>=226 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from portalocker<3,>=1.6->msal-extensions>=0.3.0->azure-identity->-r requirements.txt (line 5)) (306)\n",
                        "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\damauri\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer->-r requirements.txt (line 4)) (3.2.2)\n"
                    ]
                }
            ],
            "source": [
                "#Setup the python libraries required for this notebook\n",
                "#Please ensure that you navigate to the directory containing the `requirements.txt` file in your terminal\n",
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "4c29709e-1c3a-495d-83ec-05737e220847",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "#Load the env details\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "4b543f05-9036-4887-8737-09aa9f865ec2",
                "language": "python"
            },
            "source": [
                "# **PART 1: Extracting and Chunking Text from PDF Resumes using Azure Document Intelligence**\n",
                "\n",
                "Create an instance of the [DocumentAnalysisClient](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) using the endpoint and API key. \n",
                "\n",
                "[Azure Document Intelligence](https://learn.microsoft.com/azure/ai-services/document-intelligence/?view=doc-intel-4.0.0_)(previously known as Form Recognizer) is a Azure cloud service that uses machine learning to analyze text and structured data from your documents. This client will be used to send requests to the [Azure Document Intelligence](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python) service and receive responses containing the extracted text from the PDF resumes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "b20cc66a-50ce-4486-b275-d4683f4ba545",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
                "from azure.core.credentials import AzureKeyCredential\n",
                "\n",
                "# Load environment variables\n",
                "endpoint = os.getenv(\"AZUREDOCINTELLIGENCE_ENDPOINT\")\n",
                "api_key = os.getenv(\"AZUREDOCINTELLIGENCE_API_KEY\")\n",
                "\n",
                "# Create a DocumentAnalysisClient\n",
                "document_analysis_client = DocumentAnalysisClient(\n",
                "    endpoint=endpoint,\n",
                "    credential=AzureKeyCredential(api_key)\n",
                ")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1ecc04b0-1a5f-4d48-819d-2cc06a070062",
                "language": "python"
            },
            "source": [
                "### **Analyze input documents using prebuilt model in Azure Document Intelligence**\n",
                "\n",
                "- DocumentAnalysisClient provides operations for analyzing input documents using prebuilt and custom models through the `begin_analyze_document` and `begin_analyze_document_from_url` APIs. In this tutorial we are using the [prebuilt-layout](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python#using-prebuilt-models)\n",
                "    \n",
                "\n",
                "### **Split text into chunks of 500 tokens**\n",
                "\n",
                "- When faced with content that exceeds the embedding limit, we usually also chunk the content into smaller pieces and then embed those one at a time. Here we will use [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file) to chunk the extracted text into token sizes of 500, as we will later pass the extracted chunks to to the `text-embedding-small` model for [generating text embeddings](https://learn.microsoft.com/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python) as this has a model input token limit of 8192.\n",
                "\n",
                "**Note**: You need to provide the location of the folder where the PDF files reside in the below script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "azdata_cell_guid": "0355f92c-0546-4eac-aea8-b55d8bbef194",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of PDF files in the directory: 6\n",
                        "Processing file 1/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10005171.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10005171.pdf: 2\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10005171.pdf, Chunk ID: 0, Unique Chunk ID: 0_0, Chunk Length: 2791, Chunk Text: MEDIA ACTIVITIES SPECIALIST Summary MultiTasking M...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10005171.pdf, Chunk ID: 1, Unique Chunk ID: 0_1, Chunk Length: 2705, Chunk Text: Worked with local production companies to create c...\n",
                        "Processing file 2/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10515955.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10515955.pdf: 2\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10515955.pdf, Chunk ID: 0, Unique Chunk ID: 1_0, Chunk Length: 2853, Chunk Text: DIGITAL MEDIA SALES CONSULTANT Summary Dedicated a...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\DIGITAL-MEDIA\\10515955.pdf, Chunk ID: 1, Unique Chunk ID: 1_1, Chunk Length: 2564, Chunk Text: national rates Classified Private Party Rep Jan 20...\n",
                        "Processing file 3/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10030015.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10030015.pdf: 1\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10030015.pdf, Chunk ID: 0, Unique Chunk ID: 2_0, Chunk Length: 2639, Chunk Text: ENGINEERING LAB TECHNICIAN Career Focus My main ob...\n",
                        "Processing file 4/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10219099.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10219099.pdf: 3\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10219099.pdf, Chunk ID: 0, Unique Chunk ID: 3_0, Chunk Length: 3086, Chunk Text: EQUIPMENT ENGINEERING TECHNICIAN Professional Summ...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10219099.pdf, Chunk ID: 1, Unique Chunk ID: 3_1, Chunk Length: 2966, Chunk Text: Operates repair and maintain FacilityPlant Machine...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\ENGINEERING\\10219099.pdf, Chunk ID: 2, Unique Chunk ID: 3_2, Chunk Length: 1466, Chunk Text: City State  Expedites materials through production...\n",
                        "Processing file 5/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10089434.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10089434.pdf: 3\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10089434.pdf, Chunk ID: 0, Unique Chunk ID: 4_0, Chunk Length: 3035, Chunk Text: INFORMATION TECHNOLOGY TECHNICIAN I Summary Versat...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10089434.pdf, Chunk ID: 1, Unique Chunk ID: 4_1, Chunk Length: 2959, Chunk Text: Disaster Recovery plan and procedures  Researching...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10089434.pdf, Chunk ID: 2, Unique Chunk ID: 4_2, Chunk Length: 2048, Chunk Text: Installing configuring and supporting McAfee antiv...\n",
                        "Processing file 6/6: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10247517.pdf\n",
                        "Number of chunks for file w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10247517.pdf: 3\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10247517.pdf, Chunk ID: 0, Unique Chunk ID: 5_0, Chunk Length: 3191, Chunk Text: INFORMATION TECHNOLOGY MANAGER Professional Summar...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10247517.pdf, Chunk ID: 1, Unique Chunk ID: 5_1, Chunk Length: 2744, Chunk Text: network which entailed changing software and LAN c...\n",
                        "File: w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-with-Documents\\.\\docs\\INFORMATION-TECHNOLOGY\\10247517.pdf, Chunk ID: 2, Unique Chunk ID: 5_2, Chunk Length: 729, Chunk Text: i4 City State 2015 Master of Science  Information ...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>file_name</th>\n",
                            "      <th>chunk_id</th>\n",
                            "      <th>chunk_text</th>\n",
                            "      <th>unique_chunk_id</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>MEDIA ACTIVITIES SPECIALIST Summary MultiTaski...</td>\n",
                            "      <td>0_0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...</td>\n",
                            "      <td>1</td>\n",
                            "      <td>Worked with local production companies to crea...</td>\n",
                            "      <td>0_1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>DIGITAL MEDIA SALES CONSULTANT Summary Dedicat...</td>\n",
                            "      <td>1_0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                           file_name  chunk_id  \\\n",
                            "0  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         0   \n",
                            "1  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         1   \n",
                            "2  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         0   \n",
                            "\n",
                            "                                          chunk_text unique_chunk_id  \n",
                            "0  MEDIA ACTIVITIES SPECIALIST Summary MultiTaski...             0_0  \n",
                            "1  Worked with local production companies to crea...             0_1  \n",
                            "2  DIGITAL MEDIA SALES CONSULTANT Summary Dedicat...             1_0  "
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import re\n",
                "import pandas as pd\n",
                "import tiktoken\n",
                "\n",
                "# Path to the directory containing PDF files\n",
                "folder_path = os.path.join(os.getcwd(), '.\\docs')\n",
                "\n",
                "def get_pdf_files(folder_path):\n",
                "    for path, subdirs, files in os.walk(folder_path):\n",
                "        for name in files:\n",
                "            if (name.endswith(\".pdf\")):\n",
                "                yield os.path.join(path, name)\n",
                "\n",
                "# Function to read PDF files and extract text using Azure AI Document Intelligence\n",
                "def extract_text_from_pdf(pdf_path):\n",
                "    with open(pdf_path, \"rb\") as f:\n",
                "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", document=f)\n",
                "    result = poller.result()\n",
                "    text = \"\"\n",
                "    for page in result.pages:\n",
                "        for line in page.lines:\n",
                "            text += line.content + \" \"\n",
                "    return text\n",
                "\n",
                "# Function to clean text and remove special characters\n",
                "def clean_text(text):\n",
                "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
                "    return text\n",
                "\n",
                "# Function to split text into chunks of 500 tokens\n",
                "def split_text_into_token_chunks(text, max_tokens=500):\n",
                "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "    tokens = tokenizer.encode(text)\n",
                "    chunks = []\n",
                "    \n",
                "    for i in range(0, len(tokens), max_tokens):\n",
                "        chunk_tokens = tokens[i:i + max_tokens]\n",
                "        chunk_text = tokenizer.decode(chunk_tokens)\n",
                "        chunks.append(chunk_text)\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Count the number of PDF files in the directory\n",
                "pdf_files = [f for f in get_pdf_files(folder_path)]\n",
                "num_files = len(pdf_files)\n",
                "print(f\"Number of PDF files in the directory: {num_files}\")\n",
                "\n",
                "# Create a DataFrame to store the chunks\n",
                "data = []\n",
                "\n",
                "for file_id, pdf_file in enumerate(pdf_files):\n",
                "    print(f\"Processing file {file_id + 1}/{num_files}: {pdf_file}\")\n",
                "    pdf_path = os.path.join(folder_path, pdf_file)\n",
                "    text = extract_text_from_pdf(pdf_path)\n",
                "    cleaned_text = clean_text(text)\n",
                "    chunks = split_text_into_token_chunks(cleaned_text)\n",
                "    \n",
                "    print(f\"Number of chunks for file {pdf_file}: {len(chunks)}\")\n",
                "    \n",
                "    for chunk_id, chunk in enumerate(chunks):\n",
                "        chunk_text = chunk.strip() if chunk.strip() else \"NULL\"\n",
                "        unique_chunk_id = f\"{file_id}_{chunk_id}\"\n",
                "        print(f\"File: {pdf_file}, Chunk ID: {chunk_id}, Unique Chunk ID: {unique_chunk_id}, Chunk Length: {len(chunk_text)}, Chunk Text: {chunk_text[:50]}...\")  # Print first 50 characters of chunk text\n",
                "        data.append({\n",
                "            \"file_name\": pdf_file,\n",
                "            \"chunk_id\": chunk_id,\n",
                "            \"chunk_text\": chunk_text,\n",
                "            \"unique_chunk_id\": unique_chunk_id\n",
                "        })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "df.head(3)\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "655a8389-1018-4e80-a39d-85187cf3c46f",
                "language": "python"
            },
            "source": [
                "### **Tokenization vs. Character Length (OPTIONAL)**\n",
                "\n",
                "In this section, we will explore the difference between the character length of a text chunk and its tokenized representation. Character length simply counts the number of characters in a text, while tokenization breaks the text into meaningful units called tokens.\n",
                "\n",
                "Character Length First, let’s add a new column to our DataFrame to view the length of each chunk in terms of characters: Here, chunk\\_length represents the number of characters in each chunk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "azdata_cell_guid": "5e171928-764a-4d61-899a-83a8e0c3ac79",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                           file_name  chunk_id  chunk_length\n",
                        "0  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         0          2791\n",
                        "1  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         1          2705\n",
                        "2  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         0          2853\n",
                        "3  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         1          2564\n",
                        "4  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...         0          2639\n"
                    ]
                }
            ],
            "source": [
                "# Add a new column 'chunk_length' to the DataFrame to view the length of each chunk\n",
                "df['chunk_length'] = df['chunk_text'].apply(len)\n",
                "\n",
                "# Display the first few rows of the DataFrame with the new column\n",
                "print(df[['file_name', 'chunk_id', 'chunk_length']].head(5))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "9b87f84b-149c-4eb5-b9f0-c1b55f6d606c",
                "language": "python"
            },
            "source": [
                "### Tokenization\n",
                "To understand how text ultimately is tokenized, it can be helpful to run the below code: \n",
                "\n",
                "- We use the tiktoken library to tokenize the text. Tokenization breaks the text into smaller units, which can be words, subwords, or characters, depending on the tokenizer used. You can see that in some cases an entire word is represented with a single token whereas in others parts of words are split across multiple tokens. \n",
                "\n",
                "- If you then check the length of the decode variable, you'll find it matches 500 our specified token number. It is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192\n",
                "\n",
                "- When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples below and then convert the tokens to a series of floating point numbers that will be accessible via vector search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "azdata_cell_guid": "28188280-2bef-414a-a663-a017c944bc19",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[b'MEDIA',\n",
                            " b' ACT',\n",
                            " b'IV',\n",
                            " b'ITIES',\n",
                            " b' SPECIAL',\n",
                            " b'IST',\n",
                            " b' Summary',\n",
                            " b' Multi',\n",
                            " b'Task',\n",
                            " b'ing',\n",
                            " b' Media',\n",
                            " b' Relations',\n",
                            " b' Results',\n",
                            " b'oriented',\n",
                            " b' Strategic',\n",
                            " b' Initi',\n",
                            " b'atives',\n",
                            " b' Event',\n",
                            " b' Planning',\n",
                            " b' Writer',\n",
                            " b' ',\n",
                            " b' Editor',\n",
                            " b' Manager',\n",
                            " b'Sup',\n",
                            " b'ervisor',\n",
                            " b' Flex',\n",
                            " b'ibility',\n",
                            " b' Ad',\n",
                            " b'ap',\n",
                            " b'table',\n",
                            " b' Highlights',\n",
                            " b' ',\n",
                            " b' Great',\n",
                            " b'ly',\n",
                            " b' improved',\n",
                            " b' media',\n",
                            " b' coverage',\n",
                            " b' of',\n",
                            " b' press',\n",
                            " b' conferences',\n",
                            " b' and',\n",
                            " b' other',\n",
                            " b' events',\n",
                            " b' on',\n",
                            " b' campus',\n",
                            " b' ',\n",
                            " b' Increased',\n",
                            " b' the',\n",
                            " b' frequency',\n",
                            " b' of',\n",
                            " b' newspaper',\n",
                            " b' radio',\n",
                            " b' and',\n",
                            " b' television',\n",
                            " b' interviews',\n",
                            " b' featuring',\n",
                            " b' Chattanooga',\n",
                            " b' State',\n",
                            " b' administrators',\n",
                            " b' faculty',\n",
                            " b' and',\n",
                            " b' staff',\n",
                            " b' ',\n",
                            " b' Host',\n",
                            " b'ed',\n",
                            " b' popular',\n",
                            " b' television',\n",
                            " b' show',\n",
                            " b' that',\n",
                            " b' focused',\n",
                            " b' on',\n",
                            " b' campus',\n",
                            " b' and',\n",
                            " b' community',\n",
                            " b' events',\n",
                            " b' ',\n",
                            " b'199',\n",
                            " b'720',\n",
                            " b'04',\n",
                            " b' ',\n",
                            " b' Commission',\n",
                            " b'ed',\n",
                            " b' by',\n",
                            " b' local',\n",
                            " b' State',\n",
                            " b' Representative',\n",
                            " b' to',\n",
                            " b' produce',\n",
                            " b' a',\n",
                            " b' historical',\n",
                            " b' documentary',\n",
                            " b' on',\n",
                            " b' African',\n",
                            " b' American',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' Tennessee',\n",
                            " b' Legislature',\n",
                            " b' from',\n",
                            " b' Reconstruction',\n",
                            " b' to',\n",
                            " b' Modern',\n",
                            " b' Times',\n",
                            " b' ',\n",
                            " b'200',\n",
                            " b'4',\n",
                            " b' ',\n",
                            " b' Created',\n",
                            " b' onsite',\n",
                            " b' Spanish',\n",
                            " b' language',\n",
                            " b' classes',\n",
                            " b' for',\n",
                            " b' Emergency',\n",
                            " b' Room',\n",
                            " b' personnel',\n",
                            " b' in',\n",
                            " b' local',\n",
                            " b' hospitals',\n",
                            " b' when',\n",
                            " b' Spanish',\n",
                            " b' speaking',\n",
                            " b' population',\n",
                            " b' began',\n",
                            " b' to',\n",
                            " b' expand',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' area',\n",
                            " b' ',\n",
                            " b'199',\n",
                            " b'5',\n",
                            " b' Accom',\n",
                            " b'pl',\n",
                            " b'ishments',\n",
                            " b' Led',\n",
                            " b' Chattanooga',\n",
                            " b' State',\n",
                            " b' to',\n",
                            " b' receive',\n",
                            " b' National',\n",
                            " b' Awards',\n",
                            " b' the',\n",
                            " b' Bronze',\n",
                            " b' Par',\n",
                            " b'agon',\n",
                            " b' Award',\n",
                            " b' in',\n",
                            " b' ',\n",
                            " b'201',\n",
                            " b'2',\n",
                            " b' from',\n",
                            " b' the',\n",
                            " b' National',\n",
                            " b' Council',\n",
                            " b' for',\n",
                            " b' Marketing',\n",
                            " b' and',\n",
                            " b' Public',\n",
                            " b' Relations',\n",
                            " b' NC',\n",
                            " b'MP',\n",
                            " b'R',\n",
                            " b' for',\n",
                            " b' Degrees',\n",
                            " b' That',\n",
                            " b' Work',\n",
                            " b' ',\n",
                            " b'1',\n",
                            " b' ',\n",
                            " b' ',\n",
                            " b'2',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' Radio',\n",
                            " b'Advertisement',\n",
                            " b'PS',\n",
                            " b'A',\n",
                            " b' Series',\n",
                            " b' category',\n",
                            " b' Silver',\n",
                            " b' Par',\n",
                            " b'agon',\n",
                            " b' Award',\n",
                            " b' in',\n",
                            " b' ',\n",
                            " b'201',\n",
                            " b'1',\n",
                            " b' from',\n",
                            " b' NC',\n",
                            " b'MP',\n",
                            " b'R',\n",
                            " b' for',\n",
                            " b' The',\n",
                            " b' Power',\n",
                            " b' of',\n",
                            " b' Achievement',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' Electronic',\n",
                            " b' View',\n",
                            " b'book',\n",
                            " b' category',\n",
                            " b' W',\n",
                            " b'rote',\n",
                            " b' and',\n",
                            " b' produced',\n",
                            " b' ',\n",
                            " b'201',\n",
                            " b'0',\n",
                            " b' NC',\n",
                            " b'MP',\n",
                            " b'R',\n",
                            " b' District',\n",
                            " b' level',\n",
                            " b' winners',\n",
                            " b' Online',\n",
                            " b' Orientation',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' Online',\n",
                            " b' Services',\n",
                            " b' category',\n",
                            " b' won',\n",
                            " b' the',\n",
                            " b' Gold',\n",
                            " b' Med',\n",
                            " b'all',\n",
                            " b'ion',\n",
                            " b' ',\n",
                            " b' The',\n",
                            " b' Early',\n",
                            " b' College',\n",
                            " b' Video',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b'College',\n",
                            " b' Prom',\n",
                            " b'otional',\n",
                            " b' Video',\n",
                            " b' category',\n",
                            " b' won',\n",
                            " b' the',\n",
                            " b' Silver',\n",
                            " b' Med',\n",
                            " b'all',\n",
                            " b'ion',\n",
                            " b' ',\n",
                            " b' the',\n",
                            " b' five',\n",
                            " b' commercial',\n",
                            " b' series',\n",
                            " b' Thanks',\n",
                            " b' ',\n",
                            " b' won',\n",
                            " b' the',\n",
                            " b' Bronze',\n",
                            " b' Med',\n",
                            " b'all',\n",
                            " b'ion',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' Video',\n",
                            " b' Advertisement',\n",
                            " b'PS',\n",
                            " b'A',\n",
                            " b' Series',\n",
                            " b' category',\n",
                            " b' Experience',\n",
                            " b' ',\n",
                            " b'092',\n",
                            " b'013',\n",
                            " b' to',\n",
                            " b' Current',\n",
                            " b' Media',\n",
                            " b' Activities',\n",
                            " b' Specialist',\n",
                            " b' Company',\n",
                            " b' Name',\n",
                            " b' i',\n",
                            " b'14',\n",
                            " b' City',\n",
                            " b' State',\n",
                            " b' ',\n",
                            " b' Organ',\n",
                            " b'ize',\n",
                            " b' major',\n",
                            " b' campus',\n",
                            " b' events',\n",
                            " b' by',\n",
                            " b' overseeing',\n",
                            " b' security',\n",
                            " b' media',\n",
                            " b' services',\n",
                            " b' food',\n",
                            " b' services',\n",
                            " b' and',\n",
                            " b' marketing',\n",
                            " b' ',\n",
                            " b' Not',\n",
                            " b'able',\n",
                            " b' speakers',\n",
                            " b' in',\n",
                            " b' the',\n",
                            " b' past',\n",
                            " b' have',\n",
                            " b' included',\n",
                            " b' Neil',\n",
                            " b' de',\n",
                            " b' Gr',\n",
                            " b'asse',\n",
                            " b' Tyson',\n",
                            " b' host',\n",
                            " b' of',\n",
                            " b' Cosmos',\n",
                            " b' A',\n",
                            " b' Sp',\n",
                            " b'ac',\n",
                            " b'etime',\n",
                            " b' Odyssey',\n",
                            " b' and',\n",
                            " b' theoretical',\n",
                            " b' physicist',\n",
                            " b' Dr',\n",
                            " b'Mich',\n",
                            " b'io',\n",
                            " b' K',\n",
                            " b'aku',\n",
                            " b' ',\n",
                            " b' Ass',\n",
                            " b'isting',\n",
                            " b' academic',\n",
                            " b' departments',\n",
                            " b' with',\n",
                            " b' minor',\n",
                            " b' events',\n",
                            " b' such',\n",
                            " b' as',\n",
                            " b' conferences',\n",
                            " b' and',\n",
                            " b' speakers',\n",
                            " b' that',\n",
                            " b' require',\n",
                            " b' smaller',\n",
                            " b' venues',\n",
                            " b' ',\n",
                            " b' Create',\n",
                            " b' videos',\n",
                            " b' for',\n",
                            " b' various',\n",
                            " b' departments',\n",
                            " b' on',\n",
                            " b' campus',\n",
                            " b' for',\n",
                            " b' academic',\n",
                            " b' and',\n",
                            " b' recruitment',\n",
                            " b' purposes',\n",
                            " b' ',\n",
                            " b'031',\n",
                            " b'996',\n",
                            " b' to',\n",
                            " b' ',\n",
                            " b'082',\n",
                            " b'013',\n",
                            " b' Marketing',\n",
                            " b' Coordinator',\n",
                            " b' ',\n",
                            " b' Eng',\n",
                            " b'aged',\n",
                            " b' in',\n",
                            " b' strategic',\n",
                            " b' planning',\n",
                            " b' with',\n",
                            " b' de',\n",
                            " b'ans',\n",
                            " b' and',\n",
                            " b' department',\n",
                            " b' heads',\n",
                            " b' to',\n",
                            " b' increase',\n",
                            " b' enrollment',\n",
                            " b' and',\n",
                            " b' public',\n",
                            " b' awareness',\n",
                            " b' of',\n",
                            " b' new',\n",
                            " b' academic',\n",
                            " b' programs',\n",
                            " b' ',\n",
                            " b' Util',\n",
                            " b'ized',\n",
                            " b' focus',\n",
                            " b' groups',\n",
                            " b' surveys',\n",
                            " b' and',\n",
                            " b' other',\n",
                            " b' market',\n",
                            " b' research',\n",
                            " b' and',\n",
                            " b' analysis',\n",
                            " b' tools',\n",
                            " b' to',\n",
                            " b' develop',\n",
                            " b' strategy',\n",
                            " b' ',\n",
                            " b' Supported',\n",
                            " b' branding',\n",
                            " b' via',\n",
                            " b' press',\n",
                            " b' releases',\n",
                            " b' copy',\n",
                            " b' for',\n",
                            " b' radio',\n",
                            " b' and',\n",
                            " b' TV',\n",
                            " b' ads',\n",
                            " b' extensive',\n",
                            " b' website',\n",
                            " b' content',\n",
                            " b' and',\n",
                            " b' print',\n",
                            " b' ads',\n",
                            " b' bro',\n",
                            " b'ch',\n",
                            " b'ures',\n",
                            " b' f',\n",
                            " b'liers',\n",
                            " b' posters',\n",
                            " b' and',\n",
                            " b' bill',\n",
                            " b'boards',\n",
                            " b' ',\n",
                            " b' Managed',\n",
                            " b' advertising',\n",
                            " b' budget',\n",
                            " b' for',\n",
                            " b' print',\n",
                            " b' and',\n",
                            " b' electronic',\n",
                            " b' media',\n",
                            " b' up',\n",
                            " b' to',\n",
                            " b' ',\n",
                            " b'500',\n",
                            " b'000',\n",
                            " b' ',\n",
                            " b' Proof',\n",
                            " b'ed',\n",
                            " b' and',\n",
                            " b' edited',\n",
                            " b' materials',\n",
                            " b' for',\n",
                            " b' publication',\n",
                            " b' ',\n",
                            " b' Superv',\n",
                            " b'ised',\n",
                            " b' staff',\n",
                            " b' of',\n",
                            " b' seven',\n",
                            " b' comprised',\n",
                            " b' of',\n",
                            " b' three',\n",
                            " b' graphic',\n",
                            " b' artists',\n",
                            " b' three',\n",
                            " b' web',\n",
                            " b' designers',\n",
                            " b' and',\n",
                            " b' the',\n",
                            " b' office',\n",
                            " b' manager',\n",
                            " b' ',\n",
                            " b' Recru',\n",
                            " b'ited',\n",
                            " b' and',\n",
                            " b' ment',\n",
                            " b'ored',\n",
                            " b' students',\n",
                            " b' who',\n",
                            " b' represented',\n",
                            " b' the',\n",
                            " b' college',\n",
                            " b' at',\n",
                            " b' special',\n",
                            " b' events',\n",
                            " b' ',\n",
                            " b' W',\n",
                            " b'rote',\n",
                            " b' scripts',\n",
                            " b' and',\n",
                            " b' recruited',\n",
                            " b' talent',\n",
                            " b' for',\n",
                            " b' the',\n",
                            " b' Colleges',\n",
                            " b' radio',\n",
                            " b' and',\n",
                            " b' television',\n",
                            " b' commercials']"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import tiktoken\n",
                "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "sample_encode = tokenizer.encode(df.chunk_text[0]) \n",
                "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
                "decode\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "azdata_cell_guid": "31d80228-e140-413a-b1e3-c320a42b1f6c",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "500"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(decode)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ac19ee49-763a-4e2e-8c31-9ce1ba77bbe4"
            },
            "source": [
                "# **PART 2 : Generating Embeddings for Text Chunks using Azure Open AI**\n",
                "\n",
                "- After extracting and chunking the text from PDF resumes, we will generate embeddings for each chunk. These embeddings are numerical representations of the text that capture its semantic meaning. By creating embeddings for the text chunks, we can perform advanced similarity searches and enhance language model generation.\n",
                "\n",
                "- We will use the Azure OpenAI API to generate these embeddings. The `get_embedding` function defined below takes a piece of text as input and returns its embedding using the `text-embedding-small` model\n",
                "\n",
                "- Ensure the Environment Variables are set correctly in the .env file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "3aeda057-d0c0-40cd-bdcf-723abfed94e2",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                            filename chunkid  \\\n",
                        "0  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...     0_0   \n",
                        "1  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...     0_1   \n",
                        "2  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...     1_0   \n",
                        "3  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...     1_1   \n",
                        "4  w:\\_git\\_owned\\azure-sql-db-vector-search\\RAG-...     2_0   \n",
                        "\n",
                        "                                               chunk  \\\n",
                        "0  MEDIA ACTIVITIES SPECIALIST Summary MultiTaski...   \n",
                        "1  Worked with local production companies to crea...   \n",
                        "2  DIGITAL MEDIA SALES CONSULTANT Summary Dedicat...   \n",
                        "3  national rates Classified Private Party Rep Ja...   \n",
                        "4  ENGINEERING LAB TECHNICIAN Career Focus My mai...   \n",
                        "\n",
                        "                                           embedding  \n",
                        "0  [0.018623829, -0.041176733, 0.0022002836, 0.02...  \n",
                        "1  [-0.023771953, -0.042296108, 0.011301303, 0.02...  \n",
                        "2  [-0.0008766432, -0.0005247905, 0.030398797, 0....  \n",
                        "3  [0.0025675627, -0.015229849, 0.053091597, 0.02...  \n",
                        "4  [-0.011730898, 0.018166818, 0.043292753, -0.02...  \n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import requests\n",
                "from num2words import num2words\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Specify your model name\n",
                "openai_embedding_model = os.getenv(\"AZOPENAI_EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n",
                "\n",
                "# Assuming openai_url and openai_key are your environment variables\n",
                "openai_url = os.getenv(\"AZOPENAI_ENDPOINT\") + \"openai/deployments/\" + openai_embedding_model + \"/embeddings?api-version=2023-05-15\"\n",
                "openai_key = os.getenv(\"AZOPENAI_API_KEY\")\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\n",
                "\n",
                "    Args:\n",
                "        text (str): Text to embed.\n",
                "\n",
                "    Returns:\n",
                "        list: A list containing the embedding.\n",
                "    \"\"\"\n",
                "    response = requests.post(openai_url,\n",
                "        headers={\"api-key\": openai_key, \"Content-Type\": \"application/json\"},\n",
                "        json={\"input\": [text]}  # Embed the extracted chunk\n",
                "    )\n",
                "    \n",
                "    if response.status_code == 200:\n",
                "        response_json = response.json()\n",
                "        embedding = json.loads(str(response_json['data'][0]['embedding']))\n",
                "        return embedding\n",
                "    else:\n",
                "        return None\n",
                "\n",
                "# Example usage\n",
                "all_filenames = []\n",
                "all_chunkids = []\n",
                "all_chunks = []\n",
                "all_embeddings = []\n",
                "\n",
                "# Assuming df is already defined with the required columns\n",
                "for index, row in df.iterrows():\n",
                "    filename = row['file_name']\n",
                "    chunkid = row['unique_chunk_id']\n",
                "    chunk = row['chunk_text']\n",
                "    embedding = get_embedding(chunk)\n",
                "    \n",
                "    if embedding is not None:\n",
                "        all_filenames.append(filename)\n",
                "        all_chunkids.append(chunkid)\n",
                "        all_chunks.append(chunk)\n",
                "        all_embeddings.append(embedding)\n",
                "    \n",
                "    if (index + 1) % 200 == 0:  # Print progress every 200 rows\n",
                "        print(f\"Completed {index + 1} rows\")\n",
                "\n",
                "# Create a new DataFrame with the results\n",
                "result_df = pd.DataFrame({\n",
                "    'filename': all_filenames,\n",
                "    'chunkid': all_chunkids,\n",
                "    'chunk': all_chunks,\n",
                "    'embedding': all_embeddings\n",
                "})\n",
                "\n",
                "print(result_df.head(5))  # Display the first few rows of the dataframe\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "04d42351-41ec-4ce9-9778-fe4ea2ecb8a2"
            },
            "source": [
                "# **PART 3 : Using Azure SQL DB as a Vector Database to store and query embeddings**\n",
                "\n",
                "### **Load the embeddings into the Vector Database : Azure SQL DB**\n",
                "\n",
                "First let us define a function to connect to Azure SQLDB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "azdata_cell_guid": "930a63bc-4c08-4205-b152-b1ad5c82057a",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#lets define a function to connect to SQLDB\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "def get_mssql_connection():\n",
                "    # Retrieve the connection string from the environment variables\n",
                "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\n",
                "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\n",
                "\n",
                "    # Determine the authentication method and connect to the database\n",
                "    if entra_connection_string:\n",
                "        # Entra ID Service Principal Authentication\n",
                "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \n",
                "        token = credential.get_token('https://database.windows.net/.default')\n",
                "        token_bytes = token.token.encode('UTF-16LE')\n",
                "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
                "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\n",
                "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
                "    elif sql_connection_string:\n",
                "        # SQL Authentication\n",
                "        conn = pyodbc.connect(sql_connection_string)\n",
                "    else:\n",
                "        raise ValueError(\"No valid connection string found in the environment variables.\")\n",
                "\n",
                "    return conn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e929c112-65d4-46f1-a9a2-7c9434b2d7cb",
                "language": "python"
            },
            "source": [
                "### **Insert embeddings into the native 'Vector' Data Type**\n",
                "\n",
                "We will insert our vectors into the SQL Table now. Azure SQL DB now has a dedicated, native, data type for storing vectors: the `vector` data type. Read about the preview [here](https://devblogs.microsoft.com/azure-sql/eap-for-vector-support-refresh-introducing-vector-type)\n",
                "\n",
                "The table embeddings has a column called vector which is vector(1536) type. Ensure you have created the table using the script `CreateTable.sql` before running the below code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "680259d9-77ce-4b63-b412-9bea33bb0f43",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data inserted successfully into the 'resumedocs' table.\n"
                    ]
                }
            ],
            "source": [
                "import pyodbc\n",
                "import pandas as pd\n",
                "\n",
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                "\n",
                "# Loop through the DataFrame rows and insert them into the table\n",
                "for index, row in result_df.iterrows():\n",
                "    chunkid = row['chunkid']\n",
                "    filename = row['filename']\n",
                "    chunk = row['chunk']\n",
                "    embedding = row['embedding']\n",
                "    \n",
                "    # Use placeholders for the parameters in the SQL query\n",
                "    query = f\"\"\"\n",
                "    INSERT INTO resumedocs (chunkid, filename, chunk, embedding)\n",
                "    VALUES (?, ?, ?, CAST(? AS VECTOR(1536)))\n",
                "    \"\"\"\n",
                "    # Execute the query with the parameters\n",
                "    cursor.execute(query, chunkid, filename, chunk, json.dumps(embedding))\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "\n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'resumedocs' table.\")\n",
                "\n",
                "# Close the connection\n",
                "conn.close()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ede638b9-d681-4cb9-9ab8-9651e3099a36",
                "language": "python"
            },
            "source": [
                "Let's take a look at the data in the Resume Docs table:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "azdata_cell_guid": "88d1a90e-e93c-426e-934d-fb1a47dfd900",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------------------+---------+----------------------+----------------------+\n",
                        "|       filename       | chunkid |        chunk         |      embedding       |\n",
                        "+----------------------+---------+----------------------+----------------------+\n",
                        "| w:\\_git\\_owned\\azure |   0_0   | MEDIA ACTIVITIES SPE | [1.8623829e-002,-4.1 |\n",
                        "| w:\\_git\\_owned\\azure |   0_1   | Worked with local pr | [-2.3771953e-002,-4. |\n",
                        "| w:\\_git\\_owned\\azure |   1_0   | DIGITAL MEDIA SALES  | [-8.7664317e-004,-5. |\n",
                        "| w:\\_git\\_owned\\azure |   1_1   | national rates Class | [2.5675627e-003,-1.5 |\n",
                        "| w:\\_git\\_owned\\azure |   2_0   | ENGINEERING LAB TECH | [-1.1730898e-002,1.8 |\n",
                        "| w:\\_git\\_owned\\azure |   3_0   | EQUIPMENT ENGINEERIN | [1.4169044e-002,1.47 |\n",
                        "| w:\\_git\\_owned\\azure |   3_1   | Operates repair and  | [-2.8785424e-002,1.1 |\n",
                        "| w:\\_git\\_owned\\azure |   3_2   | City State  Expedite | [3.8580999e-003,2.69 |\n",
                        "| w:\\_git\\_owned\\azure |   4_0   | INFORMATION TECHNOLO | [-2.6888244e-002,1.9 |\n",
                        "| w:\\_git\\_owned\\azure |   4_1   | Disaster Recovery pl | [-2.9838305e-002,2.8 |\n",
                        "+----------------------+---------+----------------------+----------------------+\n"
                    ]
                }
            ],
            "source": [
                "from prettytable import PrettyTable\n",
                "\n",
                "import pyodbc\n",
                "import pandas as pd\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the connection string from the environment variables\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Use placeholders for the parameters in the SQL query\n",
                "query = \"SELECT TOP(10) filename, chunkid, chunk, CAST(embedding AS NVARCHAR(MAX)) as embedding FROM dbo.resumedocs ORDER BY Id\"\n",
                "\n",
                "# Execute the query with the parameters\n",
                "cursor.execute(query)\n",
                "queryresults = cursor.fetchall()\n",
                "\n",
                "# Get column names from cursor.description\n",
                "column_names = [column[0] for column in cursor.description]\n",
                "\n",
                "# Create a PrettyTable object\n",
                "table = PrettyTable()\n",
                "\n",
                "# Add column names to the table\n",
                "table.field_names = column_names\n",
                "\n",
                "# Set max width for each column to truncate data\n",
                "table.max_width = 20\n",
                "\n",
                "# Add rows to the table\n",
                "for row in queryresults:\n",
                "    # Truncate each value to 20 characters\n",
                "    truncated_row = [str(value)[:20] for value in row]\n",
                "    table.add_row(truncated_row)\n",
                "\n",
                "# Print the table\n",
                "print(table)\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "# Close the connection\n",
                "conn.close()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b7b0fda6-3322-4bbc-8d08-d0b567da79ec",
                "language": "python"
            },
            "source": [
                "### **Performing Vector Similarity Search in Azure SQL DB using VECTOR\\_DISTANCE built in function**\n",
                "\n",
                "Let's now query our ResumeDocs table to get the top similar candidates given the User search query.\n",
                "\n",
                "What we are doing: Given any user search query, we can obtain the vector representation of that text. We then use this vector to calculate the cosine distance against all the resume embeddings stored in the database. By selecting only the closest matches, we can identify the resumes most relevant to the user’s query. This helps in finding the most suitable candidates based on their resumes.\n",
                "\n",
                "The most common distance is the cosine similarity, which can be calculated quite easily in SQL with the help of the new distance functions.\n",
                "\n",
                "```\n",
                "VECTOR_DISTANCE('distance metric', V1, V2)\n",
                "\n",
                "```\n",
                "\n",
                "We can use **cosine**, **euclidean**, and **dot** as the distance metric today.\n",
                "\n",
                "We will define the function `vector_search_sql`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "azdata_cell_guid": "1b4f0ca2-2401-4f90-a44d-75dce03500cc",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10089434.pdf', '4_1', 'Disaster Recovery plan and procedures  Researching evaluating and recommending new hardware and new software  Communicating and defining systems design and requirements for new and existing systems and applications  Researching evaluating recommending testing and implementing third party softwareutilities  Planning and designing network infrastructure changes  addingremoving servers appliances network logical flow  Reviewing evaluating and analyzing existing system and application viability with management and staff  Administering and maintaining shares on the file servers  Reviewing server logs to troubleshoot issues  Scheduling and applying hot fixes and security patches on the server infrastructure which includes the operating system and application software  Reviewing systems reporting in SCCM System Center Configuration Manager  Resolving service requests escalated by the Help Desk or other technicians  Troubleshooting and analyzing and system problems for root cause analysis  Giving and participating in training and education programs to explain upgrades to end users  Migrating users documents from local computer storage to shares on the file servers  Configuring supporting and maintaining file shares using Distributed File System DFS  Managing implementing and testing Enterprise backup infrastructure systems such as the Symantec Veritas Netbackup Symantec Backup Exec System RecoveryLivestate and VRanger backup servers  Managing configuring and supporting DataDomain storage  Configuring and supporting Microsoft Windows Server 2003 2008 and 2012  Installing configuring and supporting Microsoft Windows 7 Windows 8 and Microsoft Office 2007 2010 and 2013  Installing configuring and supporting McAfee antivirus software on servers  Migrating Exchange infrastructure from Exchange 2003 to Exchange 2007 and from Exchange 2007 to Exchange 2010  Supporting servers in the virtualization infrastructure using VMware vSphere  Installing configuring and testing Veeam virtual machine backup software and Virtual Desktop Infrastructure VDI  Reviewing systems reporting in System Center Configuration Manager SCCM  Administering and maintaining the Symantec Enterprise Vault servers  Managing the Active Directory Domain Controllers DCs  Creating and maintaining Group Policy Objects GPOs in Microsoft Active Directory  Configuring and supporting Microsoft Exchange Active Sync on devices with Apple iOS and Android mobile operating systems  Configuring and supporting Blackberry devices on the Blackberry Enterprise Server to receive Exchange email  Developing testing designing and implementing application scripts using languages such as command batch files Visual Basic Script and PowerShell  Creating policies and procedural documentation Information Services Liaison T Aug 2005 to Aug 2007 Company Name i14 City State  Troubleshooting hardware and software problems over the telephone and through remote PC administration software', 0.4477695138727432, 0.5522304861272568),\n",
                            " ('w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10089434.pdf', '4_0', 'INFORMATION TECHNOLOGY TECHNICIAN I Summary Versatile Systems Administrator possessing superior troubleshooting skills for networking issues end user problems and network security Experienced in server management systems analysis and offering indepth understanding of IT infrastructure areas Detailoriented independent and focused on taking a systematic approach to solving complex problems Demonstrated exceptional technical knowledge and skills while working with various teams to achieve shared goals and objectives Highlights  Active Directory  New technology and product research  Group Policy Objects  Office 365 and Azure  PowerShell and VBScript  Storage management  Microsoft Exchange  Enterprise backup management  VMWare experience  Disaster recovery Experience Information Technology Technician I Aug 2007 to Current Company Name i14 City State  Migrating and managing user accounts in Microsoft Office 365 and Exchange Online  Creating and managing virtual machines for systems such as domain controllers and Active Directory Federation Services ADFS in Microsoft Windows Azure IaaS  Creating and managing storage in Microsoft Windows Azure IaaS  Installing and configuring StorSimple iSCSI cloud array STaaSBaaS  Installing configuring and testing Twinstrata iSCSI cloud array STaaSBaaS  Collaborating on project plan for Office 365 migration  Developing detailed specifications for the Office 365 migration including businesscase documentation cost benefit analyses technical diagrams and work flow documentation  Received training in MVC 4 for Visual Studio using  Net Framework 445 to develop application using HTML5 and CSS3  Installing configuring and supporting Linux machines for the open WiFi network project  Compiling and generating statistical information concerning wireless network traffic using Cacti  Configuring wireless LAN router networking and security access  Installing and configuring wireless certificates  Developing detailed specifications for the acquisition of an Enterprise backup system including systems design businesscase documentation cost benefit analysis technical diagrams and work flow documentation  Reviewing evaluating and analyzing departmental policies guidelines procedures and standards with management and staff  Developing test scripts for acceptance unit and system testing of Hyperion Phase 1 and MiamiBiz Phase 2  Developing Quality Assurance and testing plan for Hyperion Phase 1 and MiamiBiz Phase 2  Debugging and logging of errors in Hyperion and MiamiBiz using Team Foundation Server TFS  Participated in various phases of the project life cycle such as determining requirements design conceptualization testing implementation deployment and release for the Hyperion and MiamiBiz projects  Collaborating on project plans for Hyperion and MiamiBiz  Preparing presentations and documentation to demonstrate Hyperion and MiamiBiz functionality or design  Monitoring network traffic and compiling and generating statistical information using Solar Winds  Collaborating on', 0.44270843563698403, 0.557291564363016),\n",
                            " ('w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10089434.pdf', '4_2', 'Installing configuring and supporting McAfee antivirus software on desktops  Installing configuring and supporting BBars computer backup software  Developing and maintaining websites on servers running Microsoft Sharepoint Server and Internet Information Services IIS  Supporting Systems Management Server SMS  Troubleshooting LAN WAN Internet and Intranet network and security access  Troubleshooting network connectivity issues related to TCPIP Domain Name Service DNS Dynamic Host Configuration Protocol DHCP protocols Internet Security and Acceleration ISA proxy server and VPN  Troubleshooting web applicationpage issues client browsers and related software  Administering and maintaining of end user accounts permissions and access rights in in Microsoft Active Directory  Administering and maintaining of NTFS security permissions on the file servers  Installing configuring and maintaining hardware such as servers workstations laptops printers and scanners in a Windows Enterprise environment  Installing configuring and supporting printers on the print servers  Installing configuring and supporting Microsoft Windows Server 2000 and 2003 Microsoft Windows XP and Windows Vista and Microsoft Office XP 2003 and 2007 Education Bachelor of Science  Information Technology 2005 Florida International Univeristy i14 City State United States  Coursework in Programming Web Administration Network Administration Database Administration and Systems Administration  Linux  Programming Languages C Java JSP HTML CSS VBNet Bash TSQL Certifications CompTIA Network  2014 Skills Active Directory Azure antivirus Backup Exec backup Bash batch Cacti Cisco ASA databases DHCP DNS documentation DataDomain EMC Enterprise Vault ePO file servers firewall GPO HTML IIS ISA LDAP Linux McAfee Exchange Microsoft Office Microsoft Windows security policies PowerShell programming proxy server servers scripts SolarWinds SQL StorSimple troubleshooting TMG Ubuntu Visual Basic Script VBS Veritas Netbackup VPN VRanger Veeam VMWare VDI virtual manchine NMap ZenMap', 0.42152277786839787, 0.5784772221316021),\n",
                            " ('w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10247517.pdf', '5_1', 'network which entailed changing software and LAN cabling for 20 existing switches including reconfiguring the core switch with remote wiring closets with multiple stack units  Led a team of five network and equipment support specialistsConfigured maintained analyzed and troubleshot Nortel and Juniper networks systems and equipment WANLANWLAN infrastructure servers PCs notebooks thin clients printers and other peripherals used by 600 workers in GeorgiaSupported 200 units configured for operation through client serversAdvanced global company objectives through participation in IT and telecom design and implementationAdvanced to this role through promotions from Mechanical Drafter Engineering Checker Mechanical Designer and Network AdministratorSlashed support requests 75 cut equipment costs 50 and boosted work productivity 30 by applying nearzero configuration to develop install and implement a thin client solution for 90 plant manufacturing cells  Reduced standard company imaging and system deployment time 60 before stationspecific customization by replacing andor refreshing 40 PCs each quarter 100 on schedule  Planned designed installed configured and tested wireless networks WLAN from Cisco Nortel and Meraki in multiple facilities documented standards for US operations trained users and monitored performance  Replaced 125 field and service laptops by programming standard images and custom configurations on each machine completing the six week project on schedule despite disruption of parts availability due to a tsunami  Championed the use of computers in manufacturing cells to provide drawings and also facilitate order tracking in conjunction with the ERP system designed and implemented automated pick list and shipping documents as well as realtime tracking for distribution  Trained and mentored 15 new department staff members and contractors 021753 DesignerDrafter Company Name i14 City  State Plant ManagerQuality Control Manager Company Name 14 City State Drafter Company Name i14 City State Accomplishments 1 INVICTOSON WORDEXCEL ACCESSOULOOKPowerromy VISIO PuoIsnerProject Apple Pages Numbers Keynote Autodesk AUTOCAD SolidWorks SAP r3 Operating Systems Microsoft SQL Microsoft Exchange SCCM DOS 622 Windows 31Windows 10 Windows NT 35 Server Windows Server 2012 OSX 1021011 IOS Hardware and WLAN Nortel Mitel Shortel Cisco Meraki Apple AirPort Palo AltoFirewall Sonicwall Firewall Peripherals Barracuda Web Filter Barracuda Message Archiver RiverbedWAN Accelerator Dell ISCSI SAN Installation and Configuration Dell PowerVault Adtran Routers Cisco Routers Catalyst Switches HP Procurve Switches ICC Patch Panels Cross Connects Cat36A Phone and Data Education Master of Science  Leadership Walden University', 0.41778588574476205, 0.582214114255238),\n",
                            " ('w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10247517.pdf', '5_0', 'INFORMATION TECHNOLOGY MANAGER Professional Summary Possesses an extensive background in Information Technology Management along with a Masters of Science degree and multiple certifications Excels in planning implementing and evaluating the systems infrastructure and staffing necessary to execute complex initiatives and meet deadlines in dynamic fastpaced environments adept at overseeing and participating in the installation configuration maintenance and upgrade of networks hardware servers and peripherals Detailoriented and decisive applies communication and leadership skills to interface effectively with all levels of an organization Expertise spans Network Engineering Helpdesk Administration Software Licensing Disaster Recovery BackupRestore OperationsProject Management Strategic PlanningAnalysis Budgeting TeamBuildingTraining Vendor Relations PolicyProcedure Development Quality Assurance Troubleshooting Problem Solving Process Improvement Experience 042013 to Current Information Technology Manager Company Name i14 City  State  A bankruptcy Trustee office handling Chapter 13 and Chapter 12 cases for the Northern District of Georgia Manages application databasehardware systems used to track Chapter 13 bankruptcy cases all office technologies information systems and antispamanti virus servers  Evaluates recommends implements supports and troubleshoots hardware and software  Maintains LANWAN infrastructure connectivity and security as well as LAN user documentation  Performs disaster recovery planning  Administers licenses and service contracts schedules service visits from vendors and suppliers  Defines documents assesses and updates ITrelated procedures as needed  Creates and maintains user accounts including email  Inspects all employees computers quarterly to ensure compliance of configuration and settings with office policies  Coordinates special projects  Designed and implemented network infrastructure enhancements to improve performance security remote access and connectivity  Standardized hardware peripherals and software  Established a helpdesk support system for monitoring prioritizing and scheduling requests  Spearheaded the deployment of a new tracking system that provided performance metrics used to identify where additional training and resources were required  Researched installed and configured an enterprise backup solution for disaster recovery that included features to allow users to recover deleted items without the need to recover from DR backups  Doubled WAN bandwidth and propelled LAN and WLAN performance to 100 or higher through WAN LAN and WLAN reconfiguration  Improved accuracy more than doubled efficiency from 35 to 87 and increased cost savings from 250quarter to 2500quarter by developing an automated script for retrieving data from an outside server  Automated asset tracking by employing a hand scanner and database system to improve efficiency and accuracy 081987 to 062012 Network Analyst Company Name i14 City State  A 1B global manufacturer of equipment for precision material dispensing testing inspection surface preparation and curing Fueled a 1000 improvement in LAN performance by replacing the existing', 0.41279846608519477, 0.5872015339148052)]"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import pyodbc\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "def vector_search_sql(query, num_results=5):\n",
                "    # Load environment variables from .env file\n",
                "    load_dotenv()\n",
                "\n",
                "    # Use the get_mssql_connection function to get the connection string details\n",
                "    conn = get_mssql_connection()\n",
                "\n",
                "    # Create a cursor object\n",
                "    cursor = conn.cursor()\n",
                "\n",
                "    # Generate the query embedding for the user's search query\n",
                "    user_query_embedding = get_embedding(query)\n",
                "    \n",
                "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\n",
                "    sql_similarity_search = f\"\"\"\n",
                "    SELECT TOP(?) filename, chunkid, chunk,\n",
                "           1-vector_distance('cosine', CAST(? AS VECTOR(1536)), embedding) AS similarity_score,\n",
                "           vector_distance('cosine', CAST(? AS VECTOR(1536)), embedding) AS distance_score\n",
                "    FROM dbo.resumedocs\n",
                "    ORDER BY distance_score \n",
                "    \"\"\"\n",
                "\n",
                "    cursor.execute(sql_similarity_search, num_results, json.dumps(user_query_embedding), json.dumps(user_query_embedding))\n",
                "    results = cursor.fetchall()\n",
                "\n",
                "    # Close the database connection\n",
                "    conn.close()\n",
                "\n",
                "    return results\n",
                "\n",
                "vector_search_sql(\"system administrator\", num_results=5)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1e194f3c-6a7a-4f16-95ec-05f60a3770a4",
                "language": "python"
            },
            "source": [
                "# **Part 4 : Use embeddings retrieved from a Azure SQL vector database to augment LLM generation**\n",
                "\n",
                "Lets create a helper function to feed prompts into the [Completions model](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4) & create interactive loop where you can pose questions to the model and receive information grounded in your data.\n",
                "\n",
                "The function `generate_completion` is defined to help ground the gpt-4o model with prompts and system instructions.   \n",
                "Note that we are passing the results of the `vector_search_sql` we defined earlier to the model and we define the system prompt .  \n",
                "We are using gpt-4o model here. \n",
                "\n",
                "You can get more information on using Azure Open AI GPT chat models [here](https://learn.microsoft.com/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "azdata_cell_guid": "2865a0d0-2ee3-4d0a-8ee8-89075ab541bc",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Load environment variables from a .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Use environment variables for the API key and endpoint\n",
                "api_key = os.getenv(\"AZOPENAI_API_KEY\")\n",
                "azure_endpoint = os.getenv(\"AZOPENAI_ENDPOINT\")\n",
                "chat_model = os.getenv(\"AZOPENAI_CHAT_MODEL_DEPLOYMENT_NAME\")\n",
                "\n",
                "# Create a chat completion request\n",
                "client = AzureOpenAI(\n",
                "    api_key=api_key,\n",
                "    api_version=\"2023-05-15\",\n",
                "    azure_endpoint=azure_endpoint\n",
                ")\n",
                "\n",
                "def generate_completion(search_results, user_input):\n",
                "    system_prompt = '''\n",
                "You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\n",
                "- Use the information from `search_results` to generate your top 3 responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\n",
                "  File: {filename}\n",
                "  Chunk ID: {chunkid}\n",
                "  Similarity Score: {similarity_score}\n",
                "  Add a small snippet from the Relevant Text: {chunktext}\n",
                "  Do not use the entire chunk\n",
                "- Avoid any other external data sources.\n",
                "- Add a summary about why the candidate maybe a goodfit even if exact skills and the role being hired for are not matching , at the end of the recommendations. Ensure you call out which skills match the description and which ones are missing. If the candidate doesnt have prior experience for the hiring role which we may need to pay extra attention to during the interview process.\n",
                "- Add a Microsoft related interesting fact about the technology that was searched \n",
                "'''\n",
                "\n",
                "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
                "    \n",
                "    # Create an empty list to store the results\n",
                "    result_list = []\n",
                "\n",
                "    # Iterate through the search results and append relevant information to the list\n",
                "    for result in search_results:\n",
                "        filename = result  # Assuming filename is the first column\n",
                "        chunkid = result\n",
                "        chunktext = result\n",
                "        similarity_score = result  # Assuming similarity_score is the third column\n",
                "        \n",
                "        # Append the relevant information as a dictionary to the result_list\n",
                "        result_list.append({\n",
                "            \"filename\": filename,\n",
                "            \"chunkid\": chunkid,\n",
                "            \"chunktext\": chunktext,\n",
                "            \"similarity_score\": similarity_score\n",
                "        })\n",
                "\n",
                "    # Print the result list\n",
                "    #print(result_list)\n",
                "    \n",
                "    messages.append({\"role\": \"system\", \"content\": f\"{result_list}\"})\n",
                "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
                "    response = client.chat.completions.create(model=chat_model, messages=messages, temperature=0) #replace with your model deployment name\n",
                "\n",
                "    return response.dict()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "azdata_cell_guid": "d69ab285-0cc9-4596-95f2-688d0c324f6b",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\n",
                        "\n",
                        "\n",
                        "User asked: software developer\n",
                        "\n",
                        "AI's response:\n",
                        "File: w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10089434.pdf\n",
                        "Chunk ID: 4_0\n",
                        "Similarity Score: 0.3759350958545771\n",
                        "Add a small snippet from the Relevant Text: Versatile Systems Administrator possessing superior troubleshooting skills for networking issues, end user problems, and network security... Received training in MVC 4 for Visual Studio using .Net Framework 4/4.5 to develop application using HTML5 and CSS3...\n",
                        "\n",
                        "File: w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10089434.pdf\n",
                        "Chunk ID: 4_2\n",
                        "Similarity Score: 0.33812141061846746\n",
                        "Add a small snippet from the Relevant Text: Installing, configuring, and supporting McAfee antivirus software on desktops... Developing and maintaining websites on servers running Microsoft Sharepoint Server and Internet Information Services (IIS)... Programming Languages C, Java, JSP, HTML, CSS, VB.Net, Bash, T-SQL...\n",
                        "\n",
                        "File: w:\\\\_git\\\\_owned\\\\azure-sql-db-vector-search\\\\RAG-with-Documents\\\\.\\\\docs\\\\INFORMATION-TECHNOLOGY\\\\10247517.pdf\n",
                        "Chunk ID: 5_2\n",
                        "Similarity Score: 0.33928848890540797\n",
                        "Add a small snippet from the Relevant Text: Master of Science - Information Systems Management Project Management... Skills antivirus, backup, cabling, Cisco, hardware, database, disaster recovery planning, email, ERP, imaging, information systems, LAN, laptops, Mechanical access, office enterprise, Network Administrator, network, networks, Nortel, peripherals, printers, programming, realtime, scanner, scheduling, servers, script, shipping, switches, switch, user documentation, telecom, WAN, wiring...\n",
                        "\n",
                        "Summary:\n",
                        "The candidates from the search results have a mix of systems administration, network management, and software development skills. While none of the candidates have a direct mention of a \"Software Developer\" role, they do possess relevant skills such as programming in various languages (C, Java, JSP, HTML, CSS, VB.Net, Bash, T-SQL), experience with Microsoft technologies (Sharepoint, IIS, .Net Framework), and a background in information systems management. These skills could be transferable to a software development position, especially if the role involves working with Microsoft technologies or requires a strong foundation in systems administration and networking.\n",
                        "\n",
                        "The candidates may lack specific experience in software development projects or roles, which should be explored during the interview process. However, their demonstrated ability to learn and apply new technologies, as evidenced by training and certifications, suggests they could adapt to a software development role with some additional training or mentorship.\n",
                        "\n",
                        "Interesting Microsoft-related fact:\n",
                        "Microsoft's .NET Framework, mentioned in the training of one candidate, has been a staple for developers building applications on Windows. It provides a comprehensive and consistent programming model for building applications that have visually stunning user experiences and seamless and secure communication.\n"
                    ]
                }
            ],
            "source": [
                "# Create a loop of user input and model output to perform Q&A on the PDF's that are now chunked and stored in the SQL DB with embeddings\n",
                "#\n",
                "# PLEASE NOTE: An input box will be displayed for the user to enter a question/query at the top of the scree.\n",
                "# The model will then provide a response based on the data stored in the SQL DB.\n",
                "# Type 'end' to end the session.\n",
                "#\n",
                "print(\"*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\\n\")\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"User prompt: \")\n",
                "    if user_input.lower() == \"end\":\n",
                "        break\n",
                "\n",
                "    # Print the user's question\n",
                "    print(f\"\\nUser asked: {user_input}\")\n",
                "\n",
                "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\n",
                "    search_results = vector_search_sql(user_input)\n",
                "    completions_results = generate_completion(search_results, user_input)\n",
                "\n",
                "    # Print the model's response\n",
                "    print(\"\\nAI's response:\")\n",
                "    print(completions_results['choices'][0]['message']['content'])\n",
                "\n",
                "# The loop will continue until the user types 'end'\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
