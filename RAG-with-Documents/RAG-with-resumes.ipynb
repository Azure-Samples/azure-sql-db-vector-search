{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Leveraging Azure SQL DB’s Native Vector Capabilities for Enhanced Resume Matching with Azure Document Intelligence and RAG\n",
                "\n",
                "In this tutorial, we will explore how to leverage Azure SQL DB’s new vector data type to store embeddings and perform similarity searches using built-in vector functions, enabling advanced resume matching to identify the most suitable candidates. \n",
                "\n",
                "By extracting and chunking content from PDF resumes using Azure Document Intelligence, generating embeddings with Azure OpenAI, and storing these embeddings in Azure SQL DB, we can perform sophisticated vector similarity searches and retrieval-augmented generation (RAG) to identify the most suitable candidates based on their resumes.\n",
                "\n",
                "### **Tutorial Overview**\n",
                "\n",
                "- This Python notebook will teach you to:\n",
                "    1. **Chunk PDF Resumes**: Use **`Azure Document Intelligence`** to extract and chunk content from PDF resumes.\n",
                "    2. **Create Embeddings**: Generate embeddings from the chunked content using the **`Azure OpenAI API`**.\n",
                "    3. **Vector Database Utilization**: Store embeddings in **`Azure SQL DB`** utilizing the **`new Vector Data Type`** and perform similarity searches using built-in vector functions to find the most suitable candidates.\n",
                "    4. **LLM Generation Augmentation**: Enhance language model generation with embeddings from a vector database. In this case, we use the embeddings to inform a GPT-4 chat model, enabling it to provide rich, context-aware answers about candidates based on their resumes\n",
                "\n",
                "## Dataset\n",
                "\n",
                "We use a sample dataset from [Kaggle](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) containing PDF resumes for this tutorial. For the purpose of this tutorial we will use 120 resumes from the **Information-Technology** folder\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- **Azure Subscription**: [Create one for free](https://azure.microsoft.com/free/cognitive-services?azure-portal=true)\n",
                "- **Azure SQL Database**: [Set up your database for free](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql)\n",
                "- **Azure Document Intelligence** [Create a FreeAzure Doc Intelligence resource](https:/learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0)\n",
                "- **Azure Data Studio**: Download [here](https://azure.microsoft.com/products/data-studio) to manage your Azure SQL database and [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel)\n",
                "\n",
                "## Additional Requirements for Embedding Generation\n",
                "\n",
                "- **Azure OpenAI Access**: Apply for access in the desired Azure subscription at [https://aka.ms/oai/access](https://aka.ms/oai/access)\n",
                "- **Azure OpenAI Resource**: Deploy an embeddings model (e.g., `text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4.0` model for chat completion. Refer to the [resource deployment guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
                "- **Python**: Version 3.7.1 or later from Python.org. (Sample has been tested with Python 3.11)\n",
                "- **Python Libraries**: Install the required libraries openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken, and pyodbc.\n",
                "- **Jupyter Notebooks**: Use within [Azure Data Studio](https://learn.microsoft.com/en-us/azure-data-studio/notebooks/notebooks-guidance) or Visual Studio Code .\n",
                "\n",
                "Code snippets are adapted from the [Azure OpenAI Service embeddings Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python)\n",
                "\n",
                "## Getting Started\n",
                "\n",
                "1. **Database Setup**: Execute SQL commands from the `createtable.sql` script to create the necessary table in your database.\n",
                "2. **Model Deployment**: Deploy an embeddings model (`text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Note the 2 models deployment names for later use.\n",
                "\n",
                "![Deployed OpenAI Models](../Assets/modeldeployment.png)\n",
                "\n",
                "3. **Connection String**: Find your Azure SQL DB connection string in the Azure portal under your database settings.\n",
                "4. **Configuration**: Populate the `.env` file with your SQL server connection details , Azure OpenAI key and endpoint, Azure Document Intelligence key and endpoint values.\n",
                "\n",
                "You can retrieve the Azure OpenAI _endpoint_ and _key_:\n",
                "\n",
                "![Azure OpenAI Endpoint and Key](../Assets/endpoint.png)\n",
                "\n",
                "You can [retrieve](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) the Document Intelligence _endpoint_ and _key_:\n",
                "\n",
                "![Azure Document Intelligence Endpoint and Key](../Assets/docintelendpoint.png)\n",
                "\n",
                "## Running the Notebook\n",
                "\n",
                "To [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel), connect to your Azure SQL database using Azure Data Studio, which can be downloaded [here](https://azure.microsoft.com/products/data-studio)"
            ],
            "metadata": {
                "language": "sql",
                "azdata_cell_guid": "507219d1-d713-4c41-86d5-e938bf69627c"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#Setup the python libraries required for this notebook\r\n",
                "#Please ensure that you navigate to the directory containing the `requirements.txt` file in your terminal\r\n",
                "%pip install -r requirements.txt"
            ],
            "metadata": {
                "azdata_cell_guid": "fe37c601-5918-4055-badc-6c0ba90c68ce",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#Load the env details\r\n",
                "from dotenv import load_dotenv\r\n",
                "load_dotenv()"
            ],
            "metadata": {
                "azdata_cell_guid": "4c29709e-1c3a-495d-83ec-05737e220847",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {},
                    "execution_count": 2,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 1: Extracting and Chunking Text from PDF Resumes using Azure Document Intelligence**\n",
                "\n",
                "Create an instance of the [DocumentAnalysisClient](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) using the endpoint and API key. \n",
                "\n",
                "[Azure Document Intelligence](https://learn.microsoft.com/azure/ai-services/document-intelligence/?view=doc-intel-4.0.0_)(previously known as Form Recognizer) is a Azure cloud service that uses machine learning to analyze text and structured data from your documents. This client will be used to send requests to the [Azure Document Intelligence](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python) service and receive responses containing the extracted text from the PDF resumes."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "4b543f05-9036-4887-8737-09aa9f865ec2"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "import re\r\n",
                "from azure.ai.formrecognizer import DocumentAnalysisClient\r\n",
                "from azure.core.credentials import AzureKeyCredential\r\n",
                "\r\n",
                "# Load environment variables\r\n",
                "endpoint = os.getenv(\"azuredocintelligence_endpoint\")\r\n",
                "api_key = os.getenv(\"azuredocintelligence_api_key\")\r\n",
                "\r\n",
                "# Create a DocumentAnalysisClient\r\n",
                "document_analysis_client = DocumentAnalysisClient(\r\n",
                "    endpoint=endpoint,\r\n",
                "    credential=AzureKeyCredential(api_key)\r\n",
                ")\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "b20cc66a-50ce-4486-b275-d4683f4ba545"
            },
            "outputs": [],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Analyze input documents using prebuilt model in Azure Document Intelligence**\n",
                "\n",
                "- DocumentAnalysisClient provides operations for analyzing input documents using prebuilt and custom models through the `begin_analyze_document` and `begin_analyze_document_from_url` APIs. In this tutorial we are using the [prebuilt-layout](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python#using-prebuilt-models)\n",
                "    \n",
                "\n",
                "### **Split text into chunks of 500 tokens**\n",
                "\n",
                "- When faced with content that exceeds the embedding limit, we usually also chunk the content into smaller pieces and then embed those one at a time. Here we will use [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file) to chunk the extracted text into token sizes of 500, as we will later pass the extracted chunks to to the `text-embedding-small` model for [generating text embeddings](https://learn.microsoft.com/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python) as this has a model input token limit of 8192.\n",
                "\n",
                "**Note**: You need to provide the location of the folder where the PDF files reside in the below script."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "1ecc04b0-1a5f-4d48-819d-2cc06a070062"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "import re\r\n",
                "import pandas as pd\r\n",
                "import tiktoken\r\n",
                "\r\n",
                "# Function to read PDF files and extract text using Azure AI Document Intelligence\r\n",
                "def extract_text_from_pdf(pdf_path):\r\n",
                "    with open(pdf_path, \"rb\") as f:\r\n",
                "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", document=f)\r\n",
                "    result = poller.result()\r\n",
                "    text = \"\"\r\n",
                "    for page in result.pages:\r\n",
                "        for line in page.lines:\r\n",
                "            text += line.content + \" \"\r\n",
                "    return text\r\n",
                "\r\n",
                "# Function to clean text and remove special characters\r\n",
                "def clean_text(text):\r\n",
                "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\r\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\r\n",
                "    return text\r\n",
                "\r\n",
                "# Function to split text into chunks of 500 tokens\r\n",
                "def split_text_into_token_chunks(text, max_tokens=500):\r\n",
                "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\r\n",
                "    tokens = tokenizer.encode(text)\r\n",
                "    chunks = []\r\n",
                "    \r\n",
                "    for i in range(0, len(tokens), max_tokens):\r\n",
                "        chunk_tokens = tokens[i:i + max_tokens]\r\n",
                "        chunk_text = tokenizer.decode(chunk_tokens)\r\n",
                "        chunks.append(chunk_text)\r\n",
                "    \r\n",
                "    return chunks\r\n",
                "\r\n",
                "# Path to the directory containing PDF files\r\n",
                "folder_path = r'C:\\<yourrootfolder>\\INFORMATION-TECHNOLOGY'\r\n",
                "\r\n",
                "# Count the number of PDF files in the directory\r\n",
                "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\r\n",
                "num_files = len(pdf_files)\r\n",
                "print(f\"Number of PDF files in the directory: {num_files}\")\r\n",
                "\r\n",
                "# Create a DataFrame to store the chunks\r\n",
                "data = []\r\n",
                "\r\n",
                "for file_id, pdf_file in enumerate(pdf_files):\r\n",
                "    print(f\"Processing file {file_id + 1}/{num_files}: {pdf_file}\")\r\n",
                "    pdf_path = os.path.join(folder_path, pdf_file)\r\n",
                "    text = extract_text_from_pdf(pdf_path)\r\n",
                "    cleaned_text = clean_text(text)\r\n",
                "    chunks = split_text_into_token_chunks(cleaned_text)\r\n",
                "    \r\n",
                "    print(f\"Number of chunks for file {pdf_file}: {len(chunks)}\")\r\n",
                "    \r\n",
                "    for chunk_id, chunk in enumerate(chunks):\r\n",
                "        chunk_text = chunk.strip() if chunk.strip() else \"NULL\"\r\n",
                "        unique_chunk_id = f\"{file_id}_{chunk_id}\"\r\n",
                "        print(f\"File: {pdf_file}, Chunk ID: {chunk_id}, Unique Chunk ID: {unique_chunk_id}, Chunk Length: {len(chunk_text)}, Chunk Text: {chunk_text[:50]}...\")  # Print first 50 characters of chunk text\r\n",
                "        data.append({\r\n",
                "            \"file_name\": pdf_file,\r\n",
                "            \"chunk_id\": chunk_id,\r\n",
                "            \"chunk_text\": chunk_text,\r\n",
                "            \"unique_chunk_id\": unique_chunk_id\r\n",
                "        })\r\n",
                "\r\n",
                "df = pd.DataFrame(data)\r\n",
                "df.head(3)\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "0355f92c-0546-4eac-aea8-b55d8bbef194"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 79541391.pdf: 5\nFile: 79541391.pdf, Chunk ID: 0, Unique Chunk ID: 111_0, Chunk Length: 2868, Chunk Text: SUBJECT MATTER EXPERT INFORMATION TECHNOLOGY ASSIS...\nFile: 79541391.pdf, Chunk ID: 1, Unique Chunk ID: 111_1, Chunk Length: 2806, Chunk Text: va for Fixed Assets Equipment and Real Estate Item...\nFile: 79541391.pdf, Chunk ID: 2, Unique Chunk ID: 111_2, Chunk Length: 2597, Chunk Text: Supervisor post on P3 level was filled  Managed co...\nFile: 79541391.pdf, Chunk ID: 3, Unique Chunk ID: 111_3, Chunk Length: 2710, Chunk Text: Organized Wood furniture into appropriated stockro...\nFile: 79541391.pdf, Chunk ID: 4, Unique Chunk ID: 111_4, Chunk Length: 1451, Chunk Text: accurate and faster tracking of supplies  Increase...\nProcessing file 113/120: 81761658.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 81761658.pdf: 3\nFile: 81761658.pdf, Chunk ID: 0, Unique Chunk ID: 112_0, Chunk Length: 3052, Chunk Text: IT MANAGER Highlights  Customer and Client Relatio...\nFile: 81761658.pdf, Chunk ID: 1, Unique Chunk ID: 112_1, Chunk Length: 3122, Chunk Text: personnel  Communicate with PresidentCEO on all te...\nFile: 81761658.pdf, Chunk ID: 2, Unique Chunk ID: 112_2, Chunk Length: 350, Chunk Text: 9 2000 NT migration Network Networking PACS person...\nProcessing file 114/120: 83816738.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 83816738.pdf: 4\nFile: 83816738.pdf, Chunk ID: 0, Unique Chunk ID: 113_0, Chunk Length: 2989, Chunk Text: INFORMATION TECHNOLOGY INTERN TEST AUTOMATION ENGI...\nFile: 83816738.pdf, Chunk ID: 1, Unique Chunk ID: 113_1, Chunk Length: 2869, Chunk Text: aviordriven testing framework to allow Quality Ass...\nFile: 83816738.pdf, Chunk ID: 2, Unique Chunk ID: 113_2, Chunk Length: 2757, Chunk Text: and versioning sophisticated IDEs such as IntelliJ...\nFile: 83816738.pdf, Chunk ID: 3, Unique Chunk ID: 113_3, Chunk Length: 263, Chunk Text: in safety and security during Tech Exhibition 2013...\nProcessing file 115/120: 89413122.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 89413122.pdf: 2\nFile: 89413122.pdf, Chunk ID: 0, Unique Chunk ID: 114_0, Chunk Length: 3326, Chunk Text: OPERATIONS RESEARCH ANALYST Summary Personable pro...\nFile: 89413122.pdf, Chunk ID: 1, Unique Chunk ID: 114_1, Chunk Length: 1321, Chunk Text: radiation detection data in an operational setting...\nProcessing file 116/120: 90867631.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 90867631.pdf: 5\nFile: 90867631.pdf, Chunk ID: 0, Unique Chunk ID: 115_0, Chunk Length: 2439, Chunk Text: INFORMATION TECHNOLOGY SPECIALIST Career Overview ...\nFile: 90867631.pdf, Chunk ID: 1, Unique Chunk ID: 115_1, Chunk Length: 2853, Chunk Text: troubleshooting software and hardware issues of mi...\nFile: 90867631.pdf, Chunk ID: 2, Unique Chunk ID: 115_2, Chunk Length: 3003, Chunk Text: Developed and modifies databases  Performed databa...\nFile: 90867631.pdf, Chunk ID: 3, Unique Chunk ID: 115_3, Chunk Length: 2588, Chunk Text: Credits Earned 36 Semester hours Masters of Scienc...\nFile: 90867631.pdf, Chunk ID: 4, Unique Chunk ID: 115_4, Chunk Length: 1148, Chunk Text: XML fax machines features functional Help desk HTM...\nProcessing file 117/120: 91121135.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 91121135.pdf: 3\nFile: 91121135.pdf, Chunk ID: 0, Unique Chunk ID: 116_0, Chunk Length: 3001, Chunk Text: ADMINISTRATIVE ASSISTANT DIRECTOR HUMAN RESOURCES ...\nFile: 91121135.pdf, Chunk ID: 1, Unique Chunk ID: 116_1, Chunk Length: 3023, Chunk Text: site organization chart  Processed invoices  Gener...\nFile: 91121135.pdf, Chunk ID: 2, Unique Chunk ID: 116_2, Chunk Length: 487, Chunk Text: Senior Management filing Forms Human Resources ins...\nProcessing file 118/120: 91635250.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 91635250.pdf: 3\nFile: 91635250.pdf, Chunk ID: 0, Unique Chunk ID: 117_0, Chunk Length: 3355, Chunk Text: Christopher Townes Summary Knowledgeable Informati...\nFile: 91635250.pdf, Chunk ID: 1, Unique Chunk ID: 117_1, Chunk Length: 3095, Chunk Text: functional features with minimal defects  Authored...\nFile: 91635250.pdf, Chunk ID: 2, Unique Chunk ID: 117_2, Chunk Length: 871, Chunk Text: inmates provided receipts and inspected items for ...\nProcessing file 119/120: 91697974.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 91697974.pdf: 2\nFile: 91697974.pdf, Chunk ID: 0, Unique Chunk ID: 118_0, Chunk Length: 3198, Chunk Text: INFORMATION TECHNOLOGY COORDINATOR Professional Su...\nFile: 91697974.pdf, Chunk ID: 1, Unique Chunk ID: 118_1, Chunk Length: 2360, Chunk Text: administrative personnel related activities monito...\nProcessing file 120/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 119_0, Chunk Length: 3232, Chunk Text: DIRECTOR OF INFORMATION TECHNOLOGY Executive Profi...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 119_1, Chunk Length: 2101, Chunk Text: needs for the City of Greensboro  Installed and co...\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "      file_name  chunk_id                                         chunk_text  \\\n0  10089434.pdf         0  INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...   \n1  10089434.pdf         1  Disaster Recovery plan and procedures  Researc...   \n2  10089434.pdf         2  Installing configuring and supporting McAfee a...   \n\n  unique_chunk_id  \n0             0_0  \n1             0_1  \n2             0_2  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>chunk_id</th>\n      <th>chunk_text</th>\n      <th>unique_chunk_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10089434.pdf</td>\n      <td>0</td>\n      <td>INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...</td>\n      <td>0_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10089434.pdf</td>\n      <td>1</td>\n      <td>Disaster Recovery plan and procedures  Researc...</td>\n      <td>0_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10089434.pdf</td>\n      <td>2</td>\n      <td>Installing configuring and supporting McAfee a...</td>\n      <td>0_2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 9,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Tokenization vs. Character Length (OPTIONAL)**\n",
                "\n",
                "In this section, we will explore the difference between the character length of a text chunk and its tokenized representation. Character length simply counts the number of characters in a text, while tokenization breaks the text into meaningful units called tokens.\n",
                "\n",
                "Character Length First, let’s add a new column to our DataFrame to view the length of each chunk in terms of characters: Here, chunk\\_length represents the number of characters in each chunk."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "655a8389-1018-4e80-a39d-85187cf3c46f"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Add a new column 'chunk_length' to the DataFrame to view the length of each chunk\r\n",
                "df['chunk_length'] = df['chunk_text'].apply(len)\r\n",
                "\r\n",
                "# Display the first few rows of the DataFrame with the new column\r\n",
                "print(df[['file_name', 'chunk_id', 'chunk_length']].head(5))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "5e171928-764a-4d61-899a-83a8e0c3ac79",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "      file_name  chunk_id  chunk_length\n0  10089434.pdf         0          3035\n1  10089434.pdf         1          2959\n2  10089434.pdf         2          2048\n3  10247517.pdf         0          3191\n4  10247517.pdf         1          2744\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Tokenization\r\n",
                "To understand how text ultimately is tokenized, it can be helpful to run the below code: \r\n",
                "\r\n",
                "- We use the tiktoken library to tokenize the text. Tokenization breaks the text into smaller units, which can be words, subwords, or characters, depending on the tokenizer used. You can see that in some cases an entire word is represented with a single token whereas in others parts of words are split across multiple tokens. \r\n",
                "\r\n",
                "- If you then check the length of the decode variable, you'll find it matches 500 our specified token number. It is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192\r\n",
                "\r\n",
                "- When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples below and then convert the tokens to a series of floating point numbers that will be accessible via vector search"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "9b87f84b-149c-4eb5-b9f0-c1b55f6d606c"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import tiktoken\r\n",
                "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\r\n",
                "sample_encode = tokenizer.encode(df.chunk_text[0]) \r\n",
                "decode = tokenizer.decode_tokens_bytes(sample_encode)\r\n",
                "decode\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "28188280-2bef-414a-a663-a017c944bc19"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[b'IN',\n b'FORMATION',\n b' TECHNO',\n b'LOGY',\n b' TECH',\n b'NIC',\n b'IAN',\n b' I',\n b' Summary',\n b' Vers',\n b'atile',\n b' Systems',\n b' Administrator',\n b' possessing',\n b' superior',\n b' troubleshooting',\n b' skills',\n b' for',\n b' networking',\n b' issues',\n b' end',\n b' user',\n b' problems',\n b' and',\n b' network',\n b' security',\n b' Experienced',\n b' in',\n b' server',\n b' management',\n b' systems',\n b' analysis',\n b' and',\n b' offering',\n b' inde',\n b'pth',\n b' understanding',\n b' of',\n b' IT',\n b' infrastructure',\n b' areas',\n b' Detail',\n b'oriented',\n b' independent',\n b' and',\n b' focused',\n b' on',\n b' taking',\n b' a',\n b' systematic',\n b' approach',\n b' to',\n b' solving',\n b' complex',\n b' problems',\n b' Demonstr',\n b'ated',\n b' exceptional',\n b' technical',\n b' knowledge',\n b' and',\n b' skills',\n b' while',\n b' working',\n b' with',\n b' various',\n b' teams',\n b' to',\n b' achieve',\n b' shared',\n b' goals',\n b' and',\n b' objectives',\n b' Highlights',\n b' ',\n b' Active',\n b' Directory',\n b' ',\n b' New',\n b' technology',\n b' and',\n b' product',\n b' research',\n b' ',\n b' Group',\n b' Policy',\n b' Objects',\n b' ',\n b' Office',\n b' ',\n b'365',\n b' and',\n b' Azure',\n b' ',\n b' PowerShell',\n b' and',\n b' VB',\n b'Script',\n b' ',\n b' Storage',\n b' management',\n b' ',\n b' Microsoft',\n b' Exchange',\n b' ',\n b' Enterprise',\n b' backup',\n b' management',\n b' ',\n b' VM',\n b'Ware',\n b' experience',\n b' ',\n b' Disaster',\n b' recovery',\n b' Experience',\n b' Information',\n b' Technology',\n b' Technician',\n b' I',\n b' Aug',\n b' ',\n b'200',\n b'7',\n b' to',\n b' Current',\n b' Company',\n b' Name',\n b' i',\n b'14',\n b' City',\n b' State',\n b' ',\n b' M',\n b'igr',\n b'ating',\n b' and',\n b' managing',\n b' user',\n b' accounts',\n b' in',\n b' Microsoft',\n b' Office',\n b' ',\n b'365',\n b' and',\n b' Exchange',\n b' Online',\n b' ',\n b' Creating',\n b' and',\n b' managing',\n b' virtual',\n b' machines',\n b' for',\n b' systems',\n b' such',\n b' as',\n b' domain',\n b' controllers',\n b' and',\n b' Active',\n b' Directory',\n b' Federation',\n b' Services',\n b' A',\n b'DFS',\n b' in',\n b' Microsoft',\n b' Windows',\n b' Azure',\n b' I',\n b'aaS',\n b' ',\n b' Creating',\n b' and',\n b' managing',\n b' storage',\n b' in',\n b' Microsoft',\n b' Windows',\n b' Azure',\n b' I',\n b'aaS',\n b' ',\n b' Installing',\n b' and',\n b' configuring',\n b' St',\n b'or',\n b'Simple',\n b' i',\n b'SC',\n b'SI',\n b' cloud',\n b' array',\n b' ST',\n b'aa',\n b'SB',\n b'aaS',\n b' ',\n b' Installing',\n b' configuring',\n b' and',\n b' testing',\n b' Twin',\n b'str',\n b'ata',\n b' i',\n b'SC',\n b'SI',\n b' cloud',\n b' array',\n b' ST',\n b'aa',\n b'SB',\n b'aaS',\n b' ',\n b' Collabor',\n b'ating',\n b' on',\n b' project',\n b' plan',\n b' for',\n b' Office',\n b' ',\n b'365',\n b' migration',\n b' ',\n b' Developing',\n b' detailed',\n b' specifications',\n b' for',\n b' the',\n b' Office',\n b' ',\n b'365',\n b' migration',\n b' including',\n b' business',\n b'case',\n b' documentation',\n b' cost',\n b' benefit',\n b' analyses',\n b' technical',\n b' diagrams',\n b' and',\n b' work',\n b' flow',\n b' documentation',\n b' ',\n b' Received',\n b' training',\n b' in',\n b' MVC',\n b' ',\n b'4',\n b' for',\n b' Visual',\n b' Studio',\n b' using',\n b' ',\n b' Net',\n b' Framework',\n b' ',\n b'445',\n b' to',\n b' develop',\n b' application',\n b' using',\n b' HTML',\n b'5',\n b' and',\n b' CSS',\n b'3',\n b' ',\n b' Installing',\n b' configuring',\n b' and',\n b' supporting',\n b' Linux',\n b' machines',\n b' for',\n b' the',\n b' open',\n b' WiFi',\n b' network',\n b' project',\n b' ',\n b' Comp',\n b'iling',\n b' and',\n b' generating',\n b' statistical',\n b' information',\n b' concerning',\n b' wireless',\n b' network',\n b' traffic',\n b' using',\n b' C',\n b'act',\n b'i',\n b' ',\n b' Config',\n b'uring',\n b' wireless',\n b' LAN',\n b' router',\n b' networking',\n b' and',\n b' security',\n b' access',\n b' ',\n b' Installing',\n b' and',\n b' configuring',\n b' wireless',\n b' certificates',\n b' ',\n b' Developing',\n b' detailed',\n b' specifications',\n b' for',\n b' the',\n b' acquisition',\n b' of',\n b' an',\n b' Enterprise',\n b' backup',\n b' system',\n b' including',\n b' systems',\n b' design',\n b' business',\n b'case',\n b' documentation',\n b' cost',\n b' benefit',\n b' analysis',\n b' technical',\n b' diagrams',\n b' and',\n b' work',\n b' flow',\n b' documentation',\n b' ',\n b' Review',\n b'ing',\n b' evaluating',\n b' and',\n b' analyzing',\n b' department',\n b'al',\n b' policies',\n b' guidelines',\n b' procedures',\n b' and',\n b' standards',\n b' with',\n b' management',\n b' and',\n b' staff',\n b' ',\n b' Developing',\n b' test',\n b' scripts',\n b' for',\n b' acceptance',\n b' unit',\n b' and',\n b' system',\n b' testing',\n b' of',\n b' Hyper',\n b'ion',\n b' Phase',\n b' ',\n b'1',\n b' and',\n b' Miami',\n b'Biz',\n b' Phase',\n b' ',\n b'2',\n b' ',\n b' Developing',\n b' Quality',\n b' Assurance',\n b' and',\n b' testing',\n b' plan',\n b' for',\n b' Hyper',\n b'ion',\n b' Phase',\n b' ',\n b'1',\n b' and',\n b' Miami',\n b'Biz',\n b' Phase',\n b' ',\n b'2',\n b' ',\n b' Debug',\n b'ging',\n b' and',\n b' logging',\n b' of',\n b' errors',\n b' in',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' using',\n b' Team',\n b' Foundation',\n b' Server',\n b' T',\n b'FS',\n b' ',\n b' Particip',\n b'ated',\n b' in',\n b' various',\n b' phases',\n b' of',\n b' the',\n b' project',\n b' life',\n b' cycle',\n b' such',\n b' as',\n b' determining',\n b' requirements',\n b' design',\n b' conceptual',\n b'ization',\n b' testing',\n b' implementation',\n b' deployment',\n b' and',\n b' release',\n b' for',\n b' the',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' projects',\n b' ',\n b' Collabor',\n b'ating',\n b' on',\n b' project',\n b' plans',\n b' for',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' ',\n b' Pre',\n b'paring',\n b' presentations',\n b' and',\n b' documentation',\n b' to',\n b' demonstrate',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' functionality',\n b' or',\n b' design',\n b' ',\n b' Monitoring',\n b' network',\n b' traffic',\n b' and',\n b' compiling',\n b' and',\n b' generating',\n b' statistical',\n b' information',\n b' using',\n b' Solar',\n b' Winds',\n b' ',\n b' Collabor',\n b'ating',\n b' on']"
                    },
                    "metadata": {},
                    "execution_count": 85,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 85
        },
        {
            "cell_type": "code",
            "source": [
                "len(decode)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "31d80228-e140-413a-b1e3-c320a42b1f6c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "500"
                    },
                    "metadata": {},
                    "execution_count": 86,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 86
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 2 : Generating Embeddings for Text Chunks using Azure Open AI**\n",
                "\n",
                "- After extracting and chunking the text from PDF resumes, we will generate embeddings for each chunk. These embeddings are numerical representations of the text that capture its semantic meaning. By creating embeddings for the text chunks, we can perform advanced similarity searches and enhance language model generation.\n",
                "\n",
                "- We will use the Azure OpenAI API to generate these embeddings. The `get_embedding` function defined below takes a piece of text as input and returns its embedding using the `text-embedding-small` model\n",
                "\n",
                "- Ensure the Environment Variables are set correctly in the .env file"
            ],
            "metadata": {
                "azdata_cell_guid": "ac19ee49-763a-4e2e-8c31-9ce1ba77bbe4"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "import json\r\n",
                "import requests\r\n",
                "import sys\r\n",
                "from num2words import num2words\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import tiktoken\r\n",
                "from openai import AzureOpenAI\r\n",
                "\r\n",
                "# Specify your model name\r\n",
                "openai_embedding_model = os.getenv(\"AZOPENAI_EMBEDDING_MODEL_DEPLOYMENT_NAME\")\r\n",
                "\r\n",
                "# Assuming openai_url and openai_key are your environment variables\r\n",
                "openai_url = os.getenv(\"AZOPENAI_ENDPOINT\") + \"openai/deployments/\" + openai_embedding_model + \"/embeddings?api-version=2023-05-15\"\r\n",
                "openai_key = os.getenv(\"AZOPENAI_API_KEY\")\r\n",
                "\r\n",
                "def get_embedding(text):\r\n",
                "    \"\"\"\r\n",
                "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\r\n",
                "\r\n",
                "    Args:\r\n",
                "        text (str): Text to embed.\r\n",
                "\r\n",
                "    Returns:\r\n",
                "        list: A list containing the embedding.\r\n",
                "    \"\"\"\r\n",
                "    response = requests.post(openai_url,\r\n",
                "        headers={\"api-key\": openai_key, \"Content-Type\": \"application/json\"},\r\n",
                "        json={\"input\": [text]}  # Embed the extracted chunk\r\n",
                "    )\r\n",
                "    \r\n",
                "    if response.status_code == 200:\r\n",
                "        response_json = response.json()\r\n",
                "        embedding = response_json['data'][0]['embedding']\r\n",
                "        return embedding\r\n",
                "    else:\r\n",
                "        return None\r\n",
                "\r\n",
                "# Example usage\r\n",
                "all_filenames = []\r\n",
                "all_chunkids = []\r\n",
                "all_chunks = []\r\n",
                "all_embeddings = []\r\n",
                "\r\n",
                "# Assuming df is already defined with the required columns\r\n",
                "for index, row in df.iterrows():\r\n",
                "    filename = row['file_name']\r\n",
                "    chunkid = row['unique_chunk_id']\r\n",
                "    chunk = row['chunk_text']\r\n",
                "    embedding = get_embedding(chunk)\r\n",
                "    \r\n",
                "    if embedding is not None:\r\n",
                "        all_filenames.append(filename)\r\n",
                "        all_chunkids.append(chunkid)\r\n",
                "        all_chunks.append(chunk)\r\n",
                "        all_embeddings.append(embedding)\r\n",
                "    \r\n",
                "    if (index + 1) % 200 == 0:  # Print progress every 200 rows\r\n",
                "        print(f\"Completed {index + 1} rows\")\r\n",
                "\r\n",
                "# Create a new DataFrame with the results\r\n",
                "result_df = pd.DataFrame({\r\n",
                "    'filename': all_filenames,\r\n",
                "    'chunkid': all_chunkids,\r\n",
                "    'chunk': all_chunks,\r\n",
                "    'embedding': all_embeddings\r\n",
                "})\r\n",
                "\r\n",
                "print(result_df.head(5))  # Display the first few rows of the dataframe\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "3aeda057-d0c0-40cd-bdcf-723abfed94e2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Completed 200 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "       filename chunkid                                              chunk  \\\n0  10089434.pdf     0_0  INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...   \n1  10089434.pdf     0_1  Disaster Recovery plan and procedures  Researc...   \n2  10089434.pdf     0_2  Installing configuring and supporting McAfee a...   \n3  10247517.pdf     1_0  INFORMATION TECHNOLOGY MANAGER Professional Su...   \n4  10247517.pdf     1_1  network which entailed changing software and L...   \n\n                                           embedding  \n0  [-0.009804371, -0.0077886474, -0.0043056956, -...  \n1  [-0.0055441344, -0.0042689154, 0.00038358863, ...  \n2  [-0.005266213, -0.0009840217, -0.00897835, -0....  \n3  [-0.0156019265, -0.007396069, 0.0030721354, -0...  \n4  [-0.0134326555, -0.011126321, 0.023804175, -0....  \n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 3 : Using Azure SQL DB as a Vector Database to store and query embeddings**\n",
                "\n",
                "### **Load the embeddings into the Vector Database : Azure SQL DB**\n",
                "\n",
                "First let us define a function to connect to Azure SQLDB"
            ],
            "metadata": {
                "azdata_cell_guid": "04d42351-41ec-4ce9-9778-fe4ea2ecb8a2"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#lets define a function to connect to SQLDB\r\n",
                "import os\r\n",
                "from dotenv import load_dotenv\r\n",
                "import pyodbc\r\n",
                "import struct\r\n",
                "from azure.identity import DefaultAzureCredential\r\n",
                "\r\n",
                "# Load environment variables from .env file\r\n",
                "load_dotenv()\r\n",
                "\r\n",
                "def get_mssql_connection():\r\n",
                "    # Retrieve the connection string from the environment variables\r\n",
                "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\r\n",
                "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\r\n",
                "\r\n",
                "    # Determine the authentication method and connect to the database\r\n",
                "    if entra_connection_string:\r\n",
                "        # Entra ID Service Principal Authentication\r\n",
                "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \r\n",
                "        token = credential.get_token('https://database.windows.net/.default')\r\n",
                "        token_bytes = token.token.encode('UTF-16LE')\r\n",
                "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\r\n",
                "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\r\n",
                "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\r\n",
                "    elif sql_connection_string:\r\n",
                "        # SQL Authentication\r\n",
                "        conn = pyodbc.connect(sql_connection_string)\r\n",
                "    else:\r\n",
                "        raise ValueError(\"No valid connection string found in the environment variables.\")\r\n",
                "\r\n",
                "    return conn"
            ],
            "metadata": {
                "azdata_cell_guid": "930a63bc-4c08-4205-b152-b1ad5c82057a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 75
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Insert embeddings into the native 'Vector' Data Type**\n",
                "\n",
                "We will insert our vectors into the SQL Table now. Azure SQL DB now has a dedicated, native, data type for storing vectors: the `vector` data type. Read about the preview [here](https://devblogs.microsoft.com/azure-sql/eap-for-vector-support-refresh-introducing-vector-type)\n",
                "\n",
                "The table embeddings has a column called vector which is vector(1536) type. Ensure you have created the table using the script `CreateTable.sql` before running the below code."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "e929c112-65d4-46f1-a9a2-7c9434b2d7cb"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pyodbc\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "# Retrieve the connection string from the function get_mssql_connection()\r\n",
                "conn = get_mssql_connection()\r\n",
                "\r\n",
                "# Create a cursor object\r\n",
                "cursor = conn.cursor()\r\n",
                "\r\n",
                "# Enable fast_executemany\r\n",
                "cursor.fast_executemany = True\r\n",
                "\r\n",
                "# Loop through the DataFrame rows and insert them into the table\r\n",
                "for index, row in result_df.iterrows():\r\n",
                "    chunkid = row['chunkid']\r\n",
                "    filename = row['filename']\r\n",
                "    chunk = row['chunk']\r\n",
                "    embedding = row['embedding']\r\n",
                "    \r\n",
                "    # Convert the embedding list to a string format suitable for SQL\r\n",
                "    embedding_str = str(embedding).replace('[', 'N\\'[').replace(']', ']\\'')\r\n",
                "    \r\n",
                "    # Use placeholders for the parameters in the SQL query\r\n",
                "    query = f\"\"\"\r\n",
                "    INSERT INTO resumedocs (chunkid, filename, chunk, embedding)\r\n",
                "    VALUES (?, ?, ?, CAST({embedding_str} AS VECTOR(1536)))\r\n",
                "    \"\"\"\r\n",
                "    # Execute the query with the parameters\r\n",
                "    cursor.execute(query, chunkid, filename, chunk)\r\n",
                "\r\n",
                "# Commit the changes\r\n",
                "conn.commit()\r\n",
                "\r\n",
                "# Print a success message\r\n",
                "print(\"Data inserted successfully into the 'resumedocs' table.\")\r\n",
                "\r\n",
                "# Close the connection\r\n",
                "conn.close()\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "680259d9-77ce-4b63-b412-9bea33bb0f43"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Data inserted successfully into the 'resumedocs' table.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 20
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take a look at the data in the Resume Docs table:"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "ede638b9-d681-4cb9-9ab8-9651e3099a36"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from prettytable import PrettyTable\r\n",
                "\r\n",
                "import pyodbc\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "# Load environment variables from .env file\r\n",
                "load_dotenv()\r\n",
                "\r\n",
                "# Retrieve the connection string from the environment variables\r\n",
                "conn = get_mssql_connection()\r\n",
                "\r\n",
                "# Create a cursor object\r\n",
                "cursor = conn.cursor()\r\n",
                "\r\n",
                "# Use placeholders for the parameters in the SQL query\r\n",
                "query = \"SELECT TOP(10) filename, chunkid, chunk , embedding FROM dbo.resumedocs ORDER BY Id\"\r\n",
                "\r\n",
                "# Execute the query with the parameters\r\n",
                "cursor.execute(query)\r\n",
                "queryresults = cursor.fetchall()\r\n",
                "\r\n",
                "# Get column names from cursor.description\r\n",
                "column_names = [column[0] for column in cursor.description]\r\n",
                "\r\n",
                "# Create a PrettyTable object\r\n",
                "table = PrettyTable()\r\n",
                "\r\n",
                "# Add column names to the table\r\n",
                "table.field_names = column_names\r\n",
                "\r\n",
                "# Set max width for each column to truncate data\r\n",
                "table.max_width = 20\r\n",
                "\r\n",
                "# Add rows to the table\r\n",
                "for row in queryresults:\r\n",
                "    # Truncate each value to 20 characters\r\n",
                "    truncated_row = [str(value)[:20] for value in row]\r\n",
                "    table.add_row(truncated_row)\r\n",
                "\r\n",
                "# Print the table\r\n",
                "print(table)\r\n",
                "\r\n",
                "# Commit the changes\r\n",
                "conn.commit()\r\n",
                "# Close the connection\r\n",
                "conn.close()\r\n",
                ""
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "88d1a90e-e93c-426e-934d-fb1a47dfd900"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+--------------+---------+----------------------+----------------------+\n|   filename   | chunkid |        chunk         |      embedding       |\n+--------------+---------+----------------------+----------------------+\n| 10089434.pdf |   0_0   | INFORMATION TECHNOLO | b'\\xa9\\x01\\x00\\x06\\x |\n| 10089434.pdf |   0_1   | Disaster Recovery pl | b'\\xa9\\x01\\x00\\x06\\x |\n| 10089434.pdf |   0_2   | Installing configuri | b'\\xa9\\x01\\x00\\x06\\x |\n| 10247517.pdf |   1_0   | INFORMATION TECHNOLO | b'\\xa9\\x01\\x00\\x06\\x |\n| 10247517.pdf |   1_1   | network which entail | b'\\xa9\\x01\\x00\\x06\\x |\n| 10247517.pdf |   1_2   | i4 City State 2015 M | b'\\xa9\\x01\\x00\\x06\\x |\n| 10265057.pdf |   2_0   | WORKING RF SYSTEMS E | b'\\xa9\\x01\\x00\\x06\\x |\n| 10265057.pdf |   2_1   | surveys ElectricalVa | b'\\xa9\\x01\\x00\\x06\\x |\n| 10553553.pdf |   3_0   | INFORMATION TECHNOLO | b'\\xa9\\x01\\x00\\x06\\x |\n| 10553553.pdf |   3_1   | XP Vista and Mac ope | b'\\xa9\\x01\\x00\\x06\\x |\n+--------------+---------+----------------------+----------------------+\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 77
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Performing Vector Similarity Search in Azure SQL DB using VECTOR\\_DISTANCE built in function**\n",
                "\n",
                "Let's now query our ResumeDocs table to get the top similar candidates given the User search query.\n",
                "\n",
                "What we are doing: Given any user search query, we can obtain the vector representation of that text. We then use this vector to calculate the cosine distance against all the resume embeddings stored in the database. By selecting only the closest matches, we can identify the resumes most relevant to the user’s query. This helps in finding the most suitable candidates based on their resumes.\n",
                "\n",
                "The most common distance is the cosine similarity, which can be calculated quite easily in SQL with the help of the new distance functions.\n",
                "\n",
                "```\n",
                "VECTOR_DISTANCE('distance metric', V1, V2)\n",
                "\n",
                "```\n",
                "\n",
                "We can use **cosine**, **euclidean**, and **dot** as the distance metric today.\n",
                "\n",
                "We will define the function `vector_search_sql`."
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "b7b0fda6-3322-4bbc-8d08-d0b567da79ec"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "import pyodbc\r\n",
                "from dotenv import load_dotenv\r\n",
                "\r\n",
                "def vector_search_sql(query, num_results=5):\r\n",
                "    # Load environment variables from .env file\r\n",
                "    load_dotenv()\r\n",
                "\r\n",
                "    # Use the get_mssql_connection function to get the connection string details\r\n",
                "    conn = get_mssql_connection()\r\n",
                "\r\n",
                "    # Create a cursor object\r\n",
                "    cursor = conn.cursor()\r\n",
                "\r\n",
                "    # Generate the query embedding for the user's search query\r\n",
                "    user_query_embedding = get_embedding(query)\r\n",
                "\r\n",
                "    # Convert the embedding list to a string format suitable for SQL\r\n",
                "    embedding_str = str(user_query_embedding).replace('[', 'N\\'[').replace(']', ']\\'')\r\n",
                "    \r\n",
                "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\r\n",
                "    sql_similarity_search = f\"\"\"\r\n",
                "    SELECT TOP(?) filename, chunkid, chunk,\r\n",
                "           1 - vector_distance('cosine', CAST({embedding_str} AS VECTOR(1536)), embedding) AS similarity_score\r\n",
                "    FROM dbo.resumedocs\r\n",
                "    ORDER BY similarity_score DESC\r\n",
                "    \"\"\"\r\n",
                "\r\n",
                "    cursor.execute(sql_similarity_search, num_results)\r\n",
                "    results = cursor.fetchall()\r\n",
                "\r\n",
                "    # Close the database connection\r\n",
                "    conn.close()\r\n",
                "\r\n",
                "    return results"
            ],
            "metadata": {
                "azdata_cell_guid": "1b4f0ca2-2401-4f90-a44d-75dce03500cc",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 53
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **Part 4 : Use embeddings retrieved from a Azure SQL vector database to augment LLM generation**\r\n",
                "\r\n",
                "Lets create a helper function to feed prompts into the [Completions model](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4) & create interactive loop where you can pose questions to the model and receive information grounded in your data.\r\n",
                "\r\n",
                "The function `generate_completion` is defined to help ground the gpt-4o model with prompts and system instructions.   \r\n",
                "Note that we are passing the results of the `vector_search_sql` we defined earlier to the model and we define the system prompt .  \r\n",
                "We are using gpt-4o model here. \r\n",
                "\r\n",
                "You can get more information on using Azure Open AI GPT chat models [here](https://learn.microsoft.com/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)"
            ],
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "1e194f3c-6a7a-4f16-95ec-05f60a3770a4"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\r\n",
                "from dotenv import load_dotenv\r\n",
                "from openai import AzureOpenAI\r\n",
                "\r\n",
                "# Load environment variables from a .env file\r\n",
                "load_dotenv()\r\n",
                "\r\n",
                "# Use environment variables for the API key and endpoint\r\n",
                "api_key = os.getenv(\"AZOPENAI_API_KEY\")\r\n",
                "azure_endpoint = os.getenv(\"AZOPENAI_ENDPOINT\")\r\n",
                "\r\n",
                "# Create a chat completion request\r\n",
                "client = AzureOpenAI(\r\n",
                "    api_key=api_key,\r\n",
                "    api_version=\"2023-05-15\",\r\n",
                "    azure_endpoint=azure_endpoint\r\n",
                ")\r\n",
                "\r\n",
                "def generate_completion(search_results, user_input):\r\n",
                "    system_prompt = '''\r\n",
                "You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\r\n",
                "- Use the information from `search_results` to generate your top 3 responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\r\n",
                "  File: {filename}\r\n",
                "  Chunk ID: {chunkid}\r\n",
                "  Similarity Score: {similarity_score}\r\n",
                "  Add a small snippet from the Relevant Text: {chunktext}\r\n",
                "  Do not use the entire chunk\r\n",
                "- Avoid any other external data sources.\r\n",
                "- Add a summary about why the candidate maybe a goodfit even if exact skills and the role being hired for are not matching , at the end of the recommendations. Ensure you call out which skills match the description and which ones are missing. If the candidate doesnt have prior experience for the hiring role which we may need to pay extra attention to during the interview process.\r\n",
                "- Add a Microsoft related interesting fact about the technology that was searched \r\n",
                "'''\r\n",
                "\r\n",
                "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\r\n",
                "    \r\n",
                "    # Create an empty list to store the results\r\n",
                "    result_list = []\r\n",
                "\r\n",
                "    # Iterate through the search results and append relevant information to the list\r\n",
                "    for result in search_results:\r\n",
                "        filename = result  # Assuming filename is the first column\r\n",
                "        chunkid = result\r\n",
                "        chunktext = result\r\n",
                "        similarity_score = result  # Assuming similarity_score is the third column\r\n",
                "        \r\n",
                "        # Append the relevant information as a dictionary to the result_list\r\n",
                "        result_list.append({\r\n",
                "            \"filename\": filename,\r\n",
                "            \"chunkid\": chunkid,\r\n",
                "            \"chunktext\": chunktext,\r\n",
                "            \"similarity_score\": similarity_score\r\n",
                "        })\r\n",
                "\r\n",
                "    # Print the result list\r\n",
                "    #print(result_list)\r\n",
                "    \r\n",
                "    messages.append({\"role\": \"system\", \"content\": f\"{result_list}\"})\r\n",
                "    messages.append({\"role\": \"user\", \"content\": user_input})\r\n",
                "    response = client.chat.completions.create(model='chatcompletion', messages=messages, temperature=0) #replace with your model deployment name\r\n",
                "\r\n",
                "    return response.dict()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2865a0d0-2ee3-4d0a-8ee8-89075ab541bc",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 64
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a loop of user input and model output to perform Q&A on the PDF's that are now chunked and stored in the SQL DB with embeddings\r\n",
                "\r\n",
                "print(\"*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\\n\")\r\n",
                "\r\n",
                "while True:\r\n",
                "    user_input = input(\"User prompt: \")\r\n",
                "    if user_input.lower() == \"end\":\r\n",
                "        break\r\n",
                "\r\n",
                "    # Print the user's question\r\n",
                "    print(f\"\\nUser asked: {user_input}\")\r\n",
                "\r\n",
                "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\r\n",
                "    search_results = vector_search_sql(user_input)\r\n",
                "    completions_results = generate_completion(search_results, user_input)\r\n",
                "\r\n",
                "    # Print the model's response\r\n",
                "    print(\"\\nAI's response:\")\r\n",
                "    print(completions_results['choices'][0]['message']['content'])\r\n",
                "\r\n",
                "# The loop will continue until the user types 'end'\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "d69ab285-0cc9-4596-95f2-688d0c324f6b",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\n\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nUser asked: Help me find a candidate for a Product Management Role in Database Cloud Migration. While this is a role for Microsoft SQL Server team we are looking for experience in compete like Oracle , PostgreSQL , MySQL , Amazon RDS\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nAI's response:\nHere are the top 3 candidates based on the provided search results:\n\n### Candidate 1:\n**File:** 29051656.pdf  \n**Chunk ID:** 81_0  \n**Similarity Score:** 0.8125  \n**Relevant Text Snippet:** \n\"An organized DBA professional with over 6 years hands-on experience supporting Oracle databases, SQL Server databases, and AWS infrastructure... Migrated databases from on-premise to AWS using Database migration services... Launched and maintained RDS and EC2 instances in AWS... Planned and implemented high availability solutions such as Real Application Cluster (RAC) in Oracle 11gR2 Grid and 10g on ASM.\"\n\n**Summary:** This candidate has extensive experience in database administration and cloud migration, particularly with Oracle and AWS. They have hands-on experience with SQL Server, Oracle, and MySQL, making them a strong fit for a product management role focused on database cloud migration.\n\n### Candidate 2:\n**File:** 21283365.pdf  \n**Chunk ID:** 48_2  \n**Similarity Score:** 0.7968  \n**Relevant Text Snippet:** \n\"Design & Implementation network policies processes... Project Management... technical support Desktop Support VOIP Windows Server... written communication skills.\"\n\n**Summary:** This candidate has a background in project management and technical support, with experience in Windows Server and network policies. While they may not have direct experience in database cloud migration, their project management skills and technical background could be valuable in a product management role.\n\n### Candidate 3:\n**File:** 30223363.pdf  \n**Chunk ID:** 84_3  \n**Similarity Score:** 0.7961  \n**Relevant Text Snippet:** \n\"User Access Writing Functional C Data Warehouse Front End Front End Design... Microsoft Project Ms Project Ms SQL Server Ms SQL Server 2005 Ms Visio Mysql Oracle Perl Rational Rational Rose... Sql Server Sql Server 2005 Subject Matter Expert Technical Specifications Translated Uml Visio Warranty Windows Xp Xml Ghosting It Support Maintenance Architecture Database Design Information Architecture Php Software Requirements.\"\n\n**Summary:** This candidate has a diverse skill set, including experience with SQL Server, MySQL, Oracle, and project management tools like Microsoft Project. Their background in database design and information architecture makes them a strong candidate for a product management role in database cloud migration.\n\n### Summary of Fit:\n- **Candidate 1** is the strongest fit due to their direct experience with database cloud migration and hands-on experience with Oracle, SQL Server, and AWS.\n- **Candidate 2** has strong project management skills but lacks direct experience in database cloud migration.\n- **Candidate 3** has a broad technical background and experience with multiple database systems, making them a versatile candidate for the role.\n\n### Microsoft Related Interesting Fact:\nDid you know that Microsoft Azure offers a fully managed relational database service called Azure SQL Database, which provides built-in high availability, automated backups, and scaling capabilities? This makes it an excellent choice for enterprises looking to migrate their databases to the cloud.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": null
        }
    ]
}