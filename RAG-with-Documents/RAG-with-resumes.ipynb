{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Leveraging Azure SQL DB’s Native Vector Capabilities for Enhanced Resume Matching with Azure Document Intelligence and RAG\n",
                "\n",
                "In this tutorial, we will explore how to leverage Azure SQL DB’s new vector data type to store embeddings and perform similarity searches using built-in vector functions, enabling advanced resume matching to identify the most suitable candidates. \n",
                "\n",
                "By extracting and chunking content from PDF resumes using Azure Document Intelligence, generating embeddings with Azure OpenAI, and storing these embeddings in Azure SQL DB, we can perform sophisticated vector similarity searches and retrieval-augmented generation (RAG) to identify the most suitable candidates based on their resumes.\n",
                "\n",
                "### **Tutorial Overview**\n",
                "\n",
                "- This Python notebook will teach you to:\n",
                "    1. **Chunk PDF Resumes**: Use **`Azure Document Intelligence`** to extract and chunk content from PDF resumes.\n",
                "    2. **Create Embeddings**: Generate embeddings from the chunked content using the **`Azure OpenAI API`**.\n",
                "    3. **Vector Database Utilization**: Store embeddings in **`Azure SQL DB`** utilizing the **`new Vector Data Type`** and perform similarity searches using built-in vector functions to find the most suitable candidates.\n",
                "    4. **LLM Generation Augmentation**: Enhance language model generation with embeddings from a vector database. In this case, we use the embeddings to inform a GPT-4 chat model, enabling it to provide rich, context-aware answers about candidates based on their resumes\n",
                "\n",
                "## Dataset\n",
                "\n",
                "We use a sample dataset from [Kaggle](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset) containing PDF resumes for this tutorial. For the purpose of this tutorial we will use 120 resumes from the **Information-Technology** folder\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- **Azure Subscription**: [Create one for free](https://azure.microsoft.com/free/cognitive-services?azure-portal=true)\n",
                "- **Azure SQL Database**: [Set up your database for free](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql)\n",
                "- **Azure Document Intelligence** [Create a FreeAzure Doc Intelligence resource](https:/learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0)\n",
                "- **Azure Data Studio**: Download [here](https://azure.microsoft.com/products/data-studio) to manage your Azure SQL database and [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel)\n",
                "\n",
                "## Additional Requirements for Embedding Generation\n",
                "\n",
                "- **Azure OpenAI Access**: Apply for access in the desired Azure subscription at [https://aka.ms/oai/access](https://aka.ms/oai/access)\n",
                "- **Azure OpenAI Resource**: Deploy an embeddings model (e.g., `text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4.0` model for chat completion. Refer to the [resource deployment guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
                "- **Python**: Version 3.7.1 or later from Python.org. (Sample has been tested with Python 3.11)\n",
                "- **Python Libraries**: Install the required libraries openai, num2words, matplotlib, plotly, scipy, scikit-learn, pandas, tiktoken, and pyodbc.\n",
                "- **Jupyter Notebooks**: Use within [Azure Data Studio](https://learn.microsoft.com/en-us/azure-data-studio/notebooks/notebooks-guidance) or Visual Studio Code .\n",
                "\n",
                "Code snippets are adapted from the [Azure OpenAI Service embeddings Tutorial](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python)\n",
                "\n",
                "## Getting Started\n",
                "\n",
                "1. **Database Setup**: Execute SQL commands from the `createtable.sql` script to create the necessary table in your database.\n",
                "2. **Model Deployment**: Deploy an embeddings model (`text-embedding-small` or `text-embedding-ada-002`) and a `GPT-4` model for chat completion. Note the 2 models deployment names for later use.\n",
                "\n",
                "![Deployed OpenAI Models](../Assets/modeldeployment.png)\n",
                "\n",
                "3. **Connection String**: Find your Azure SQL DB connection string in the Azure portal under your database settings.\n",
                "4. **Configuration**: Populate the `.env` file with your SQL server connection details , Azure OpenAI key and endpoint, Azure Document Intelligence key and endpoint values.\n",
                "\n",
                "You can retrieve the Azure OpenAI _endpoint_ and _key_:\n",
                "\n",
                "![Azure OpenAI Endpoint and Key](../Assets/endpoint.png)\n",
                "\n",
                "You can [retrieve](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) the Document Intelligence _endpoint_ and _key_:\n",
                "\n",
                "![Azure Document Intelligence Endpoint and Key](../Assets/docintelendpoint.png)\n",
                "\n",
                "## Running the Notebook\n",
                "\n",
                "To [execute the notebook](https://learn.microsoft.com/azure-data-studio/notebooks/notebooks-python-kernel), connect to your Azure SQL database using Azure Data Studio, which can be downloaded [here](https://azure.microsoft.com/products/data-studio)"
            ],
            "metadata": {
                "azdata_cell_guid": "507219d1-d713-4c41-86d5-e938bf69627c",
                "language": "sql"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#Setup the python libraries required for this notebook\n",
                "#Please ensure that you navigate to the directory containing the `requirements.txt` file in your terminal\n",
                "%pip install -r requirements.txt"
            ],
            "metadata": {
                "azdata_cell_guid": "fe37c601-5918-4055-badc-6c0ba90c68ce",
                "language": "python"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#Load the env details\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()"
            ],
            "metadata": {
                "azdata_cell_guid": "4c29709e-1c3a-495d-83ec-05737e220847",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {},
                    "execution_count": 3,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 1: Extracting and Chunking Text from PDF Resumes using Azure Document Intelligence**\n",
                "\n",
                "Create an instance of the [DocumentAnalysisClient](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0#get-endpoint-url-and-keys) using the endpoint and API key. \n",
                "\n",
                "[Azure Document Intelligence](https://learn.microsoft.com/azure/ai-services/document-intelligence/?view=doc-intel-4.0.0_)(previously known as Form Recognizer) is a Azure cloud service that uses machine learning to analyze text and structured data from your documents. This client will be used to send requests to the [Azure Document Intelligence](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python) service and receive responses containing the extracted text from the PDF resumes."
            ],
            "metadata": {
                "azdata_cell_guid": "4b543f05-9036-4887-8737-09aa9f865ec2",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import re\n",
                "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
                "from azure.core.credentials import AzureKeyCredential\n",
                "\n",
                "# Load environment variables\n",
                "endpoint = os.getenv(\"AZUREDOCINTELLIGENCE_ENDPOINT\")\n",
                "api_key = os.getenv(\"AZUREDOCINTELLIGENCE_API_KEY\")\n",
                "\n",
                "# Create a DocumentAnalysisClient\n",
                "document_analysis_client = DocumentAnalysisClient(\n",
                "    endpoint=endpoint,\n",
                "    credential=AzureKeyCredential(api_key)\n",
                ")\n"
            ],
            "metadata": {
                "azdata_cell_guid": "b20cc66a-50ce-4486-b275-d4683f4ba545",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Analyze input documents using prebuilt model in Azure Document Intelligence**\n",
                "\n",
                "- DocumentAnalysisClient provides operations for analyzing input documents using prebuilt and custom models through the `begin_analyze_document` and `begin_analyze_document_from_url` APIs. In this tutorial we are using the [prebuilt-layout](https://learn.microsoft.com/python/api/overview/azure/ai-formrecognizer-readme?view=azure-python#using-prebuilt-models)\n",
                "    \n",
                "\n",
                "### **Split text into chunks of 500 tokens**\n",
                "\n",
                "- When faced with content that exceeds the embedding limit, we usually also chunk the content into smaller pieces and then embed those one at a time. Here we will use [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file) to chunk the extracted text into token sizes of 500, as we will later pass the extracted chunks to to the `text-embedding-small` model for [generating text embeddings](https://learn.microsoft.com/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python) as this has a model input token limit of 8192.\n",
                "\n",
                "**Note**: You need to provide the location of the folder where the PDF files reside in the below script."
            ],
            "metadata": {
                "azdata_cell_guid": "1ecc04b0-1a5f-4d48-819d-2cc06a070062",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import re\n",
                "import pandas as pd\n",
                "import tiktoken\n",
                "\n",
                "# Path to the directory containing PDF files\n",
                "folder_path = os.path.join(os.getcwd(),'C:\\\\vectortest\\\\resumedata\\\\data\\\\data\\\\INFORMATION-TECHNOLOGY')\n",
                "\n",
                "def get_pdf_files(folder_path):\n",
                "    for path, subdirs, files in os.walk(folder_path):\n",
                "        for name in files:\n",
                "            if (name.endswith(\".pdf\")):\n",
                "                yield os.path.join(path, name)\n",
                "\n",
                "# Function to read PDF files and extract text using Azure AI Document Intelligence\n",
                "def extract_text_from_pdf(pdf_path):\n",
                "    with open(pdf_path, \"rb\") as f:\n",
                "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", document=f)\n",
                "    result = poller.result()\n",
                "    text = \"\"\n",
                "    for page in result.pages:\n",
                "        for line in page.lines:\n",
                "            text += line.content + \" \"\n",
                "    return text\n",
                "\n",
                "# Function to clean text and remove special characters\n",
                "def clean_text(text):\n",
                "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
                "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
                "    return text\n",
                "\n",
                "# Function to split text into chunks of 500 tokens\n",
                "def split_text_into_token_chunks(text, max_tokens=500):\n",
                "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "    tokens = tokenizer.encode(text)\n",
                "    chunks = []\n",
                "    \n",
                "    for i in range(0, len(tokens), max_tokens):\n",
                "        chunk_tokens = tokens[i:i + max_tokens]\n",
                "        chunk_text = tokenizer.decode(chunk_tokens)\n",
                "        chunks.append(chunk_text)\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Count the number of PDF files in the directory\n",
                "pdf_files = [f for f in get_pdf_files(folder_path)]\n",
                "num_files = len(pdf_files)\n",
                "print(f\"Number of PDF files in the directory: {num_files}\")\n",
                "\n",
                "#Extract the file name\n",
                "for pdf_file in pdf_files:\n",
                "    file_name = os.path.basename(pdf_file)\n",
                "\n",
                "# Create a DataFrame to store the chunks\n",
                "data = []\n",
                "\n",
                "for file_id, pdf_file in enumerate(pdf_files):\n",
                "    print(f\"Processing file {file_id + 1}/{num_files}: {file_name}\")\n",
                "    pdf_path = os.path.join(folder_path, pdf_file)\n",
                "    text = extract_text_from_pdf(pdf_path)\n",
                "    cleaned_text = clean_text(text)\n",
                "    chunks = split_text_into_token_chunks(cleaned_text)\n",
                "    \n",
                "    print(f\"Number of chunks for file {file_name}: {len(chunks)}\")\n",
                "    \n",
                "    for chunk_id, chunk in enumerate(chunks):\n",
                "        chunk_text = chunk.strip() if chunk.strip() else \"NULL\"\n",
                "        unique_chunk_id = f\"{file_id}_{chunk_id}\"\n",
                "        print(f\"File: {file_name}, Chunk ID: {chunk_id}, Unique Chunk ID: {unique_chunk_id}, Chunk Length: {len(chunk_text)}, Chunk Text: {chunk_text[:50]}...\")  # Print first 50 characters of chunk text\n",
                "        data.append({\n",
                "            \"file_name\": file_name,\n",
                "            \"chunk_id\": chunk_id,\n",
                "            \"chunk_text\": chunk_text,\n",
                "            \"unique_chunk_id\": unique_chunk_id\n",
                "        })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "df.head(3)\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "0355f92c-0546-4eac-aea8-b55d8bbef194",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Number of PDF files in the directory: 120\nProcessing file 1/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 3\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 0_0, Chunk Length: 3035, Chunk Text: INFORMATION TECHNOLOGY TECHNICIAN I Summary Versat...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 0_1, Chunk Length: 2959, Chunk Text: Disaster Recovery plan and procedures  Researching...\nFile: 92069209.pdf, Chunk ID: 2, Unique Chunk ID: 0_2, Chunk Length: 2048, Chunk Text: Installing configuring and supporting McAfee antiv...\nProcessing file 2/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 3\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 1_0, Chunk Length: 3191, Chunk Text: INFORMATION TECHNOLOGY MANAGER Professional Summar...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 1_1, Chunk Length: 2744, Chunk Text: network which entailed changing software and LAN c...\nFile: 92069209.pdf, Chunk ID: 2, Unique Chunk ID: 1_2, Chunk Length: 729, Chunk Text: i4 City State 2015 Master of Science  Information ...\nProcessing file 3/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 2_0, Chunk Length: 3024, Chunk Text: WORKING RF SYSTEMS ENGINEER Qualifications Microso...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 2_1, Chunk Length: 1892, Chunk Text: surveys ElectricalValidation Engineer May 2011 to ...\nProcessing file 4/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 3_0, Chunk Length: 2934, Chunk Text: INFORMATION TECHNOLOGY MANAGER Summary Dedicated I...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 3_1, Chunk Length: 1917, Chunk Text: XP Vista and Mac operating systems  Responsible fo...\nProcessing file 5/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 4_0, Chunk Length: 2854, Chunk Text: IT MANAGEMENT Career Overview Detailoriented profe...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 4_1, Chunk Length: 2209, Chunk Text: to the device itself being recharged by a small so...\nProcessing file 6/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 5_0, Chunk Length: 2928, Chunk Text: INFORMATION TECHNOLOGY SPECIALIST Professional Sum...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 5_1, Chunk Length: 1336, Chunk Text: business and or technical problems Applies metrics...\nProcessing file 7/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 3\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 6_0, Chunk Length: 3295, Chunk Text: BRANCH CHIEF INFORMATION TECHNOLOGY SPECIALIST Pro...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 6_1, Chunk Length: 3179, Chunk Text: computer network operations plans including defens...\nFile: 92069209.pdf, Chunk ID: 2, Unique Chunk ID: 6_2, Chunk Length: 681, Chunk Text: 2009 DISA Action Officers Course 10Dec2009 DOD Inf...\nProcessing file 8/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 7_0, Chunk Length: 2784, Chunk Text: INFORMATION TECHNOLOGY COORDINATOR Career Overview...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 7_1, Chunk Length: 2741, Chunk Text: dispatch process to an automated delivery system v...\nProcessing file 9/120: 92069209.pdf\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Number of chunks for file 92069209.pdf: 2\nFile: 92069209.pdf, Chunk ID: 0, Unique Chunk ID: 8_0, Chunk Length: 3013, Chunk Text: MANAGER  INFORMATION TECHNOLOGY AND BUILDING AUTOM...\nFile: 92069209.pdf, Chunk ID: 1, Unique Chunk ID: 8_1, Chunk Length: 1972, Chunk Text: and Operation New Dawn focusing on network securit...\nProcessing file 10/120: 92069209.pdf\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "#read the top5 rows of the dataframe\r\n",
                "df.head(5)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3c0b7dba-c798-40d1-ac92-80288633497a",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "      file_name  chunk_id                                         chunk_text  \\\n0  10089434.pdf         0  INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...   \n1  10089434.pdf         1  Disaster Recovery plan and procedures  Researc...   \n2  10089434.pdf         2  Installing configuring and supporting McAfee a...   \n3  10247517.pdf         0  INFORMATION TECHNOLOGY MANAGER Professional Su...   \n4  10247517.pdf         1  network which entailed changing software and L...   \n\n  unique_chunk_id  chunk_length  \n0             0_0          3035  \n1             0_1          2959  \n2             0_2          2048  \n3             1_0          3191  \n4             1_1          2744  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>chunk_id</th>\n      <th>chunk_text</th>\n      <th>unique_chunk_id</th>\n      <th>chunk_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10089434.pdf</td>\n      <td>0</td>\n      <td>INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...</td>\n      <td>0_0</td>\n      <td>3035</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10089434.pdf</td>\n      <td>1</td>\n      <td>Disaster Recovery plan and procedures  Researc...</td>\n      <td>0_1</td>\n      <td>2959</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10089434.pdf</td>\n      <td>2</td>\n      <td>Installing configuring and supporting McAfee a...</td>\n      <td>0_2</td>\n      <td>2048</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10247517.pdf</td>\n      <td>0</td>\n      <td>INFORMATION TECHNOLOGY MANAGER Professional Su...</td>\n      <td>1_0</td>\n      <td>3191</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10247517.pdf</td>\n      <td>1</td>\n      <td>network which entailed changing software and L...</td>\n      <td>1_1</td>\n      <td>2744</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 54,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 54
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Tokenization vs. Character Length (OPTIONAL)**\n",
                "\n",
                "In this section, we will explore the difference between the character length of a text chunk and its tokenized representation. Character length simply counts the number of characters in a text, while tokenization breaks the text into meaningful units called tokens.\n",
                "\n",
                "Character Length First, let’s add a new column to our DataFrame to view the length of each chunk in terms of characters: Here, chunk\\_length represents the number of characters in each chunk."
            ],
            "metadata": {
                "azdata_cell_guid": "655a8389-1018-4e80-a39d-85187cf3c46f",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Add a new column 'chunk_length' to the DataFrame to view the length of each chunk\n",
                "df['chunk_length'] = df['chunk_text'].apply(len)\n",
                "\n",
                "# Display the first few rows of the DataFrame with the new column\n",
                "print(df[['file_name', 'chunk_id', 'chunk_length']].head(5))\n"
            ],
            "metadata": {
                "azdata_cell_guid": "5e171928-764a-4d61-899a-83a8e0c3ac79",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "      file_name  chunk_id  chunk_length\n0  10089434.pdf         0          3035\n1  10089434.pdf         1          2959\n2  10089434.pdf         2          2048\n3  10247517.pdf         0          3191\n4  10247517.pdf         1          2744\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 55
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Tokenization\n",
                "To understand how text ultimately is tokenized, it can be helpful to run the below code: \n",
                "\n",
                "- We use the tiktoken library to tokenize the text. Tokenization breaks the text into smaller units, which can be words, subwords, or characters, depending on the tokenizer used. You can see that in some cases an entire word is represented with a single token whereas in others parts of words are split across multiple tokens. \n",
                "\n",
                "- If you then check the length of the decode variable, you'll find it matches 500 our specified token number. It is simply a way of making sure none of the data we pass to the model for tokenization and embedding exceeds the input token limit of 8,192\n",
                "\n",
                "- When we pass the documents to the embeddings model, it will break the documents into tokens similar (though not necessarily identical) to the examples below and then convert the tokens to a series of floating point numbers that will be accessible via vector search"
            ],
            "metadata": {
                "azdata_cell_guid": "9b87f84b-149c-4eb5-b9f0-c1b55f6d606c",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import tiktoken\n",
                "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
                "sample_encode = tokenizer.encode(df.chunk_text[0]) \n",
                "decode = tokenizer.decode_tokens_bytes(sample_encode)\n",
                "decode\n"
            ],
            "metadata": {
                "azdata_cell_guid": "28188280-2bef-414a-a663-a017c944bc19",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[b'IN',\n b'FORMATION',\n b' TECHNO',\n b'LOGY',\n b' TECH',\n b'NIC',\n b'IAN',\n b' I',\n b' Summary',\n b' Vers',\n b'atile',\n b' Systems',\n b' Administrator',\n b' possessing',\n b' superior',\n b' troubleshooting',\n b' skills',\n b' for',\n b' networking',\n b' issues',\n b' end',\n b' user',\n b' problems',\n b' and',\n b' network',\n b' security',\n b' Experienced',\n b' in',\n b' server',\n b' management',\n b' systems',\n b' analysis',\n b' and',\n b' offering',\n b' inde',\n b'pth',\n b' understanding',\n b' of',\n b' IT',\n b' infrastructure',\n b' areas',\n b' Detail',\n b'oriented',\n b' independent',\n b' and',\n b' focused',\n b' on',\n b' taking',\n b' a',\n b' systematic',\n b' approach',\n b' to',\n b' solving',\n b' complex',\n b' problems',\n b' Demonstr',\n b'ated',\n b' exceptional',\n b' technical',\n b' knowledge',\n b' and',\n b' skills',\n b' while',\n b' working',\n b' with',\n b' various',\n b' teams',\n b' to',\n b' achieve',\n b' shared',\n b' goals',\n b' and',\n b' objectives',\n b' Highlights',\n b' ',\n b' Active',\n b' Directory',\n b' ',\n b' New',\n b' technology',\n b' and',\n b' product',\n b' research',\n b' ',\n b' Group',\n b' Policy',\n b' Objects',\n b' ',\n b' Office',\n b' ',\n b'365',\n b' and',\n b' Azure',\n b' ',\n b' PowerShell',\n b' and',\n b' VB',\n b'Script',\n b' ',\n b' Storage',\n b' management',\n b' ',\n b' Microsoft',\n b' Exchange',\n b' ',\n b' Enterprise',\n b' backup',\n b' management',\n b' ',\n b' VM',\n b'Ware',\n b' experience',\n b' ',\n b' Disaster',\n b' recovery',\n b' Experience',\n b' Information',\n b' Technology',\n b' Technician',\n b' I',\n b' Aug',\n b' ',\n b'200',\n b'7',\n b' to',\n b' Current',\n b' Company',\n b' Name',\n b' i',\n b'14',\n b' City',\n b' State',\n b' ',\n b' M',\n b'igr',\n b'ating',\n b' and',\n b' managing',\n b' user',\n b' accounts',\n b' in',\n b' Microsoft',\n b' Office',\n b' ',\n b'365',\n b' and',\n b' Exchange',\n b' Online',\n b' ',\n b' Creating',\n b' and',\n b' managing',\n b' virtual',\n b' machines',\n b' for',\n b' systems',\n b' such',\n b' as',\n b' domain',\n b' controllers',\n b' and',\n b' Active',\n b' Directory',\n b' Federation',\n b' Services',\n b' A',\n b'DFS',\n b' in',\n b' Microsoft',\n b' Windows',\n b' Azure',\n b' I',\n b'aaS',\n b' ',\n b' Creating',\n b' and',\n b' managing',\n b' storage',\n b' in',\n b' Microsoft',\n b' Windows',\n b' Azure',\n b' I',\n b'aaS',\n b' ',\n b' Installing',\n b' and',\n b' configuring',\n b' St',\n b'or',\n b'Simple',\n b' i',\n b'SC',\n b'SI',\n b' cloud',\n b' array',\n b' ST',\n b'aa',\n b'SB',\n b'aaS',\n b' ',\n b' Installing',\n b' configuring',\n b' and',\n b' testing',\n b' Twin',\n b'str',\n b'ata',\n b' i',\n b'SC',\n b'SI',\n b' cloud',\n b' array',\n b' ST',\n b'aa',\n b'SB',\n b'aaS',\n b' ',\n b' Collabor',\n b'ating',\n b' on',\n b' project',\n b' plan',\n b' for',\n b' Office',\n b' ',\n b'365',\n b' migration',\n b' ',\n b' Developing',\n b' detailed',\n b' specifications',\n b' for',\n b' the',\n b' Office',\n b' ',\n b'365',\n b' migration',\n b' including',\n b' business',\n b'case',\n b' documentation',\n b' cost',\n b' benefit',\n b' analyses',\n b' technical',\n b' diagrams',\n b' and',\n b' work',\n b' flow',\n b' documentation',\n b' ',\n b' Received',\n b' training',\n b' in',\n b' MVC',\n b' ',\n b'4',\n b' for',\n b' Visual',\n b' Studio',\n b' using',\n b' ',\n b' Net',\n b' Framework',\n b' ',\n b'445',\n b' to',\n b' develop',\n b' application',\n b' using',\n b' HTML',\n b'5',\n b' and',\n b' CSS',\n b'3',\n b' ',\n b' Installing',\n b' configuring',\n b' and',\n b' supporting',\n b' Linux',\n b' machines',\n b' for',\n b' the',\n b' open',\n b' WiFi',\n b' network',\n b' project',\n b' ',\n b' Comp',\n b'iling',\n b' and',\n b' generating',\n b' statistical',\n b' information',\n b' concerning',\n b' wireless',\n b' network',\n b' traffic',\n b' using',\n b' C',\n b'act',\n b'i',\n b' ',\n b' Config',\n b'uring',\n b' wireless',\n b' LAN',\n b' router',\n b' networking',\n b' and',\n b' security',\n b' access',\n b' ',\n b' Installing',\n b' and',\n b' configuring',\n b' wireless',\n b' certificates',\n b' ',\n b' Developing',\n b' detailed',\n b' specifications',\n b' for',\n b' the',\n b' acquisition',\n b' of',\n b' an',\n b' Enterprise',\n b' backup',\n b' system',\n b' including',\n b' systems',\n b' design',\n b' business',\n b'case',\n b' documentation',\n b' cost',\n b' benefit',\n b' analysis',\n b' technical',\n b' diagrams',\n b' and',\n b' work',\n b' flow',\n b' documentation',\n b' ',\n b' Review',\n b'ing',\n b' evaluating',\n b' and',\n b' analyzing',\n b' department',\n b'al',\n b' policies',\n b' guidelines',\n b' procedures',\n b' and',\n b' standards',\n b' with',\n b' management',\n b' and',\n b' staff',\n b' ',\n b' Developing',\n b' test',\n b' scripts',\n b' for',\n b' acceptance',\n b' unit',\n b' and',\n b' system',\n b' testing',\n b' of',\n b' Hyper',\n b'ion',\n b' Phase',\n b' ',\n b'1',\n b' and',\n b' Miami',\n b'Biz',\n b' Phase',\n b' ',\n b'2',\n b' ',\n b' Developing',\n b' Quality',\n b' Assurance',\n b' and',\n b' testing',\n b' plan',\n b' for',\n b' Hyper',\n b'ion',\n b' Phase',\n b' ',\n b'1',\n b' and',\n b' Miami',\n b'Biz',\n b' Phase',\n b' ',\n b'2',\n b' ',\n b' Debug',\n b'ging',\n b' and',\n b' logging',\n b' of',\n b' errors',\n b' in',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' using',\n b' Team',\n b' Foundation',\n b' Server',\n b' T',\n b'FS',\n b' ',\n b' Particip',\n b'ated',\n b' in',\n b' various',\n b' phases',\n b' of',\n b' the',\n b' project',\n b' life',\n b' cycle',\n b' such',\n b' as',\n b' determining',\n b' requirements',\n b' design',\n b' conceptual',\n b'ization',\n b' testing',\n b' implementation',\n b' deployment',\n b' and',\n b' release',\n b' for',\n b' the',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' projects',\n b' ',\n b' Collabor',\n b'ating',\n b' on',\n b' project',\n b' plans',\n b' for',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' ',\n b' Pre',\n b'paring',\n b' presentations',\n b' and',\n b' documentation',\n b' to',\n b' demonstrate',\n b' Hyper',\n b'ion',\n b' and',\n b' Miami',\n b'Biz',\n b' functionality',\n b' or',\n b' design',\n b' ',\n b' Monitoring',\n b' network',\n b' traffic',\n b' and',\n b' compiling',\n b' and',\n b' generating',\n b' statistical',\n b' information',\n b' using',\n b' Solar',\n b' Winds',\n b' ',\n b' Collabor',\n b'ating',\n b' on']"
                    },
                    "metadata": {},
                    "execution_count": 14,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 14
        },
        {
            "cell_type": "code",
            "source": [
                "len(decode)"
            ],
            "metadata": {
                "azdata_cell_guid": "31d80228-e140-413a-b1e3-c320a42b1f6c",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "500"
                    },
                    "metadata": {},
                    "execution_count": 15,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 15
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 2 : Generating Embeddings for Text Chunks using Azure Open AI**\n",
                "\n",
                "- After extracting and chunking the text from PDF resumes, we will generate embeddings for each chunk. These embeddings are numerical representations of the text that capture its semantic meaning. By creating embeddings for the text chunks, we can perform advanced similarity searches and enhance language model generation.\n",
                "\n",
                "- We will use the Azure OpenAI API to generate these embeddings. The `get_embedding` function defined below takes a piece of text as input and returns its embedding using the `text-embedding-small` model\n",
                "\n",
                "- Ensure the Environment Variables are set correctly in the .env file"
            ],
            "metadata": {
                "azdata_cell_guid": "ac19ee49-763a-4e2e-8c31-9ce1ba77bbe4"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import requests\n",
                "from num2words import num2words\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Specify your model name\n",
                "openai_embedding_model = os.getenv(\"AZOPENAI_EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n",
                "\n",
                "# Assuming openai_url and openai_key are your environment variables\n",
                "openai_url = os.getenv(\"AZOPENAI_ENDPOINT\") + \"openai/deployments/\" + openai_embedding_model + \"/embeddings?api-version=2023-05-15\"\n",
                "openai_key = os.getenv(\"AZOPENAI_API_KEY\")\n",
                "\n",
                "def get_embedding(text):\n",
                "    \"\"\"\n",
                "    Get sentence embedding using the Azure OpenAI text-embedding-small model.\n",
                "\n",
                "    Args:\n",
                "        text (str): Text to embed.\n",
                "\n",
                "    Returns:\n",
                "        list: A list containing the embedding.\n",
                "    \"\"\"\n",
                "    response = requests.post(openai_url,\n",
                "        headers={\"api-key\": openai_key, \"Content-Type\": \"application/json\"},\n",
                "        json={\"input\": [text]}  # Embed the extracted chunk\n",
                "    )\n",
                "    \n",
                "    if response.status_code == 200:\n",
                "        response_json = response.json()\n",
                "        embedding = json.loads(str(response_json['data'][0]['embedding']))\n",
                "        return embedding\n",
                "    else:\n",
                "        return None\n",
                "\n",
                "# Example usage\n",
                "all_filenames = []\n",
                "all_chunkids = []\n",
                "all_chunks = []\n",
                "all_embeddings = []\n",
                "\n",
                "# Assuming df is already defined with the required columns\n",
                "for index, row in df.iterrows():\n",
                "    filename = row['file_name']\n",
                "    chunkid = row['unique_chunk_id']\n",
                "    chunk = row['chunk_text']\n",
                "    embedding = get_embedding(chunk)\n",
                "    \n",
                "    if embedding is not None:\n",
                "        all_filenames.append(filename)\n",
                "        all_chunkids.append(chunkid)\n",
                "        all_chunks.append(chunk)\n",
                "        all_embeddings.append(embedding)\n",
                "    \n",
                "    if (index + 1) % 50 == 0:  # Print progress every 50 rows\n",
                "        print(f\"Completed {index + 1} rows\")\n",
                "\n",
                "# Create a new DataFrame with the results\n",
                "result_df = pd.DataFrame({\n",
                "    'filename': all_filenames,\n",
                "    'chunkid': all_chunkids,\n",
                "    'chunk': all_chunks,\n",
                "    'embedding': all_embeddings\n",
                "})\n",
                "\n",
                "print(result_df.head(5))  # Display the first few rows of the dataframe\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3aeda057-d0c0-40cd-bdcf-723abfed94e2",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Completed 50 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Completed 100 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Completed 150 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Completed 200 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Completed 250 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "Completed 300 rows\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "                                            filename chunkid  \\\n0  C:\\vectortest\\resumedata\\data\\data\\INFORMATION...     0_0   \n1  C:\\vectortest\\resumedata\\data\\data\\INFORMATION...     0_1   \n2  C:\\vectortest\\resumedata\\data\\data\\INFORMATION...     0_2   \n3  C:\\vectortest\\resumedata\\data\\data\\INFORMATION...     1_0   \n4  C:\\vectortest\\resumedata\\data\\data\\INFORMATION...     1_1   \n\n                                               chunk  \\\n0  INFORMATION TECHNOLOGY TECHNICIAN I Summary Ve...   \n1  Disaster Recovery plan and procedures  Researc...   \n2  Installing configuring and supporting McAfee a...   \n3  INFORMATION TECHNOLOGY MANAGER Professional Su...   \n4  network which entailed changing software and L...   \n\n                                           embedding  \n0  [-0.009804371, -0.0077886474, -0.0043056956, -...  \n1  [-0.0055441344, -0.0042689154, 0.00038358863, ...  \n2  [-0.005266213, -0.0009840217, -0.00897835, -0....  \n3  [-0.0156019265, -0.007396069, 0.0030721354, -0...  \n4  [-0.0134326555, -0.011126321, 0.023804175, -0....  \n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **PART 3 : Using Azure SQL DB as a Vector Database to store and query embeddings**\n",
                "\n",
                "### **Load the embeddings into the Vector Database : Azure SQL DB**\n",
                "\n",
                "First let us define a function to connect to Azure SQLDB"
            ],
            "metadata": {
                "azdata_cell_guid": "04d42351-41ec-4ce9-9778-fe4ea2ecb8a2"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "#lets define a function to connect to SQLDB\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import pyodbc\n",
                "import struct\n",
                "from azure.identity import DefaultAzureCredential\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "def get_mssql_connection():\n",
                "    # Retrieve the connection string from the environment variables\n",
                "    entra_connection_string = os.getenv('ENTRA_CONNECTION_STRING')\n",
                "    sql_connection_string = os.getenv('SQL_CONNECTION_STRING')\n",
                "\n",
                "    # Determine the authentication method and connect to the database\n",
                "    if entra_connection_string:\n",
                "        # Entra ID Service Principal Authentication\n",
                "        credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)    \n",
                "        token = credential.get_token('https://database.windows.net/.default')\n",
                "        token_bytes = token.token.encode('UTF-16LE')\n",
                "        token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
                "        SQL_COPT_SS_ACCESS_TOKEN = 1256  # This connection option is defined by Microsoft in msodbcsql.h\n",
                "        conn = pyodbc.connect(entra_connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n",
                "    \n",
                "    elif sql_connection_string:\n",
                "        # SQL Authentication\n",
                "        conn = pyodbc.connect(sql_connection_string)\n",
                "        \n",
                "    else:\n",
                "        raise ValueError(\"No valid connection string found in the environment variables.\")\n",
                "\n",
                "    return conn\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "930a63bc-4c08-4205-b152-b1ad5c82057a",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 74
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Insert embeddings into the native 'Vector' Data Type**\n",
                "\n",
                "We will insert our vectors into the SQL Table now. Azure SQL DB now has a dedicated, native, data type for storing vectors: the `vector` data type. Read about the preview [here](https://devblogs.microsoft.com/azure-sql/eap-for-vector-support-refresh-introducing-vector-type)\n",
                "\n",
                "The table embeddings has a column called vector which is vector(1536) type. Ensure you have created the table using the script `CreateTable.sql` before running the below code."
            ],
            "metadata": {
                "azdata_cell_guid": "e929c112-65d4-46f1-a9a2-7c9434b2d7cb",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pyodbc\n",
                "import pandas as pd\n",
                "\n",
                "# Retrieve the connection string from the function get_mssql_connection()\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Enable fast_executemany\n",
                "cursor.fast_executemany = True\n",
                "\n",
                "# Loop through the DataFrame rows and insert them into the table\n",
                "for index, row in result_df.iterrows():\n",
                "    chunkid = row['chunkid']\n",
                "    filename = row['filename']\n",
                "    chunk = row['chunk']\n",
                "    embedding = row['embedding']\n",
                "    \n",
                "    # Use placeholders for the parameters in the SQL query\n",
                "    query = f\"\"\"\n",
                "    INSERT INTO resumedocs (chunkid, filename, chunk, embedding)\n",
                "    VALUES (?, ?, ?, CAST(? AS VECTOR(1536)))\n",
                "    \"\"\"\n",
                "    # Execute the query with the parameters\n",
                "    cursor.execute(query, chunkid, filename, chunk, json.dumps(embedding))\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "\n",
                "# Print a success message\n",
                "print(\"Data inserted successfully into the 'resumedocs' table.\")\n",
                "\n",
                "# Close the connection\n",
                "conn.close()\n"
            ],
            "metadata": {
                "azdata_cell_guid": "680259d9-77ce-4b63-b412-9bea33bb0f43",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Data inserted successfully into the 'resumedocs' table.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 21
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take a look at the data in the Resume Docs table:"
            ],
            "metadata": {
                "azdata_cell_guid": "ede638b9-d681-4cb9-9ab8-9651e3099a36",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from prettytable import PrettyTable\n",
                "\n",
                "import pyodbc\n",
                "import pandas as pd\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Retrieve the connection string from the environment variables\n",
                "conn = get_mssql_connection()\n",
                "\n",
                "# Create a cursor object\n",
                "cursor = conn.cursor()\n",
                "\n",
                "# Use placeholders for the parameters in the SQL query\n",
                "query = \"SELECT TOP(10) filename, chunkid, chunk, CAST(embedding AS NVARCHAR(MAX)) as embedding FROM dbo.resumedocs ORDER BY Id\"\n",
                "\n",
                "# Execute the query with the parameters\n",
                "cursor.execute(query)\n",
                "queryresults = cursor.fetchall()\n",
                "\n",
                "# Get column names from cursor.description\n",
                "column_names = [column[0] for column in cursor.description]\n",
                "\n",
                "# Create a PrettyTable object\n",
                "table = PrettyTable()\n",
                "\n",
                "# Add column names to the table\n",
                "table.field_names = column_names\n",
                "\n",
                "# Set max width for each column to truncate data\n",
                "table.max_width = 20\n",
                "\n",
                "# Add rows to the table\n",
                "for row in queryresults:\n",
                "    # Truncate each value to 20 characters\n",
                "    truncated_row = [str(value)[:20] for value in row]\n",
                "    table.add_row(truncated_row)\n",
                "\n",
                "# Print the table\n",
                "print(table)\n",
                "\n",
                "# Commit the changes\n",
                "conn.commit()\n",
                "# Close the connection\n",
                "conn.close()\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "88d1a90e-e93c-426e-934d-fb1a47dfd900",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+--------------+---------+----------------------+----------------------+\n|   filename   | chunkid |        chunk         |      embedding       |\n+--------------+---------+----------------------+----------------------+\n| 10089434.pdf |   0_0   | INFORMATION TECHNOLO | [-9.8043708e-003,-7. |\n| 10089434.pdf |   0_1   | Disaster Recovery pl | [-5.5441344e-003,-4. |\n| 10089434.pdf |   0_2   | Installing configuri | [-5.2662129e-003,-9. |\n| 10247517.pdf |   1_0   | INFORMATION TECHNOLO | [-1.5601926e-002,-7. |\n| 10247517.pdf |   1_1   | network which entail | [-1.3432655e-002,-1. |\n| 10247517.pdf |   1_2   | i4 City State 2015 M | [7.1866140e-003,-7.4 |\n| 10265057.pdf |   2_0   | WORKING RF SYSTEMS E | [-1.8229846e-002,-6. |\n| 10265057.pdf |   2_1   | surveys ElectricalVa | [-2.1091947e-002,2.1 |\n| 10553553.pdf |   3_0   | INFORMATION TECHNOLO | [-9.1670882e-003,-1. |\n| 10553553.pdf |   3_1   | XP Vista and Mac ope | [3.9297581e-004,-3.9 |\n+--------------+---------+----------------------+----------------------+\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 75
        },
        {
            "cell_type": "markdown",
            "source": [
                "### **Performing Vector Similarity Search in Azure SQL DB using VECTOR\\_DISTANCE built in function**\n",
                "\n",
                "Let's now query our ResumeDocs table to get the top similar candidates given the User search query.\n",
                "\n",
                "What we are doing: Given any user search query, we can obtain the vector representation of that text. We then use this vector to calculate the cosine distance against all the resume embeddings stored in the database. By selecting only the closest matches, we can identify the resumes most relevant to the user’s query. This helps in finding the most suitable candidates based on their resumes.\n",
                "\n",
                "The most common distance is the cosine similarity, which can be calculated quite easily in SQL with the help of the new distance functions.\n",
                "\n",
                "```\n",
                "VECTOR_DISTANCE('distance metric', V1, V2)\n",
                "\n",
                "```\n",
                "\n",
                "We can use **cosine**, **euclidean**, and **dot** as the distance metric today.\n",
                "\n",
                "We will define the function `vector_search_sql`."
            ],
            "metadata": {
                "azdata_cell_guid": "b7b0fda6-3322-4bbc-8d08-d0b567da79ec",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import pyodbc\n",
                "import json\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "def vector_search_sql(query, num_results=5):\n",
                "    # Load environment variables from .env file\n",
                "    load_dotenv()\n",
                "\n",
                "    # Use the get_mssql_connection function to get the connection string details\n",
                "    conn = get_mssql_connection()\n",
                "\n",
                "    # Create a cursor object\n",
                "    cursor = conn.cursor()\n",
                "\n",
                "    # Generate the query embedding for the user's search query\n",
                "    user_query_embedding = get_embedding(query)\n",
                "    \n",
                "    # SQL query for similarity search using the function vector_distance to calculate cosine similarity\n",
                "    sql_similarity_search = f\"\"\"\n",
                "    SELECT TOP(?) filename, chunkid, chunk,\n",
                "           1-vector_distance('cosine', CAST(? AS VECTOR(1536)), embedding) AS similarity_score,\n",
                "           vector_distance('cosine', CAST(? AS VECTOR(1536)), embedding) AS distance_score\n",
                "    FROM dbo.resumedocs\n",
                "    ORDER BY distance_score \n",
                "    \"\"\"\n",
                "\n",
                "    cursor.execute(sql_similarity_search, num_results, json.dumps(user_query_embedding), json.dumps(user_query_embedding))\n",
                "    results = cursor.fetchall()\n",
                "\n",
                "    # Close the database connection\n",
                "    conn.close()\n",
                "\n",
                "    return results\n",
                "    \n",
                "#example usage\n",
                "vector_search_sql(\"database administrator\", num_results=3)"
            ],
            "metadata": {
                "azdata_cell_guid": "1b4f0ca2-2401-4f90-a44d-75dce03500cc",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "[('20001721.pdf', '41_0', 'INFORMATION TECHNOLOGY STUDENT Career Overview Resultsdriven Database Administrator with extensive education in programming relational database management and computer technology maintenance Qualifications  Database servers  Structured query language SQL expert  Programming and design skills  Document management  Strong collaborative skills  Strong analytical skills  Customer needs assessment  Excellent problem solving skills Technical Skills Skills Experience Total Years Last Used Windows Unix Linux Mac OSX VMWare HTTPApache DNSBIND SSH SNMP DNS DHCP FTP Intermediate 2 May 2016 Accomplishments Customer Service  Handled customers effectively by identifying needs quickly gaining trust approaching complex situations and resolving problems to maximize efficiency Data Preparation  Prepared chain of custody packets for title sale reviews of procedures and feesservices justification  Administration  Performed administration tasks such as filing developing spreadsheets faxing reports photocopying collateral and scanning documents for interdepartmental use  Reporting  Maintained status reports to provide management with updated information for client projects  Application Design  Used objectoriented designprogramming to design new standalone application  Planned installed configured and monitored document management infrastructure Coordinated scheduled software and hardware patches upgrades and enhancements to platforms Collaborated with IT teams to design and implement continuous process improvements to prevent production application incidents Work Experience Company Name January 2014 to Current INFORMATION TECHNOLOGY STUDENT City State Presented various projects including VPN RDMS and IT Proposals  to several classes and instructors  Worked independently and as part of a team to achieve most equitable outcome Company Name September 2010 to October 2013 FORECLOSURE PROCESSOR PARALEGAL City State Diligently reviewed the specialty loan portfolio for compliance with all reporting requirements Communicated regularly with management regarding portfolio performance and new loan transaction quality Maintained confidentiality of bank records and client information Scanned and filed forms reports correspondence and receipts Entered information into computer databases Reviewed files to check for complete and accurate information Examined Deeds of Trust to determine the grantor grantee trustee and loan amount Coordinated with multiple departments regarding responsive documents and document retention Researched bankruptcy loan files to confirm federal guideline compliance Supported a team of three attorneys with generating and filing of pleadings motions and various court documents  Company Name February 2008 to May 2008 TOEFLTESL INSTRUCTOR City State Developed interesting course plans to meet academic intellectual and social needs of students Developed and implemented interesting and interactive learning mediums to increase student understanding of course materials Performed student background reviews to develop tailored lessons based on student needs Developed administered and corrected tests and quizzes in a timely manner Combined discipline plan with effective measures', 0.8498380726099971, 0.1501619273900029),\n ('18187364.pdf', '35_2', 'State  DBA for telesales signature verification and electronic payment systems  Participated in offsite disaster recovery exercises  Reviewed schema tuned queries and managed change control process  Developed Cost Based SQL Standards and trained development staff on SQL tuning  Provided database design consultation to other projects  Developed database installation and administration guidelines Senior Database Administrator June 1997 to December 1997 Company Name i14 City State  Converted document management system from Sybase to Oracle  Mentored and trained Oracle database administrators at client sites  Monitored and tuned Oracle system and applications to prevent resource shortages and shorten the execution time of longrunning queries  Conducted training in database concepts and SQL Database Administrator September 1996 to June 1997 Company Name i14 City State  Implemented and maintained critical high volume online and Internet server Oracle databases in UNIX environment  Performed performance monitoring capacity planning and application tuning  Worked closely with engineering consulting firm to trouble shoot database and applications optimize system performance ensure data integrity and increase system reliability  Wrote extensive SQL and PLSQL programs to manage data and create ad hoc reports  Developed implemented and enforced Oracle design and usage standards Associate Computer ProgrammerAnalyst June 1991 to September 1996 Company Name i14 City State  Technical lead responsible for Pavement and Bridge Management Systems development and production Oracle databases operating in clientserver environment  Prepared EDP sections of consulting contracts and budgets  Managing analyst for Pavement and Bridge Maintenance Systems jointly developed by Rensselaer Polytechnic Institute and the Thruway Authority  Developed and maintained data standards and agency data dictionary system Education Master of Science  Management College of Saint Rose 14 City State Management Bachelor of Arts  Music History City State Music History Skills account management ad analyst application development ASM agency audit reports auditing backup budgets c Capability Maturity Model CMM capacity planning clientserver COGNOS concept conferences consultation consulting contracts client data dictionary system database and applications database administration DBA databases Database database design Designing disaster recovery document management due diligence government regulations IBM Information Security Information Systems law Managing meetings mentoring access Money Windows NT modeling Enterprise operating systems Oracle Enterprise Manager Oracle Oracle database PLSQL page PeopleSoft policies procurement Oracle RDBMS Risk Assessment scanning servers scripts Scripting software development SQL SQLLoader statistics Sybase Systems development Tivoli training programs troubleshooting UNIX UNIX shell scripts upgrade workshops', 0.8442204413166462, 0.1557795586833538),\n ('18187364.pdf', '35_1', 'Soft upgrade and AntiMoney Laundering projects  Responsible for operational aspects of Oracle database administration activities including capacity planning installation and configuration of the Oracle RDBMS Grid Control and ASM software patches and supporting products backup  recovery database tuning monitoring and troubleshooting utilizing TKPROF OEM STATSPACK DBArtisan Tivoli and custom SQL PLSQL and UNIX shell scripts  Plan and manage multilocation disaster recovery exercises  Provide operational 24X7 support of all corporate Oracle systems 341 databases 65 servers 5 versions of Oracle and 5 operating systems  Developed and implemented procedures that reduced inhouse database problem tickets by 60 job failures by 80 and oncall support issues by 80  Created enterprise wide capacity planning troubleshooting and performance monitoring models  Coordinated and supported application development testing and performance improvement efforts including data model revisions SQL tuning and client configurations  Instituted a series of workshops classes and training programs for developers to expand their knowledge and understanding of SQL Oracle and data security  This group is now selfsufficient  Performed blocklevel data recovery that Oracle Corporation said was not possible saving critical business data and minimizing impact to business functions Database Manager February 2000 to April 2002 Company Name i14 City State  Created and supported multiinstance spatial environments for internet startup company  Gathered user requirements and designed and built logical and physical database structures  Managed Unix server farm to ensure proper sizing organization and recoverability  Wrote PLSQL SQLLoader and custom routines to load and integrate data from various outside sources and to enforce data security reliability and integrity  Monitored shared system resources and recommend improvements to application development staff  Wrote databasemonitoring scripts used to page DBA in the event of database problems  Automated DBA functions for table restructuring statistics space management and backup Senior Database Administrator January 1999 to February 2000 Company Name i14 City State  Technical liaison and support manager for international leasing company  Traveled abroad as needed  Participated in due diligence audits of takeover candidate companies  Wrote Oracle installation and configuration standards for Windows NT and UNIX  Created DBA practice lab and developed practice lab exercises for other DBA staff to learn backup and recovery software  Worked closely with various vendors and development groups to improve application reliability and performance  Developed a Capability Maturity Model and created CMM training program for database administration  Provided 24X7support of international commercial leasing applications System Staff SpecialistDatabase Administrator December 1997 to January 1999 Company Name i14 City', 0.8387444263775664, 0.1612555736224336)]"
                    },
                    "metadata": {},
                    "execution_count": 43,
                    "output_type": "execute_result"
                }
            ],
            "execution_count": 43
        },
        {
            "cell_type": "markdown",
            "source": [
                "# **Part 4 : Use embeddings retrieved from a Azure SQL vector database to augment LLM generation**\n",
                "\n",
                "Lets create a helper function to feed prompts into the [Completions model](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4) & create interactive loop where you can pose questions to the model and receive information grounded in your data.\n",
                "\n",
                "The function `generate_completion` is defined to help ground the gpt-4o model with prompts and system instructions.   \n",
                "Note that we are passing the results of the `vector_search_sql` we defined earlier to the model and we define the system prompt .  \n",
                "We are using gpt-4o model here. \n",
                "\n",
                "You can get more information on using Azure Open AI GPT chat models [here](https://learn.microsoft.com/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python)"
            ],
            "metadata": {
                "azdata_cell_guid": "1e194f3c-6a7a-4f16-95ec-05f60a3770a4",
                "language": "python"
            },
            "attachments": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import AzureOpenAI\n",
                "\n",
                "# Load environment variables from a .env file\n",
                "load_dotenv()\n",
                "\n",
                "# Use environment variables for the API key and endpoint\n",
                "api_key = os.getenv(\"AZOPENAI_API_KEY\")\n",
                "azure_endpoint = os.getenv(\"AZOPENAI_ENDPOINT\")\n",
                "chat_model = os.getenv(\"AZOPENAI_CHAT_MODEL_DEPLOYMENT_NAME\")\n",
                "\n",
                "# Create a chat completion request\n",
                "client = AzureOpenAI(\n",
                "    api_key=api_key,\n",
                "    api_version=\"2023-05-15\",\n",
                "    azure_endpoint=azure_endpoint\n",
                ")\n",
                "\n",
                "def generate_completion(search_results, user_input):\n",
                "    system_prompt = '''\n",
                "You are an intelligent & funny assistant who will exclusively answer based on the data provided in the `search_results`:\n",
                "- Use the information from `search_results` to generate your top 3 responses. If the data is not a perfect match for the user's query, use your best judgment to provide helpful suggestions and include the following format:\n",
                "  File: {filename}\n",
                "  Chunk ID: {chunkid}\n",
                "  Similarity Score: {similarity_score}\n",
                "  Add a small snippet from the Relevant Text: {chunktext}\n",
                "  Do not use the entire chunk\n",
                "- Avoid any other external data sources.\n",
                "- Add a summary about why the candidate maybe a goodfit even if exact skills and the role being hired for are not matching , at the end of the recommendations. Ensure you call out which skills match the description and which ones are missing. If the candidate doesnt have prior experience for the hiring role which we may need to pay extra attention to during the interview process.\n",
                "- Add a Microsoft related interesting fact about the technology that was searched \n",
                "'''\n",
                "\n",
                "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
                "    \n",
                "    # Create an empty list to store the results\n",
                "    result_list = []\n",
                "\n",
                "    # Iterate through the search results and append relevant information to the list\n",
                "    for result in search_results:\n",
                "        filename = result  # Assuming filename is the first column\n",
                "        chunkid = result\n",
                "        chunktext = result\n",
                "        similarity_score = result  # Assuming similarity_score is the third column\n",
                "        \n",
                "        # Append the relevant information as a dictionary to the result_list\n",
                "        result_list.append({\n",
                "            \"filename\": filename,\n",
                "            \"chunkid\": chunkid,\n",
                "            \"chunktext\": chunktext,\n",
                "            \"similarity_score\": similarity_score\n",
                "        })\n",
                "\n",
                "    # Print the result list\n",
                "    #print(result_list)\n",
                "    \n",
                "    messages.append({\"role\": \"system\", \"content\": f\"{result_list}\"})\n",
                "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
                "    response = client.chat.completions.create(model=chat_model, messages=messages, temperature=0) \n",
                "\n",
                "    return response.dict()\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2865a0d0-2ee3-4d0a-8ee8-89075ab541bc",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 76
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a loop of user input and model output to perform Q&A on the PDF's that are now chunked and stored in the SQL DB with embeddings\n",
                "#\n",
                "# PLEASE NOTE: An input box will be displayed for the user to enter a question/query at the top of the scree.\n",
                "# The model will then provide a response based on the data stored in the SQL DB.\n",
                "# Type 'end' to end the session.\n",
                "#\n",
                "print(\"*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\\n\")\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"User prompt: \")\n",
                "    if user_input.lower() == \"end\":\n",
                "        break\n",
                "\n",
                "    # Print the user's question\n",
                "    print(f\"\\nUser asked: {user_input}\")\n",
                "\n",
                "  \n",
                "    # Assuming vector_search_sql and generate_completion are defined functions that work correctly\n",
                "    search_results = vector_search_sql(user_input)\n",
                "    completions_results = generate_completion(search_results, user_input)\n",
                "\n",
                "    # Print the model's response\n",
                "    print(\"\\nAI's response:\")\n",
                "    print(completions_results['choices'][0]['message']['content'])\n",
                "\n",
                "# The loop will continue until the user types 'end'\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "d69ab285-0cc9-4596-95f2-688d0c324f6b",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "*** What Role are you hiring for? And What skills are you looking for? Ask me & I can help you find a candidate :) Type 'end' to end the session.\n\n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nUser asked: We are hiring a Product Manager in the Microsoft Azure Database Migration team. Candidate should have good knowledge on SQL and any other databases like Oracle, PostgreSQL etc. It would be beneficial if they have had cloud experience . We seek strong handson skills in migration projects \n",
                    "output_type": "stream"
                },
                {
                    "name": "stdout",
                    "text": "\nAI's response:\nHere are the top 3 candidates based on the provided search results:\n\n### Candidate 1:\n**File:** 18067556.pdf  \n**Chunk ID:** 32_6  \n**Similarity Score:** 0.8163  \n**Relevant Text Snippet:** \n\"Strong knowledge of AWS, Azure, Cisco Switch Management, VMWare, HyperV, RDP, Automation Anywhere, Active Directory, and hardware and software administration for IOS, Android, Windows, Red Hat Linux, RF devices... Programming Databases SQL, SSRS, SSIS, SSAS, VBA, DAX, HTML, CSS, VBA, VB.NET, R, Powershell, Python, Oracle... Business Intelligence Packages PowerBI, Qlik, Qlik View, QlikSense, SiSense, Tableau, Datorama, Yellowfin, Crystal, SSRS.\"\n\n**Summary:** This candidate has extensive experience with SQL, Oracle, and Azure, which aligns well with the requirements for the Product Manager role in the Azure Database Migration team. They also have experience with various business intelligence tools and cloud platforms, including AWS and Azure. However, specific hands-on migration project experience is not explicitly mentioned and should be explored during the interview.\n\n### Candidate 2:\n**File:** 29051656.pdf  \n**Chunk ID:** 81_0  \n**Similarity Score:** 0.8146  \n**Relevant Text Snippet:** \n\"An organized DBA professional with over 6 years hands-on experience supporting Oracle databases, SQL Server databases, and AWS infrastructure... Migrated databases from on-premise to AWS using Database migration services... Launched and maintained RDS and EC2 instances in AWS.\"\n\n**Summary:** This candidate has significant hands-on experience with database migration projects, particularly with Oracle and SQL Server databases. They also have cloud experience with AWS, which is beneficial. While Azure experience is not explicitly mentioned, their strong background in database migration and cloud infrastructure makes them a strong candidate for the role.\n\n### Candidate 3:\n**File:** 26746496.pdf  \n**Chunk ID:** 67_0  \n**Similarity Score:** 0.8115  \n**Relevant Text Snippet:** \n\"Software Engineer with 2 years in Web Developer specializing in front end development... Good experience in writing Class Library using C#, LINQ to SQL queries in Database Access layer to interface with SQL Database... Worked extensively with .NET Server Controls, Web User Controls, Data Grid Web Control, Form Validation Controls, and created Custom controls.\"\n\n**Summary:** This candidate has experience with SQL and .NET technologies, which are relevant to the role. However, their experience seems more focused on web development and front-end technologies rather than database migration. They have some cloud experience but lack explicit mention of hands-on migration projects, which should be further investigated during the interview.\n\n### Summary:\n- **Candidate 1**: Strong in SQL, Oracle, and Azure with broad technical skills. Missing explicit hands-on migration project experience.\n- **Candidate 2**: Extensive hands-on migration experience with Oracle and SQL Server, strong cloud experience with AWS. Missing explicit Azure experience.\n- **Candidate 3**: Good SQL and .NET experience, but more focused on web development. Missing explicit hands-on migration project experience and Azure experience.\n\n### Microsoft Fact:\nDid you know that Microsoft Azure's Database Migration Service (DMS) supports seamless migrations from multiple database sources, including SQL Server, Oracle, and PostgreSQL, to Azure with minimal downtime? This service is designed to simplify and accelerate the migration process, making it easier for organizations to move their databases to the cloud.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": null
        }
    ]
}